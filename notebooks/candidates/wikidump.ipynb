{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract plain text from Wikidump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = '/home/mostendorff/datasets/wikinews_en/20201201/enwikinews-20201201.json.gz'\n",
    "fp = '/home/mostendorff/datasets/wikinews_en/20201201/enwikinews-20201201-pages-meta-current.xml.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Copyright (C) 2010 Radim Rehurek <radimrehurek@seznam.cz>\n",
    "# Copyright (C) 2012 Lars Buitinck <larsmans@gmail.com>\n",
    "# Copyright (C) 2018 Emmanouil Stergiadis <em.stergiadis@gmail.com>\n",
    "# Licensed under the GNU LGPL v2.1 - http://www.gnu.org/licenses/lgpl.html\n",
    "\n",
    "\"\"\"Construct a corpus from a Wikipedia (or other MediaWiki-based) database dump.\n",
    "\n",
    "Uses multiprocessing internally to parallelize the work and process the dump more quickly.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "If you have the `pattern <https://github.com/clips/pattern>`_ package installed,\n",
    "this module will use a fancy lemmatization to get a lemma of each token (instead of plain alphabetic tokenizer).\n",
    "\n",
    "See :mod:`gensim.scripts.make_wiki` for a canned (example) command-line script based on this module.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import bz2\n",
    "import logging\n",
    "import multiprocessing\n",
    "import re\n",
    "import signal\n",
    "from pickle import PicklingError\n",
    "# LXML isn't faster, so let's go with the built-in solution\n",
    "from xml.etree.ElementTree import iterparse\n",
    "\n",
    "\n",
    "from gensim import utils\n",
    "# cannot import whole gensim.corpora, because that imports wikicorpus...\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.corpora.textcorpus import TextCorpus\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "ARTICLE_MIN_WORDS = 50\n",
    "\"\"\"Ignore shorter articles (after full preprocessing).\"\"\"\n",
    "\n",
    "# default thresholds for lengths of individual tokens\n",
    "TOKEN_MIN_LEN = 2\n",
    "TOKEN_MAX_LEN = 15\n",
    "\n",
    "RE_P0 = re.compile(r'<!--.*?-->', re.DOTALL | re.UNICODE)\n",
    "\"\"\"Comments.\"\"\"\n",
    "RE_P1 = re.compile(r'<ref([> ].*?)(</ref>|/>)', re.DOTALL | re.UNICODE)\n",
    "\"\"\"Footnotes.\"\"\"\n",
    "RE_P2 = re.compile(r'(\\n\\[\\[[a-z][a-z][\\w-]*:[^:\\]]+\\]\\])+$', re.UNICODE)\n",
    "\"\"\"Links to languages.\"\"\"\n",
    "RE_P3 = re.compile(r'{{([^}{]*)}}', re.DOTALL | re.UNICODE)\n",
    "\"\"\"Template.\"\"\"\n",
    "RE_P4 = re.compile(r'{{([^}]*)}}', re.DOTALL | re.UNICODE)\n",
    "\"\"\"Template.\"\"\"\n",
    "RE_P5 = re.compile(r'\\[(\\w+):\\/\\/(.*?)(( (.*?))|())\\]', re.UNICODE)\n",
    "\"\"\"Remove URL, keep description.\"\"\"\n",
    "RE_P6 = re.compile(r'\\[([^][]*)\\|([^][]*)\\]', re.DOTALL | re.UNICODE)\n",
    "\"\"\"Simplify links, keep description.\"\"\"\n",
    "RE_P7 = re.compile(r'\\n\\[\\[[iI]mage(.*?)(\\|.*?)*\\|(.*?)\\]\\]', re.UNICODE)\n",
    "\"\"\"Keep description of images.\"\"\"\n",
    "RE_P8 = re.compile(r'\\n\\[\\[[fF]ile(.*?)(\\|.*?)*\\|(.*?)\\]\\]', re.UNICODE)\n",
    "\"\"\"Keep description of files.\"\"\"\n",
    "RE_P9 = re.compile(r'<nowiki([> ].*?)(</nowiki>|/>)', re.DOTALL | re.UNICODE)\n",
    "\"\"\"External links.\"\"\"\n",
    "RE_P10 = re.compile(r'<math([> ].*?)(</math>|/>)', re.DOTALL | re.UNICODE)\n",
    "\"\"\"Math content.\"\"\"\n",
    "RE_P11 = re.compile(r'<(.*?)>', re.DOTALL | re.UNICODE)\n",
    "\"\"\"All other tags.\"\"\"\n",
    "RE_P12 = re.compile(r'(({\\|)|(\\|-(?!\\d))|(\\|}))(.*?)(?=\\n)', re.UNICODE)\n",
    "\"\"\"Table formatting.\"\"\"\n",
    "RE_P13 = re.compile(r'(?<=(\\n[ ])|(\\n\\n)|([ ]{2})|(.\\n)|(.\\t))(\\||\\!)([^[\\]\\n]*?\\|)*', re.UNICODE)\n",
    "\"\"\"Table cell formatting.\"\"\"\n",
    "RE_P14 = re.compile(r'\\[\\[Category:[^][]*\\]\\]', re.UNICODE)\n",
    "\"\"\"Categories.\"\"\"\n",
    "RE_P15 = re.compile(r'\\[\\[([fF]ile:|[iI]mage)[^]]*(\\]\\])', re.UNICODE)\n",
    "\"\"\"Remove File and Image templates.\"\"\"\n",
    "RE_P16 = re.compile(r'\\[{2}(.*?)\\]{2}', re.UNICODE)\n",
    "\"\"\"Capture interlinks text and article linked\"\"\"\n",
    "RE_P17 = re.compile(\n",
    "    r'(\\n.{0,4}((bgcolor)|(\\d{0,1}[ ]?colspan)|(rowspan)|(style=)|(class=)|(align=)|(scope=))(.*))|'\n",
    "    r'(^.{0,2}((bgcolor)|(\\d{0,1}[ ]?colspan)|(rowspan)|(style=)|(class=)|(align=))(.*))',\n",
    "    re.UNICODE\n",
    ")\n",
    "\"\"\"Table markup\"\"\"\n",
    "IGNORED_NAMESPACES = [\n",
    "    'Wikipedia', 'Category', 'File', 'Portal', 'Template',\n",
    "    'MediaWiki', 'User', 'Help', 'Book', 'Draft', 'WikiProject',\n",
    "    'Special', 'Talk'\n",
    "]\n",
    "\"\"\"MediaWiki namespaces that ought to be ignored.\"\"\"\n",
    "\n",
    "\n",
    "def filter_example(elem, text, *args, **kwargs):\n",
    "    \"\"\"Example function for filtering arbitrary documents from wikipedia dump.\n",
    "\n",
    "\n",
    "    The custom filter function is called _before_ tokenisation and should work on\n",
    "    the raw text and/or XML element information.\n",
    "\n",
    "    The filter function gets the entire context of the XML element passed into it,\n",
    "    but you can of course choose not the use some or all parts of the context. Please\n",
    "    refer to :func:`gensim.corpora.wikicorpus.extract_pages` for the exact details\n",
    "    of the page context.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    elem : etree.Element\n",
    "        XML etree element\n",
    "    text : str\n",
    "        The text of the XML node\n",
    "    namespace : str\n",
    "        XML namespace of the XML element\n",
    "    title : str\n",
    "       Page title\n",
    "    page_tag : str\n",
    "        XPath expression for page.\n",
    "    text_path : str\n",
    "        XPath expression for text.\n",
    "    title_path : str\n",
    "        XPath expression for title.\n",
    "    ns_path : str\n",
    "        XPath expression for namespace.\n",
    "    pageid_path : str\n",
    "        XPath expression for page id.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    .. sourcecode:: pycon\n",
    "\n",
    "        >>> import gensim.corpora\n",
    "        >>> filter_func = gensim.corpora.wikicorpus.filter_example\n",
    "        >>> dewiki = gensim.corpora.WikiCorpus(\n",
    "        ...     './dewiki-20180520-pages-articles-multistream.xml.bz2',\n",
    "        ...     filter_articles=filter_func)\n",
    "\n",
    "    \"\"\"\n",
    "    # Filter German wikipedia dump for articles that are marked either as\n",
    "    # Lesenswert (featured) or Exzellent (excellent) by wikipedia editors.\n",
    "    # *********************\n",
    "    # regex is in the function call so that we do not pollute the wikicorpus\n",
    "    # namespace do not do this in production as this function is called for\n",
    "    # every element in the wiki dump\n",
    "    _regex_de_excellent = re.compile(r'.*\\{\\{(Exzellent.*?)\\}\\}[\\s]*', flags=re.DOTALL)\n",
    "    _regex_de_featured = re.compile(r'.*\\{\\{(Lesenswert.*?)\\}\\}[\\s]*', flags=re.DOTALL)\n",
    "\n",
    "    if text is None:\n",
    "        return False\n",
    "    if _regex_de_excellent.match(text) or _regex_de_featured.match(text):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def find_interlinks(raw):\n",
    "    \"\"\"Find all interlinks to other articles in the dump.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw : str\n",
    "        Unicode or utf-8 encoded string.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of tuples in format [(linked article, the actual text found), ...].\n",
    "\n",
    "    \"\"\"\n",
    "    filtered = filter_wiki(raw, promote_remaining=False, simplify_links=False)\n",
    "    interlinks_raw = re.findall(RE_P16, filtered)\n",
    "\n",
    "    interlinks = []\n",
    "    for parts in [i.split('|') for i in interlinks_raw]:\n",
    "        actual_title = parts[0]\n",
    "        try:\n",
    "            interlink_text = parts[1]\n",
    "        except IndexError:\n",
    "            interlink_text = actual_title\n",
    "        interlink_tuple = (actual_title, interlink_text)\n",
    "        interlinks.append(interlink_tuple)\n",
    "\n",
    "    legit_interlinks = [(i, j) for i, j in interlinks if '[' not in i and ']' not in i]\n",
    "    return legit_interlinks\n",
    "\n",
    "\n",
    "def filter_wiki(raw, promote_remaining=True, simplify_links=True):\n",
    "    \"\"\"Filter out wiki markup from `raw`, leaving only text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw : str\n",
    "        Unicode or utf-8 encoded string.\n",
    "    promote_remaining : bool\n",
    "        Whether uncaught markup should be promoted to plain text.\n",
    "    simplify_links : bool\n",
    "        Whether links should be simplified keeping only their description text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        `raw` without markup.\n",
    "\n",
    "    \"\"\"\n",
    "    # parsing of the wiki markup is not perfect, but sufficient for our purposes\n",
    "    # contributions to improving this code are welcome :)\n",
    "    text = utils.to_unicode(raw, 'utf8', errors='ignore')\n",
    "    text = utils.decode_htmlentities(text)  # '&amp;nbsp;' --> '\\xa0'\n",
    "    return remove_markup(text, promote_remaining, simplify_links)\n",
    "\n",
    "\n",
    "def remove_markup(text, promote_remaining=True, simplify_links=True):\n",
    "    \"\"\"Filter out wiki markup from `text`, leaving only text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        String containing markup.\n",
    "    promote_remaining : bool\n",
    "        Whether uncaught markup should be promoted to plain text.\n",
    "    simplify_links : bool\n",
    "        Whether links should be simplified keeping only their description text.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        `text` without markup.\n",
    "\n",
    "    \"\"\"\n",
    "    text = re.sub(RE_P2, '', text)  # remove the last list (=languages)\n",
    "    # the wiki markup is recursive (markup inside markup etc)\n",
    "    # instead of writing a recursive grammar, here we deal with that by removing\n",
    "    # markup in a loop, starting with inner-most expressions and working outwards,\n",
    "    # for as long as something changes.\n",
    "    text = remove_template(text)\n",
    "    text = remove_file(text)\n",
    "    iters = 0\n",
    "    while True:\n",
    "        old, iters = text, iters + 1\n",
    "        text = re.sub(RE_P0, '', text)  # remove comments\n",
    "        text = re.sub(RE_P1, '', text)  # remove footnotes\n",
    "        text = re.sub(RE_P9, '', text)  # remove outside links\n",
    "        text = re.sub(RE_P10, '', text)  # remove math content\n",
    "        text = re.sub(RE_P11, '', text)  # remove all remaining tags\n",
    "        text = re.sub(RE_P14, '', text)  # remove categories\n",
    "        text = re.sub(RE_P5, '\\\\3', text)  # remove urls, keep description\n",
    "\n",
    "        if simplify_links:\n",
    "            text = re.sub(RE_P6, '\\\\2', text)  # simplify links, keep description only\n",
    "        # remove table markup\n",
    "        text = text.replace(\"!!\", \"\\n|\")  # each table head cell on a separate line\n",
    "        text = text.replace(\"|-||\", \"\\n|\")  # for cases where a cell is filled with '-'\n",
    "        text = re.sub(RE_P12, '\\n', text)  # remove formatting lines\n",
    "        text = text.replace('|||', '|\\n|')  # each table cell on a separate line(where |{{a|b}}||cell-content)\n",
    "        text = text.replace('||', '\\n|')  # each table cell on a separate line\n",
    "        text = re.sub(RE_P13, '\\n', text)  # leave only cell content\n",
    "        text = re.sub(RE_P17, '\\n', text)  # remove formatting lines\n",
    "\n",
    "        # remove empty mark-up\n",
    "        text = text.replace('[]', '')\n",
    "        # stop if nothing changed between two iterations or after a fixed number of iterations\n",
    "        if old == text or iters > 2:\n",
    "            break\n",
    "\n",
    "    if promote_remaining:\n",
    "        text = text.replace('[', '').replace(']', '')  # promote all remaining markup to plain text\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_template(s):\n",
    "    \"\"\"Remove template wikimedia markup.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : str\n",
    "        String containing markup template.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Сopy of `s` with all the `wikimedia markup template <http://meta.wikimedia.org/wiki/Help:Template>`_ removed.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Since template can be nested, it is difficult remove them using regular expressions.\n",
    "\n",
    "    \"\"\"\n",
    "    # Find the start and end position of each template by finding the opening\n",
    "    # '{{' and closing '}}'\n",
    "    n_open, n_close = 0, 0\n",
    "    starts, ends = [], [-1]\n",
    "    in_template = False\n",
    "    prev_c = None\n",
    "    for i, c in enumerate(s):\n",
    "        if not in_template:\n",
    "            if c == '{' and c == prev_c:\n",
    "                starts.append(i - 1)\n",
    "                in_template = True\n",
    "                n_open = 1\n",
    "        if in_template:\n",
    "            if c == '{':\n",
    "                n_open += 1\n",
    "            elif c == '}':\n",
    "                n_close += 1\n",
    "            if n_open == n_close:\n",
    "                ends.append(i)\n",
    "                in_template = False\n",
    "                n_open, n_close = 0, 0\n",
    "        prev_c = c\n",
    "\n",
    "    # Remove all the templates\n",
    "    starts.append(None)\n",
    "    return ''.join(s[end + 1:start] for end, start in zip(ends, starts))\n",
    "\n",
    "\n",
    "def remove_file(s):\n",
    "    \"\"\"Remove the 'File:' and 'Image:' markup, keeping the file caption.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : str\n",
    "        String containing 'File:' and 'Image:' markup.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Сopy of `s` with all the 'File:' and 'Image:' markup replaced by their `corresponding captions\n",
    "        <http://www.mediawiki.org/wiki/Help:Images>`_.\n",
    "\n",
    "    \"\"\"\n",
    "    # The regex RE_P15 match a File: or Image: markup\n",
    "    for match in re.finditer(RE_P15, s):\n",
    "        m = match.group(0)\n",
    "        caption = m[:-2].split('|')[-1]\n",
    "        s = s.replace(m, caption, 1)\n",
    "    return s\n",
    "\n",
    "\n",
    "def tokenize(content, token_min_len=TOKEN_MIN_LEN, token_max_len=TOKEN_MAX_LEN, lower=True):\n",
    "    \"\"\"Tokenize a piece of text from Wikipedia.\n",
    "\n",
    "    Set `token_min_len`, `token_max_len` as character length (not bytes!) thresholds for individual tokens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    content : str\n",
    "        String without markup (see :func:`~gensim.corpora.wikicorpus.filter_wiki`).\n",
    "    token_min_len : int\n",
    "        Minimal token length.\n",
    "    token_max_len : int\n",
    "        Maximal token length.\n",
    "    lower : bool\n",
    "         Convert `content` to lower case?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        List of tokens from `content`.\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO maybe ignore tokens with non-latin characters? (no chinese, arabic, russian etc.)\n",
    "    return [\n",
    "        utils.to_unicode(token) for token in utils.tokenize(content, lower=lower, errors='ignore')\n",
    "        if token_min_len <= len(token) <= token_max_len and not token.startswith('_')\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_namespace(tag):\n",
    "    \"\"\"Get the namespace of tag.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tag : str\n",
    "        Namespace or tag.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Matched namespace or tag.\n",
    "\n",
    "    \"\"\"\n",
    "    m = re.match(\"^{(.*?)}\", tag)\n",
    "    namespace = m.group(1) if m else \"\"\n",
    "    if not namespace.startswith(\"http://www.mediawiki.org/xml/export-\"):\n",
    "        raise ValueError(\"%s not recognized as MediaWiki dump namespace\" % namespace)\n",
    "    return namespace\n",
    "\n",
    "\n",
    "_get_namespace = get_namespace\n",
    "\n",
    "\n",
    "def extract_pages(f, filter_namespaces=False, filter_articles=None):\n",
    "    \"\"\"Extract pages from a MediaWiki database dump.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : file\n",
    "        File-like object.\n",
    "    filter_namespaces : list of str or bool\n",
    "         Namespaces that will be extracted.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    tuple of (str or None, str, str)\n",
    "        Title, text and page id.\n",
    "\n",
    "    \"\"\"\n",
    "    elems = (elem for _, elem in iterparse(f, events=(\"end\",)))\n",
    "\n",
    "    # We can't rely on the namespace for database dumps, since it's changed\n",
    "    # it every time a small modification to the format is made. So, determine\n",
    "    # those from the first element we find, which will be part of the metadata,\n",
    "    # and construct element paths.\n",
    "    elem = next(elems)\n",
    "    namespace = get_namespace(elem.tag)\n",
    "    ns_mapping = {\"ns\": namespace}\n",
    "    page_tag = \"{%(ns)s}page\" % ns_mapping\n",
    "    text_path = \"./{%(ns)s}revision/{%(ns)s}text\" % ns_mapping\n",
    "    title_path = \"./{%(ns)s}title\" % ns_mapping\n",
    "    ns_path = \"./{%(ns)s}ns\" % ns_mapping\n",
    "    pageid_path = \"./{%(ns)s}id\" % ns_mapping\n",
    "\n",
    "    for elem in elems:\n",
    "        if elem.tag == page_tag:\n",
    "            title = elem.find(title_path).text\n",
    "            text = elem.find(text_path).text\n",
    "\n",
    "            if filter_namespaces:\n",
    "                ns = elem.find(ns_path).text\n",
    "                if ns not in filter_namespaces:\n",
    "                    text = None\n",
    "\n",
    "            if filter_articles is not None:\n",
    "                if not filter_articles(\n",
    "                        elem, namespace=namespace, title=title,\n",
    "                        text=text, page_tag=page_tag,\n",
    "                        text_path=text_path, title_path=title_path,\n",
    "                        ns_path=ns_path, pageid_path=pageid_path):\n",
    "                    text = None\n",
    "\n",
    "            pageid = elem.find(pageid_path).text\n",
    "            yield title, text or \"\", pageid  # empty page will yield None\n",
    "\n",
    "            # Prune the element tree, as per\n",
    "            # http://www.ibm.com/developerworks/xml/library/x-hiperfparse/\n",
    "            # except that we don't need to prune backlinks from the parent\n",
    "            # because we don't use LXML.\n",
    "            # We do this only for <page>s, since we need to inspect the\n",
    "            # ./revision/text element. The pages comprise the bulk of the\n",
    "            # file, so in practice we prune away enough.\n",
    "            elem.clear()\n",
    "\n",
    "\n",
    "_extract_pages = extract_pages  # for backward compatibility\n",
    "\n",
    "\n",
    "def process_article(args, tokenizer_func=tokenize, token_min_len=TOKEN_MIN_LEN,\n",
    "                    token_max_len=TOKEN_MAX_LEN, lower=True):\n",
    "    \"\"\"Parse a Wikipedia article, extract all tokens.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Set `tokenizer_func` (defaults is :func:`~gensim.corpora.wikicorpus.tokenize`) parameter for languages\n",
    "    like Japanese or Thai to perform better tokenization.\n",
    "    The `tokenizer_func` needs to take 4 parameters: (text: str, token_min_len: int, token_max_len: int, lower: bool).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args : (str, bool, str, int)\n",
    "        Article text, lemmatize flag (if True, :func:`~gensim.utils.lemmatize` will be used), article title,\n",
    "        page identificator.\n",
    "    tokenizer_func : function\n",
    "        Function for tokenization (defaults is :func:`~gensim.corpora.wikicorpus.tokenize`).\n",
    "        Needs to have interface:\n",
    "        tokenizer_func(text: str, token_min_len: int, token_max_len: int, lower: bool) -> list of str.\n",
    "    token_min_len : int\n",
    "        Minimal token length.\n",
    "    token_max_len : int\n",
    "        Maximal token length.\n",
    "    lower : bool\n",
    "         Convert article text to lower case?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (list of str, str, int)\n",
    "        List of tokens from article, title and page id.\n",
    "\n",
    "    \"\"\"\n",
    "    text, lemmatize, title, pageid = args\n",
    "    text = filter_wiki(text)\n",
    "    if lemmatize:\n",
    "        result = utils.lemmatize(text)\n",
    "    else:\n",
    "        result = tokenizer_func(text, token_min_len, token_max_len, lower)\n",
    "    return result, title, pageid\n",
    "\n",
    "\n",
    "def init_to_ignore_interrupt():\n",
    "    \"\"\"Enables interruption ignoring.\n",
    "\n",
    "    Warnings\n",
    "    --------\n",
    "    Should only be used when master is prepared to handle termination of\n",
    "    child processes.\n",
    "\n",
    "    \"\"\"\n",
    "    signal.signal(signal.SIGINT, signal.SIG_IGN)\n",
    "\n",
    "\n",
    "def _process_article(args):\n",
    "    \"\"\"Same as :func:`~gensim.corpora.wikicorpus.process_article`, but with args in list format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args : [(str, bool, str, int), (function, int, int, bool)]\n",
    "        First element - same as `args` from :func:`~gensim.corpora.wikicorpus.process_article`,\n",
    "        second element is tokenizer function, token minimal length, token maximal length, lowercase flag.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (list of str, str, int)\n",
    "        List of tokens from article, title and page id.\n",
    "\n",
    "    Warnings\n",
    "    --------\n",
    "    Should not be called explicitly. Use :func:`~gensim.corpora.wikicorpus.process_article` instead.\n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer_func, token_min_len, token_max_len, lower = args[-1]\n",
    "    args = args[:-1]\n",
    "\n",
    "    return process_article(\n",
    "        args, tokenizer_func=tokenizer_func, token_min_len=token_min_len,\n",
    "        token_max_len=token_max_len, lower=lower\n",
    "    )\n",
    "\n",
    "\n",
    "class WikiCorpus(TextCorpus):\n",
    "    \"\"\"Treat a Wikipedia articles dump as a read-only, streamed, memory-efficient corpus.\n",
    "\n",
    "    Supported dump formats:\n",
    "\n",
    "    * <LANG>wiki-<YYYYMMDD>-pages-articles.xml.bz2\n",
    "    * <LANG>wiki-latest-pages-articles.xml.bz2\n",
    "\n",
    "    The documents are extracted on-the-fly, so that the whole (massive) dump can stay compressed on disk.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Dumps for the English Wikipedia can be founded at https://dumps.wikimedia.org/enwiki/.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    metadata : bool\n",
    "        Whether to write articles titles to serialized corpus.\n",
    "\n",
    "    Warnings\n",
    "    --------\n",
    "    \"Multistream\" archives are *not* supported in Python 2 due to `limitations in the core bz2 library\n",
    "    <https://docs.python.org/2/library/bz2.html#de-compression-of-files>`_.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    .. sourcecode:: pycon\n",
    "\n",
    "        >>> from gensim.test.utils import datapath, get_tmpfile\n",
    "        >>> from gensim.corpora import WikiCorpus, MmCorpus\n",
    "        >>>\n",
    "        >>> path_to_wiki_dump = datapath(\"enwiki-latest-pages-articles1.xml-p000000010p000030302-shortened.bz2\")\n",
    "        >>> corpus_path = get_tmpfile(\"wiki-corpus.mm\")\n",
    "        >>>\n",
    "        >>> wiki = WikiCorpus(path_to_wiki_dump)  # create word->word_id mapping, ~8h on full wiki\n",
    "        >>> MmCorpus.serialize(corpus_path, wiki)  # another 8h, creates a file in MatrixMarket format and mapping\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, fname, processes=None, lemmatize=utils.has_pattern(), dictionary=None,\n",
    "                 filter_namespaces=('0',), tokenizer_func=tokenize, article_min_tokens=ARTICLE_MIN_WORDS,\n",
    "                 token_min_len=TOKEN_MIN_LEN, token_max_len=TOKEN_MAX_LEN, lower=True, filter_articles=None):\n",
    "        \"\"\"Initialize the corpus.\n",
    "\n",
    "        Unless a dictionary is provided, this scans the corpus once,\n",
    "        to determine its vocabulary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fname : str\n",
    "            Path to the Wikipedia dump file.\n",
    "        processes : int, optional\n",
    "            Number of processes to run, defaults to `max(1, number of cpu - 1)`.\n",
    "        lemmatize : bool\n",
    "            Use lemmatization instead of simple regexp tokenization.\n",
    "            Defaults to `True` if you have the `pattern <https://github.com/clips/pattern>`_ package installed.\n",
    "        dictionary : :class:`~gensim.corpora.dictionary.Dictionary`, optional\n",
    "            Dictionary, if not provided,  this scans the corpus once, to determine its vocabulary\n",
    "            **IMPORTANT: this needs a really long time**.\n",
    "        filter_namespaces : tuple of str, optional\n",
    "            Namespaces to consider.\n",
    "        tokenizer_func : function, optional\n",
    "            Function that will be used for tokenization. By default, use :func:`~gensim.corpora.wikicorpus.tokenize`.\n",
    "            If you inject your own tokenizer, it must conform to this interface:\n",
    "            `tokenizer_func(text: str, token_min_len: int, token_max_len: int, lower: bool) -> list of str`\n",
    "        article_min_tokens : int, optional\n",
    "            Minimum tokens in article. Article will be ignored if number of tokens is less.\n",
    "        token_min_len : int, optional\n",
    "            Minimal token length.\n",
    "        token_max_len : int, optional\n",
    "            Maximal token length.\n",
    "        lower : bool, optional\n",
    "             If True - convert all text to lower case.\n",
    "        filter_articles: callable or None, optional\n",
    "            If set, each XML article element will be passed to this callable before being processed. Only articles\n",
    "            where the callable returns an XML element are processed, returning None allows filtering out\n",
    "            some articles based on customised rules.\n",
    "\n",
    "        Warnings\n",
    "        --------\n",
    "        Unless a dictionary is provided, this scans the corpus once, to determine its vocabulary.\n",
    "\n",
    "        \"\"\"\n",
    "        self.fname = fname\n",
    "        self.filter_namespaces = filter_namespaces\n",
    "        self.filter_articles = filter_articles\n",
    "        self.metadata = False\n",
    "        if processes is None:\n",
    "            processes = max(1, multiprocessing.cpu_count() - 1)\n",
    "        self.processes = processes\n",
    "        self.lemmatize = lemmatize\n",
    "        self.tokenizer_func = tokenizer_func\n",
    "        self.article_min_tokens = article_min_tokens\n",
    "        self.token_min_len = token_min_len\n",
    "        self.token_max_len = token_max_len\n",
    "        self.lower = lower\n",
    "\n",
    "        if dictionary is None:\n",
    "            self.dictionary = Dictionary(self.get_texts())\n",
    "        else:\n",
    "            self.dictionary = dictionary\n",
    "\n",
    "    @property\n",
    "    def input(self):\n",
    "        return self.fname\n",
    "\n",
    "    def get_texts(self):\n",
    "        \"\"\"Iterate over the dump, yielding a list of tokens for each article that passed\n",
    "        the length and namespace filtering.\n",
    "\n",
    "        Uses multiprocessing internally to parallelize the work and process the dump more quickly.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        This iterates over the **texts**. If you want vectors, just use the standard corpus interface\n",
    "        instead of this method:\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        .. sourcecode:: pycon\n",
    "\n",
    "            >>> from gensim.test.utils import datapath\n",
    "            >>> from gensim.corpora import WikiCorpus\n",
    "            >>>\n",
    "            >>> path_to_wiki_dump = datapath(\"enwiki-latest-pages-articles1.xml-p000000010p000030302-shortened.bz2\")\n",
    "            >>>\n",
    "            >>> for vec in WikiCorpus(path_to_wiki_dump):\n",
    "            ...     pass\n",
    "\n",
    "        Yields\n",
    "        ------\n",
    "        list of str\n",
    "            If `metadata` is False, yield only list of token extracted from the article.\n",
    "        (list of str, (int, str))\n",
    "            List of tokens (extracted from the article), page id and article title otherwise.\n",
    "\n",
    "        \"\"\"\n",
    "        articles, articles_all = 0, 0\n",
    "        positions, positions_all = 0, 0\n",
    "\n",
    "        tokenization_params = (self.tokenizer_func, self.token_min_len, self.token_max_len, self.lower)\n",
    "        texts = (\n",
    "            (text, self.lemmatize, title, pageid, tokenization_params)\n",
    "            for title, text, pageid\n",
    "            in extract_pages(bz2.BZ2File(self.fname), self.filter_namespaces, self.filter_articles)\n",
    "        )\n",
    "        pool = multiprocessing.Pool(self.processes, init_to_ignore_interrupt)\n",
    "\n",
    "        try:\n",
    "            # process the corpus in smaller chunks of docs, because multiprocessing.Pool\n",
    "            # is dumb and would load the entire input into RAM at once...\n",
    "            for group in utils.chunkize(texts, chunksize=10 * self.processes, maxsize=1):\n",
    "                for tokens, title, pageid in pool.imap(_process_article, group):\n",
    "                    articles_all += 1\n",
    "                    positions_all += len(tokens)\n",
    "                    # article redirects and short stubs are pruned here\n",
    "                    if len(tokens) < self.article_min_tokens or \\\n",
    "                            any(title.startswith(ignore + ':') for ignore in IGNORED_NAMESPACES):\n",
    "                        continue\n",
    "                    articles += 1\n",
    "                    positions += len(tokens)\n",
    "                    if self.metadata:\n",
    "                        yield (tokens, (pageid, title))\n",
    "                    else:\n",
    "                        yield tokens\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            logger.warning(\n",
    "                \"user terminated iteration over Wikipedia corpus after %i documents with %i positions \"\n",
    "                \"(total %i articles, %i positions before pruning articles shorter than %i words)\",\n",
    "                articles, positions, articles_all, positions_all, self.article_min_tokens\n",
    "            )\n",
    "        except PicklingError as exc:\n",
    "            raise PicklingError(\n",
    "                f'Can not send filtering function {self.filter_articles} to multiprocessing, '\n",
    "                'make sure the function can be pickled.'\n",
    "            ) from exc\n",
    "        else:\n",
    "            logger.info(\n",
    "                \"finished iterating over Wikipedia corpus of %i documents with %i positions \"\n",
    "                \"(total %i articles, %i positions before pruning articles shorter than %i words)\",\n",
    "                articles, positions, articles_all, positions_all, self.article_min_tokens\n",
    "            )\n",
    "            self.length = articles  # cache corpus length\n",
    "        finally:\n",
    "            pool.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "from smart_open import open\n",
    "from xml.etree import cElementTree\n",
    "import json\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from gensim.corpora.wikicorpus import get_namespace, filter_wiki\n",
    "from gensim.scripts.segment_wiki import extract_page_xmls\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def find_sources(text, sources_translations, footnote_pattern, url_pattern):\n",
    "    sources = []\n",
    "    for footnote in footnote_pattern.findall(text):\n",
    "        footnote_title = list(footnote)[0].replace(\" \", \"\").lower()\n",
    "        footnote_content = list(footnote)[1].split(\"\\n*\")[1:]\n",
    "        if footnote_title in sources_translations:\n",
    "            for raw_source in footnote_content:\n",
    "                sources += url_pattern.findall(raw_source)\n",
    "    return sources\n",
    "\n",
    "\n",
    "def clean_sources(sources):\n",
    "    cleaned_sources = []\n",
    "    for source in sources:\n",
    "        parse = urlparse(source)\n",
    "        if (\n",
    "            (parse.path == \"\" or parse.path == \"/\")\n",
    "            and parse.params == \"\"\n",
    "            and parse.query == \"\"\n",
    "        ):\n",
    "            continue\n",
    "        cleaned_sources.append(source)\n",
    "    return cleaned_sources\n",
    "\n",
    "\n",
    "def get_pages_from_wiki_dump(wiki_dump_path, max_doc_count=0):\n",
    "\n",
    "    sources_translations = [\"quellen\", \"sources\", \"quelle\", \"source\"]\n",
    "\n",
    "    category_pattern = re.compile(\"\\[\\[(Category|Kategorie|Catégorie):(.*?)\\]\\]\")\n",
    "    footnote_pattern = re.compile(r\"==(.+?)==(.+?)\\n *\\n\", flags=re.DOTALL)\n",
    "    url_pattern = re.compile(r\"https?://[^\\s|\\]]+\")\n",
    "    blank_pattern = re.compile(r\"^\\s*$\")\n",
    "\n",
    "    with open(wiki_dump_path, \"rb\") as xml_fileobj:\n",
    "        page_xmls = extract_page_xmls(xml_fileobj)\n",
    "        i = 0\n",
    "        wrong_ns = 0\n",
    "        no_sources = 0\n",
    "        no_text = 0\n",
    "        redirect = 0\n",
    "\n",
    "        docs = []\n",
    "\n",
    "        for i, page_xml in enumerate(page_xmls):\n",
    "\n",
    "            elem = cElementTree.fromstring(page_xml)\n",
    "            filter_namespaces = (\"0\",)\n",
    "            namespace = get_namespace(elem.tag)\n",
    "            ns_mapping = {\"ns\": namespace}\n",
    "            text_path = \"./{%(ns)s}revision/{%(ns)s}text\" % ns_mapping\n",
    "            title_path = \"./{%(ns)s}title\" % ns_mapping\n",
    "            ns_path = \"./{%(ns)s}ns\" % ns_mapping\n",
    "\n",
    "            title = elem.find(title_path).text\n",
    "            \n",
    "            text = elem.find(text_path).text\n",
    "            \n",
    "            \n",
    "            \n",
    "            ns = elem.find(ns_path).text\n",
    "            if ns not in filter_namespaces:\n",
    "                wrong_ns += 1\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "\n",
    "                categories = [c for _, c in category_pattern.findall(text)]\n",
    "\n",
    "                sources = find_sources(\n",
    "                    text, sources_translations, footnote_pattern, url_pattern\n",
    "                )\n",
    "\n",
    "                cleaned_text = category_pattern.sub(\"\", text)\n",
    "                cleaned_text = footnote_pattern.sub(\"\", cleaned_text)\n",
    "                \n",
    "                # replace Wikipedia template links\n",
    "                cleaned_text = re.sub(r'\\{\\{w\\|(.*?)\\|(.*?)\\}\\}', r'\\2', cleaned_text)\n",
    "                \n",
    "                cleaned_text = filter_wiki(cleaned_text)\n",
    "                passages = [\n",
    "                    passage\n",
    "                    for passage in cleaned_text.split(\"\\n\\n\")\n",
    "                    if blank_pattern.match(passage) == None\n",
    "                ]\n",
    "\n",
    "                sources = clean_sources(sources)\n",
    "\n",
    "                if len(\" \".join(passages).split()) == 0:\n",
    "                    no_text += 1\n",
    "                    continue\n",
    "\n",
    "                if \"#REDIRECT\" in cleaned_text or \"#redirect\" in cleaned_text:\n",
    "                    redirect += 1\n",
    "                    continue\n",
    "\n",
    "                if sources == []:\n",
    "                    no_sources += 1\n",
    "                    continue\n",
    "\n",
    "                docs.append(\n",
    "                    {\n",
    "                        \"title\": title,\n",
    "                        \"text\": passages,\n",
    "                        \"categories\": categories,\n",
    "                        \"sources\": sources,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                if 0 < max_doc_count < len(docs):\n",
    "                    break\n",
    "            except (TypeError, ValueError) as e:\n",
    "                logger.error(f\"Cannot read page #{i} - {title}: {e}\")\n",
    "\n",
    "    print(\n",
    "        \"Pages read: {}\\nPages returned: {}\\nWrong namespace: {}\\nNo sources: {}\\nNo text: {}\\nRedirect: {}\".format(\n",
    "            i + 1, len(docs), wrong_ns, no_sources, no_text, redirect\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "def stats(json_path, csv_path, save_csv):\n",
    "    titles = []\n",
    "    num_words = []\n",
    "    num_sources = []\n",
    "    files = [\n",
    "        join(json_path, f)\n",
    "        for f in listdir(json_path)\n",
    "        if isfile(join(json_path, f)) and join(json_path, f)[-4:] == \"json\"\n",
    "    ]\n",
    "    for filename in files:\n",
    "        with open(filename) as json_file:\n",
    "            doc = json.load(json_file)\n",
    "        title = doc[\"title\"]\n",
    "        text = \" \".join(doc[\"text\"])\n",
    "        sources = doc[\"sources\"]\n",
    "\n",
    "        if len(text.split()) == 0:\n",
    "            print(title)\n",
    "            print(text)\n",
    "\n",
    "        titles.append(title)\n",
    "        num_words.append(len(text.split()))\n",
    "        num_sources.append(len(sources))\n",
    "\n",
    "    data = {\"title\": titles, \"num_words\": num_words, \"num_sources\": num_sources}\n",
    "    df = pd.DataFrame(data=data)\n",
    "    if save_csv:\n",
    "        df.to_csv(csv_path, index=False)\n",
    "    print(df.describe())\n",
    "\n",
    "\n",
    "def read_index(index_path):\n",
    "    with open(index_path, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    index = {}\n",
    "    for line in data.split(\"\\n\"):\n",
    "        elems = line.split(\"\\t\")\n",
    "        if len(elems) == 2:\n",
    "            index[elems[0]] = int(elems[1])\n",
    "    return index\n",
    "\n",
    "\n",
    "def write_index(index, index_path):\n",
    "    with open(index_path, \"w\") as f:\n",
    "        for k, v in index.items():\n",
    "            f.write(\"{}\\t{:06d}\\n\".format(k, v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages read: 1527\n",
      "Pages returned: 101\n",
      "Wrong namespace: 1188\n",
      "No sources: 99\n",
      "No text: 0\n",
      "Redirect: 139\n"
     ]
    }
   ],
   "source": [
    "docs = get_pages_from_wiki_dump(fp, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'President of China lunches with Brazilian President',\n",
       " 'text': [\"\\nHu Jintao, the President of the People's Republic of China had lunch today with the President of Brazil, Luiz Inácio Lula da Silva, at the ''Granja do Torto'', the President's country residence in the Brazilian Federal District. Lunch was a traditional Brazilian barbecue with different kinds of meat. \",\n",
       "  'Some Brazilian ministers were present at the event: Antonio Palocci (Economy), Eduardo Campos (Science and Technology), Roberto Rodrigues (Agriculture), Luiz Fernando Furlan (Development), Celso Amorim (Exterior Relations), Dilma Rousseff (Mines and Energy). Also present were Roger Agnelli (Vale do Rio Doce company president) and Eduardo Dutra (Petrobras, government oil company, president).',\n",
       "  \"This meeting is part of a new political economy agreement between Brazil and China where Brazil has recognized mainland China's market economy status, and China has promised to buy more Brazilian products.\"],\n",
       " 'categories': ['Politics and conflicts',\n",
       "  'South America',\n",
       "  'Asia',\n",
       "  'Brazil',\n",
       "  'China',\n",
       "  'India',\n",
       "  'Hu Jintao',\n",
       "  'Luiz Inácio Lula da Silva',\n",
       "  'Dilma Rousseff',\n",
       "  'Petrobras'],\n",
       " 'sources': ['http://br.news.yahoo.com/041113/25/p0en.html',\n",
       "  'http://web.archive.org/web/20051030032711/http://br.news.yahoo.com/041113/25/p0en.html',\n",
       "  'http://www.highbeam.com/doc/1P1-102429439.html',\n",
       "  'http://web.archive.org/web/20160421214654/https://www.highbeam.com/doc/1P1-102429439.html',\n",
       "  'http://news.bbc.co.uk/2/low/americas/4008499.stm',\n",
       "  'http://www.chinadaily.com.cn/english/doc/2004-05/24/content_333379.htm']}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{{date|November 13, 2004}}\n",
      "{{Brazil}}\n",
      "\n",
      "{{w|Hu Jintao|Hu Jintao}}, the {{w|President of the People's Republic of China|President}} of the [[People's Republic of China]] had lunch today with the {{w|President of Brazil|President}} of [[Brazil]], {{w|Luiz Inácio Lula da Silva|Luiz Inácio Lula da Silva}}, at the ''Granja do Torto'', the President's country residence in the {{w|Brazilian Federal District|Brazilian Federal District}}. Lunch was a traditional Brazilian {{w|barbecue|barbecue}} with different kinds of meat. \n",
      "\n",
      "Some Brazilian ministers were present at the event: {{w|Antonio Palocci|Antonio Palocci}} (Economy), {{w|pt:Eduardo Campos|Eduardo Campos}} ({{w|Ministry of Science and Technology (Brazil)|Science and Technology}}), {{w|João Roberto Rodrigues|Roberto Rodrigues}} (Agriculture), {{w|Luiz Fernando Furlan|Luiz Fernando Furlan}} (Development), {{w|Celso Amorim|Celso Amorim}} ({{w|Ministry of External Relations (Brazil)|Exterior Relations}}), {{w|Dilma Rousseff|Dilma Rousseff}} (Mines and Energy). Also present were {{w|pt:Roger Agnelli|Roger Agnelli}} ({{w|Vale (mining company)|Vale do Rio Doce}} company president) and Eduardo Dutra ({{w|Petrobras|Petrobras}}, government oil company, president).\n",
      "\n",
      "This meeting is part of a new {{w|political economy|political economy}} agreement between Brazil and China where Brazil has recognized mainland China's {{w|socialist market economy|market economy}} status, and China has promised to buy more {{w|economy of Brazil|Brazilian products}}.\n",
      "\n",
      "{{haveyoursay}}\n",
      "== Sources ==\n",
      "{{wikipedia|Workers' Party (Brazil)|Brazilian Workers's Party}}\n",
      "*{{source\n",
      " | url = http://br.news.yahoo.com/041113/25/p0en.html\n",
      " | title = Presidente da China almoça churrasco com Lula \n",
      " | author = {{w|Agência Estado}}\n",
      " | pub = Yahoo! Notícias\n",
      " | date = November 13, 2004\n",
      " | lang = pt \n",
      " | brokenURL           = true\n",
      " | archiveurl          = http://web.archive.org/web/20051030032711/http://br.news.yahoo.com/041113/25/p0en.html\n",
      " | archivedescription  = archived on archive.org, 2005-10-30\n",
      "}} \n",
      "*{{source\n",
      " |url=http://www.highbeam.com/doc/1P1-102429439.html\n",
      " |title=Chinese president treated to typical Brazilian barbecue\n",
      " |author={{w|Associated Press}}\n",
      " |pub=HighBeam Research\n",
      " |date=November 13, 2004\n",
      "| brokenURL           = true\n",
      "| archiveurl          = http://web.archive.org/web/20160421214654/https://www.highbeam.com/doc/1P1-102429439.html\n",
      "| archivedescription  = archived on archive.org, 2016-04-21}}\n",
      "*{{source|url=http://news.bbc.co.uk/2/low/americas/4008499.stm\n",
      " |title=Brazil backs China on trade bid\n",
      " |author=\n",
      " |pub=BBC News\n",
      " |date=November 12, 2004}}\n",
      "*{{source|url=http://www.chinadaily.com.cn/english/doc/2004-05/24/content_333379.htm\n",
      " |title=Brazil sees market economy in China\n",
      " |author=\n",
      " |pub=China Daily\n",
      " |date=May 24, 2004}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source = '''\n",
    "\n",
    "{{date|November 13, 2004}}\n",
    "{{Brazil}}\n",
    "\n",
    "{{w|Hu Jintao|Hu Jintao}}, the {{w|President of the People's Republic of China|President}} of the [[People's Republic of China]] had lunch today with the {{w|President of Brazil|President}} of [[Brazil]], {{w|Luiz Inácio Lula da Silva|Luiz Inácio Lula da Silva}}, at the ''Granja do Torto'', the President's country residence in the {{w|Brazilian Federal District|Brazilian Federal District}}. Lunch was a traditional Brazilian {{w|barbecue|barbecue}} with different kinds of meat. \n",
    "\n",
    "Some Brazilian ministers were present at the event: {{w|Antonio Palocci|Antonio Palocci}} (Economy), {{w|pt:Eduardo Campos|Eduardo Campos}} ({{w|Ministry of Science and Technology (Brazil)|Science and Technology}}), {{w|João Roberto Rodrigues|Roberto Rodrigues}} (Agriculture), {{w|Luiz Fernando Furlan|Luiz Fernando Furlan}} (Development), {{w|Celso Amorim|Celso Amorim}} ({{w|Ministry of External Relations (Brazil)|Exterior Relations}}), {{w|Dilma Rousseff|Dilma Rousseff}} (Mines and Energy). Also present were {{w|pt:Roger Agnelli|Roger Agnelli}} ({{w|Vale (mining company)|Vale do Rio Doce}} company president) and Eduardo Dutra ({{w|Petrobras|Petrobras}}, government oil company, president).\n",
    "\n",
    "This meeting is part of a new {{w|political economy|political economy}} agreement between Brazil and China where Brazil has recognized mainland China's {{w|socialist market economy|market economy}} status, and China has promised to buy more {{w|economy of Brazil|Brazilian products}}.\n",
    "\n",
    "{{haveyoursay}}\n",
    "== Sources ==\n",
    "{{wikipedia|Workers' Party (Brazil)|Brazilian Workers's Party}}\n",
    "*{{source\n",
    " | url = http://br.news.yahoo.com/041113/25/p0en.html\n",
    " | title = Presidente da China almoça churrasco com Lula \n",
    " | author = {{w|Agência Estado}}\n",
    " | pub = Yahoo! Notícias\n",
    " | date = November 13, 2004\n",
    " | lang = pt \n",
    " | brokenURL           = true\n",
    " | archiveurl          = http://web.archive.org/web/20051030032711/http://br.news.yahoo.com/041113/25/p0en.html\n",
    " | archivedescription  = archived on archive.org, 2005-10-30\n",
    "}} \n",
    "*{{source\n",
    " |url=http://www.highbeam.com/doc/1P1-102429439.html\n",
    " |title=Chinese president treated to typical Brazilian barbecue\n",
    " |author={{w|Associated Press}}\n",
    " |pub=HighBeam Research\n",
    " |date=November 13, 2004\n",
    "| brokenURL           = true\n",
    "| archiveurl          = http://web.archive.org/web/20160421214654/https://www.highbeam.com/doc/1P1-102429439.html\n",
    "| archivedescription  = archived on archive.org, 2016-04-21}}\n",
    "*{{source|url=http://news.bbc.co.uk/2/low/americas/4008499.stm\n",
    " |title=Brazil backs China on trade bid\n",
    " |author=\n",
    " |pub=BBC News\n",
    " |date=November 12, 2004}}\n",
    "*{{source|url=http://www.chinadaily.com.cn/english/doc/2004-05/24/content_333379.htm\n",
    " |title=Brazil sees market economy in China\n",
    " |author=\n",
    " |pub=China Daily\n",
    " |date=May 24, 2004}}\n",
    "'''\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{{date|November 13, 2004}}\n",
      "{{Brazil}}\n",
      "\n",
      "Hu Jintao, the President of the [[People's Republic of China]] had lunch today with the President of [[Brazil]], Luiz Inácio Lula da Silva, at the ''Granja do Torto'', the President's country residence in the Brazilian Federal District. Lunch was a traditional Brazilian barbecue with different kinds of meat. \n",
      "\n",
      "Some Brazilian ministers were present at the event: Antonio Palocci (Economy), Eduardo Campos (Science and Technology), Roberto Rodrigues (Agriculture), Luiz Fernando Furlan (Development), Celso Amorim (Exterior Relations), Dilma Rousseff (Mines and Energy). Also present were Roger Agnelli (Vale do Rio Doce company president) and Eduardo Dutra (Petrobras, government oil company, president).\n",
      "\n",
      "This meeting is part of a new political economy agreement between Brazil and China where Brazil has recognized mainland China's market economy status, and China has promised to buy more Brazilian products.\n",
      "\n",
      "{{haveyoursay}}\n",
      "== Sources ==\n",
      "{{wikipedia|Workers' Party (Brazil)|Brazilian Workers's Party}}\n",
      "*{{source\n",
      " | url = http://br.news.yahoo.com/041113/25/p0en.html\n",
      " | title = Presidente da China almoça churrasco com Lula \n",
      " | author = {{w|Agência Estado}}\n",
      " | pub = Yahoo! Notícias\n",
      " | date = November 13, 2004\n",
      " | lang = pt \n",
      " | brokenURL           = true\n",
      " | archiveurl          = http://web.archive.org/web/20051030032711/http://br.news.yahoo.com/041113/25/p0en.html\n",
      " | archivedescription  = archived on archive.org, 2005-10-30\n",
      "}} \n",
      "*{{source\n",
      " |url=http://www.highbeam.com/doc/1P1-102429439.html\n",
      " |title=Chinese president treated to typical Brazilian barbecue\n",
      " |author={{w|Associated Press}}\n",
      " |pub=HighBeam Research\n",
      " |date=November 13, 2004\n",
      "| brokenURL           = true\n",
      "| archiveurl          = http://web.archive.org/web/20160421214654/https://www.highbeam.com/doc/1P1-102429439.html\n",
      "| archivedescription  = archived on archive.org, 2016-04-21}}\n",
      "*{{source|url=http://news.bbc.co.uk/2/low/americas/4008499.stm\n",
      " |title=Brazil backs China on trade bid\n",
      " |author=\n",
      " |pub=BBC News\n",
      " |date=November 12, 2004}}\n",
      "*{{source|url=http://www.chinadaily.com.cn/english/doc/2004-05/24/content_333379.htm\n",
      " |title=Brazil sees market economy in China\n",
      " |author=\n",
      " |pub=China Daily\n",
      " |date=May 24, 2004}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(r'\\{\\{w\\|(.*?)\\|(.*?)\\}\\}', r'\\2', source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
