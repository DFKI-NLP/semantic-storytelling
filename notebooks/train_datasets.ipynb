{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aquatic-australia",
   "metadata": {},
   "source": [
    "# Parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e62acc",
   "metadata": {},
   "source": [
    "## Text Segment Input Strategies\n",
    "Different Input Strategies used for Language Model Input. Input of Language Model is pair ´(A,B)´ with ´A´ and ´B´ of type:\n",
    "1. Sentence (S)\n",
    "2. Title + Sentence (TS)\n",
    "3. Title + Sentence + Date (TSD)\n",
    "4. Sentence + Title + Date (STD)\n",
    "\n",
    "*Sentence* is the extracted Sentence from News Article, *Title* is the News Article Title and *Date* is the publishing date of the article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d6e2e1",
   "metadata": {},
   "source": [
    "### Set Input Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a661cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_strategy = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9143b5",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c9780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_checkpoint = 'microsoft/deberta-base'\n",
    "batch_size = 2\n",
    "metric_name = \"accuracy\"\n",
    "num_epoch = 10\n",
    "\n",
    "# Fold\n",
    "num_folds = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c0fa36",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0b80045",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"none\", \"attribution\", \"causal\", \"conditional\", \"contrast\", \"description\", \"equivalence\", \"fulfillment\", \"identity\", \"purpose\", \"summary\", \"temporal\"]\n",
    "\n",
    "# Use subset of labels (defined below)\n",
    "subset_labels = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c65d072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if subset_labels:\n",
    "    labels = [\"none\", \"causal\", \"contrast\", \"equivalence\", \"identity\", \"temporal\", \"others\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58120eb2",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a648343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Paths\n",
    "import_path = f\"data/export/{input_strategy}\"\n",
    "if subset_labels:\n",
    "    import_path = f\"data/export_subset/{input_strategy}\"\n",
    "\n",
    "# Export Paths\n",
    "export_path_model = \"data/model\"\n",
    "export_path_eval = \"data/eval\"\n",
    "\n",
    "if subset_labels:\n",
    "    export_path_model += \"_subset\"\n",
    "    export_path_eval += \"_subset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfcfcb1",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d51af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8188a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_fold(path, fold):\n",
    "    train = pd.read_csv(f\"{path}/train.{fold}.csv\")\n",
    "    test = pd.read_csv(f\"{path}/test.{fold}.csv\")\n",
    "    train_origin = train[\"origin\"].tolist()\n",
    "    train_target = train[\"target\"].tolist()\n",
    "    train_labels = train[\"label\"].tolist()\n",
    "    test_origin = test[\"origin\"].tolist()\n",
    "    test_target = test[\"target\"].tolist()\n",
    "    test_labels = test[\"label\"].tolist()\n",
    "    return train_origin, train_target, train_labels, test_origin, test_target, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1845b3",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a464f6",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "verbal-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import collections\n",
    "\n",
    "#classification_threshold = 0.\n",
    "\n",
    "def flatten(d, parent_key='', sep='__'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, collections.MutableMapping):\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    global labels\n",
    "    predictions, true_labels = eval_pred\n",
    "    # take most probable guess\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    return flatten(classification_report(\n",
    "        y_true=true_labels,\n",
    "        y_pred=predictions,\n",
    "        target_names=labels,\n",
    "        zero_division=0,\n",
    "        output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "opened-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "#flatten(classification_report(\n",
    "#    y_true=[0,1,2,3,4,5,6,7,8,9,10,11,12],\n",
    "#    y_pred=[0,0,0,1,3,0,0,0,0,0,0,0,0],\n",
    "#    target_names=labels,\n",
    "#    zero_division=0,\n",
    "#    output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-taste",
   "metadata": {},
   "source": [
    "## Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "referenced-wichita",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"semantic-test\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epoch,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-wedding",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sensitive-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, DebertaTokenizerFast\n",
    "if \"microsoft\" in model_checkpoint:\n",
    "    tokenizer = DebertaTokenizerFast.from_pretrained(model_checkpoint)\n",
    "else:\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b7ee1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cf32517",
   "metadata": {},
   "source": [
    "## Print Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efbd918e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbe4fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(origin_list, target_list, label_list, encodings, num_examples=10):\n",
    "    global labels\n",
    "    assert num_examples <= len(origin_list), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(origin_list)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(origin_list)-1)\n",
    "        picks.append(pick)\n",
    "    data = []\n",
    "    for n in picks:\n",
    "        data.append([n, origin_list[n], labels[label_list[n]], target_list[n], encodings.input_ids[n], encodings.token_type_ids[n], encodings.attention_mask[n]])\n",
    "    df = pd.DataFrame(data, columns=['index', 'Origin', 'Label', 'Target', 'Input_ids', 'Token_type_ids', 'Attention_mask'])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cb93df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_random_elements(train_origin, train_target, train_labels, train_encodings)\n",
    "# Output adjustet to Folds\n",
    "#show_random_elements(k_fold_origin[0][0], k_fold_target[0][0], k_fold_labels[0][0], train_encodings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-lightweight",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "impressive-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-destiny",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-winning",
   "metadata": {},
   "source": [
    "## Train & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "environmental-medication",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'config', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18760' max='18760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18760/18760 42:34, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Attribution  Precision</th>\n",
       "      <th>Attribution  Recall</th>\n",
       "      <th>Attribution  F1-score</th>\n",
       "      <th>Attribution  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Conditional  Precision</th>\n",
       "      <th>Conditional  Recall</th>\n",
       "      <th>Conditional  F1-score</th>\n",
       "      <th>Conditional  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Description  Precision</th>\n",
       "      <th>Description  Recall</th>\n",
       "      <th>Description  F1-score</th>\n",
       "      <th>Description  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Fulfillment  Precision</th>\n",
       "      <th>Fulfillment  Recall</th>\n",
       "      <th>Fulfillment  F1-score</th>\n",
       "      <th>Fulfillment  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Purpose  Precision</th>\n",
       "      <th>Purpose  Recall</th>\n",
       "      <th>Purpose  F1-score</th>\n",
       "      <th>Purpose  Support</th>\n",
       "      <th>Summary  Precision</th>\n",
       "      <th>Summary  Recall</th>\n",
       "      <th>Summary  F1-score</th>\n",
       "      <th>Summary  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.368700</td>\n",
       "      <td>1.183027</td>\n",
       "      <td>0.675087</td>\n",
       "      <td>0.976071</td>\n",
       "      <td>0.798146</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.295082</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.039735</td>\n",
       "      <td>147</td>\n",
       "      <td>0.643485</td>\n",
       "      <td>0.141485</td>\n",
       "      <td>0.109826</td>\n",
       "      <td>0.094414</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.534915</td>\n",
       "      <td>0.643485</td>\n",
       "      <td>0.531060</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.147100</td>\n",
       "      <td>1.082427</td>\n",
       "      <td>0.788222</td>\n",
       "      <td>0.876574</td>\n",
       "      <td>0.830054</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.236364</td>\n",
       "      <td>0.238532</td>\n",
       "      <td>0.237443</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.338235</td>\n",
       "      <td>0.273810</td>\n",
       "      <td>0.302632</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.372671</td>\n",
       "      <td>147</td>\n",
       "      <td>0.645084</td>\n",
       "      <td>0.154960</td>\n",
       "      <td>0.158090</td>\n",
       "      <td>0.155334</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.586331</td>\n",
       "      <td>0.645084</td>\n",
       "      <td>0.613567</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.982800</td>\n",
       "      <td>1.096252</td>\n",
       "      <td>0.818832</td>\n",
       "      <td>0.865239</td>\n",
       "      <td>0.841396</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.394958</td>\n",
       "      <td>0.431193</td>\n",
       "      <td>0.412281</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.506550</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.392000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.360294</td>\n",
       "      <td>147</td>\n",
       "      <td>0.675460</td>\n",
       "      <td>0.194679</td>\n",
       "      <td>0.207886</td>\n",
       "      <td>0.194071</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.634720</td>\n",
       "      <td>0.675460</td>\n",
       "      <td>0.650330</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.921800</td>\n",
       "      <td>1.105811</td>\n",
       "      <td>0.895863</td>\n",
       "      <td>0.790932</td>\n",
       "      <td>0.840134</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.430380</td>\n",
       "      <td>0.623853</td>\n",
       "      <td>0.509363</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.366516</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.440217</td>\n",
       "      <td>147</td>\n",
       "      <td>0.676259</td>\n",
       "      <td>0.243773</td>\n",
       "      <td>0.273380</td>\n",
       "      <td>0.254859</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.699831</td>\n",
       "      <td>0.676259</td>\n",
       "      <td>0.682014</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>1.335044</td>\n",
       "      <td>0.847747</td>\n",
       "      <td>0.876574</td>\n",
       "      <td>0.861920</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.596330</td>\n",
       "      <td>0.593607</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.383562</td>\n",
       "      <td>41</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>13</td>\n",
       "      <td>0.721519</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.699387</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.364286</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.355401</td>\n",
       "      <td>147</td>\n",
       "      <td>0.718625</td>\n",
       "      <td>0.279768</td>\n",
       "      <td>0.307810</td>\n",
       "      <td>0.286034</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.700946</td>\n",
       "      <td>0.718625</td>\n",
       "      <td>0.708039</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.450800</td>\n",
       "      <td>1.520255</td>\n",
       "      <td>0.873016</td>\n",
       "      <td>0.831234</td>\n",
       "      <td>0.851613</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.477987</td>\n",
       "      <td>0.697248</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>109</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.345679</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.459016</td>\n",
       "      <td>41</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>13</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.679739</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.382353</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.272109</td>\n",
       "      <td>0.304183</td>\n",
       "      <td>147</td>\n",
       "      <td>0.697042</td>\n",
       "      <td>0.312204</td>\n",
       "      <td>0.331487</td>\n",
       "      <td>0.296953</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.710620</td>\n",
       "      <td>0.697042</td>\n",
       "      <td>0.696444</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.344900</td>\n",
       "      <td>1.579269</td>\n",
       "      <td>0.867089</td>\n",
       "      <td>0.862720</td>\n",
       "      <td>0.864899</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.584615</td>\n",
       "      <td>0.697248</td>\n",
       "      <td>0.635983</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.611765</td>\n",
       "      <td>41</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.794521</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.738854</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.354651</td>\n",
       "      <td>0.414966</td>\n",
       "      <td>0.382445</td>\n",
       "      <td>147</td>\n",
       "      <td>0.736211</td>\n",
       "      <td>0.321538</td>\n",
       "      <td>0.339707</td>\n",
       "      <td>0.329192</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.725853</td>\n",
       "      <td>0.736211</td>\n",
       "      <td>0.729964</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.225600</td>\n",
       "      <td>1.634171</td>\n",
       "      <td>0.895461</td>\n",
       "      <td>0.819899</td>\n",
       "      <td>0.856016</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.770642</td>\n",
       "      <td>0.680162</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>41</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>13</td>\n",
       "      <td>0.861538</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.751678</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.368687</td>\n",
       "      <td>0.496599</td>\n",
       "      <td>0.423188</td>\n",
       "      <td>147</td>\n",
       "      <td>0.728217</td>\n",
       "      <td>0.354746</td>\n",
       "      <td>0.365780</td>\n",
       "      <td>0.355257</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.758050</td>\n",
       "      <td>0.728217</td>\n",
       "      <td>0.739000</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.162600</td>\n",
       "      <td>1.710331</td>\n",
       "      <td>0.879690</td>\n",
       "      <td>0.856423</td>\n",
       "      <td>0.867900</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.770642</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>109</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>14</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>41</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>13</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.779874</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.414013</td>\n",
       "      <td>0.442177</td>\n",
       "      <td>0.427632</td>\n",
       "      <td>147</td>\n",
       "      <td>0.753797</td>\n",
       "      <td>0.388997</td>\n",
       "      <td>0.392621</td>\n",
       "      <td>0.386914</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.756072</td>\n",
       "      <td>0.753797</td>\n",
       "      <td>0.753243</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>1.701412</td>\n",
       "      <td>0.873750</td>\n",
       "      <td>0.880353</td>\n",
       "      <td>0.877039</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.779817</td>\n",
       "      <td>0.726496</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>0.756098</td>\n",
       "      <td>41</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>13</td>\n",
       "      <td>0.828947</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.417722</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>0.432787</td>\n",
       "      <td>147</td>\n",
       "      <td>0.768185</td>\n",
       "      <td>0.382835</td>\n",
       "      <td>0.376591</td>\n",
       "      <td>0.377872</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.759364</td>\n",
       "      <td>0.768185</td>\n",
       "      <td>0.763045</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-c906eaaad80e>:10: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  if isinstance(v, collections.MutableMapping):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='626' max='626' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [626/626 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.768185451638689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'config', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18760' max='18760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18760/18760 42:18, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Attribution  Precision</th>\n",
       "      <th>Attribution  Recall</th>\n",
       "      <th>Attribution  F1-score</th>\n",
       "      <th>Attribution  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Conditional  Precision</th>\n",
       "      <th>Conditional  Recall</th>\n",
       "      <th>Conditional  F1-score</th>\n",
       "      <th>Conditional  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Description  Precision</th>\n",
       "      <th>Description  Recall</th>\n",
       "      <th>Description  F1-score</th>\n",
       "      <th>Description  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Fulfillment  Precision</th>\n",
       "      <th>Fulfillment  Recall</th>\n",
       "      <th>Fulfillment  F1-score</th>\n",
       "      <th>Fulfillment  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Purpose  Precision</th>\n",
       "      <th>Purpose  Recall</th>\n",
       "      <th>Purpose  F1-score</th>\n",
       "      <th>Purpose  Support</th>\n",
       "      <th>Summary  Precision</th>\n",
       "      <th>Summary  Recall</th>\n",
       "      <th>Summary  F1-score</th>\n",
       "      <th>Summary  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.474200</td>\n",
       "      <td>1.453677</td>\n",
       "      <td>0.634692</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.776528</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>148</td>\n",
       "      <td>0.634692</td>\n",
       "      <td>0.052891</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.064711</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.402834</td>\n",
       "      <td>0.634692</td>\n",
       "      <td>0.492856</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.220500</td>\n",
       "      <td>1.204238</td>\n",
       "      <td>0.775974</td>\n",
       "      <td>0.903023</td>\n",
       "      <td>0.834692</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.225664</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.329032</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.195946</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>148</td>\n",
       "      <td>0.652278</td>\n",
       "      <td>0.190901</td>\n",
       "      <td>0.158049</td>\n",
       "      <td>0.144490</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.618226</td>\n",
       "      <td>0.652278</td>\n",
       "      <td>0.607879</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.002500</td>\n",
       "      <td>1.320969</td>\n",
       "      <td>0.788260</td>\n",
       "      <td>0.947103</td>\n",
       "      <td>0.860412</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>148</td>\n",
       "      <td>0.689049</td>\n",
       "      <td>0.235596</td>\n",
       "      <td>0.209186</td>\n",
       "      <td>0.210495</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.620354</td>\n",
       "      <td>0.689049</td>\n",
       "      <td>0.639498</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.876600</td>\n",
       "      <td>1.449865</td>\n",
       "      <td>0.808266</td>\n",
       "      <td>0.886650</td>\n",
       "      <td>0.845646</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>108</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.430108</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.451977</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.398305</td>\n",
       "      <td>0.317568</td>\n",
       "      <td>0.353383</td>\n",
       "      <td>148</td>\n",
       "      <td>0.692246</td>\n",
       "      <td>0.285099</td>\n",
       "      <td>0.253284</td>\n",
       "      <td>0.247600</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.653665</td>\n",
       "      <td>0.692246</td>\n",
       "      <td>0.666544</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.670100</td>\n",
       "      <td>1.745064</td>\n",
       "      <td>0.842371</td>\n",
       "      <td>0.841310</td>\n",
       "      <td>0.841840</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.398773</td>\n",
       "      <td>0.601852</td>\n",
       "      <td>0.479705</td>\n",
       "      <td>108</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>14</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>42</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>14</td>\n",
       "      <td>0.456000</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.277027</td>\n",
       "      <td>0.320312</td>\n",
       "      <td>148</td>\n",
       "      <td>0.684253</td>\n",
       "      <td>0.330608</td>\n",
       "      <td>0.281047</td>\n",
       "      <td>0.280006</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.676382</td>\n",
       "      <td>0.684253</td>\n",
       "      <td>0.673161</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.433200</td>\n",
       "      <td>1.655115</td>\n",
       "      <td>0.868661</td>\n",
       "      <td>0.841310</td>\n",
       "      <td>0.854766</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>108</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>14</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.561404</td>\n",
       "      <td>42</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>14</td>\n",
       "      <td>0.617284</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.388535</td>\n",
       "      <td>0.412162</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>148</td>\n",
       "      <td>0.721023</td>\n",
       "      <td>0.338349</td>\n",
       "      <td>0.342265</td>\n",
       "      <td>0.327686</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.717408</td>\n",
       "      <td>0.721023</td>\n",
       "      <td>0.715612</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.367400</td>\n",
       "      <td>1.781421</td>\n",
       "      <td>0.864516</td>\n",
       "      <td>0.843829</td>\n",
       "      <td>0.854047</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.682243</td>\n",
       "      <td>0.675926</td>\n",
       "      <td>0.679070</td>\n",
       "      <td>108</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>14</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>42</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>14</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.371795</td>\n",
       "      <td>0.391892</td>\n",
       "      <td>0.381579</td>\n",
       "      <td>148</td>\n",
       "      <td>0.721823</td>\n",
       "      <td>0.333067</td>\n",
       "      <td>0.346407</td>\n",
       "      <td>0.332457</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.719334</td>\n",
       "      <td>0.721823</td>\n",
       "      <td>0.718509</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.208900</td>\n",
       "      <td>1.914872</td>\n",
       "      <td>0.881579</td>\n",
       "      <td>0.843829</td>\n",
       "      <td>0.862291</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.436620</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.548673</td>\n",
       "      <td>42</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.654762</td>\n",
       "      <td>0.639535</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.413580</td>\n",
       "      <td>0.452703</td>\n",
       "      <td>0.432258</td>\n",
       "      <td>148</td>\n",
       "      <td>0.734612</td>\n",
       "      <td>0.337933</td>\n",
       "      <td>0.352687</td>\n",
       "      <td>0.338273</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.735860</td>\n",
       "      <td>0.734612</td>\n",
       "      <td>0.732735</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.157700</td>\n",
       "      <td>1.816871</td>\n",
       "      <td>0.877102</td>\n",
       "      <td>0.853904</td>\n",
       "      <td>0.865348</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>108</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>14</td>\n",
       "      <td>0.434211</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.559322</td>\n",
       "      <td>42</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>14</td>\n",
       "      <td>0.648936</td>\n",
       "      <td>0.726190</td>\n",
       "      <td>0.685393</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.378378</td>\n",
       "      <td>0.394366</td>\n",
       "      <td>148</td>\n",
       "      <td>0.740208</td>\n",
       "      <td>0.341746</td>\n",
       "      <td>0.372554</td>\n",
       "      <td>0.351992</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.738082</td>\n",
       "      <td>0.740208</td>\n",
       "      <td>0.736956</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.093300</td>\n",
       "      <td>1.833759</td>\n",
       "      <td>0.864899</td>\n",
       "      <td>0.862720</td>\n",
       "      <td>0.863808</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.690265</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>108</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>14</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>42</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>14</td>\n",
       "      <td>0.691358</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.678788</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.391892</td>\n",
       "      <td>0.391892</td>\n",
       "      <td>0.391892</td>\n",
       "      <td>148</td>\n",
       "      <td>0.741007</td>\n",
       "      <td>0.347646</td>\n",
       "      <td>0.361958</td>\n",
       "      <td>0.351029</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.733029</td>\n",
       "      <td>0.741007</td>\n",
       "      <td>0.735760</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='626' max='626' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [626/626 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7410071942446043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'config', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18760' max='18760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18760/18760 42:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Attribution  Precision</th>\n",
       "      <th>Attribution  Recall</th>\n",
       "      <th>Attribution  F1-score</th>\n",
       "      <th>Attribution  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Conditional  Precision</th>\n",
       "      <th>Conditional  Recall</th>\n",
       "      <th>Conditional  F1-score</th>\n",
       "      <th>Conditional  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Description  Precision</th>\n",
       "      <th>Description  Recall</th>\n",
       "      <th>Description  F1-score</th>\n",
       "      <th>Description  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Fulfillment  Precision</th>\n",
       "      <th>Fulfillment  Recall</th>\n",
       "      <th>Fulfillment  F1-score</th>\n",
       "      <th>Fulfillment  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Purpose  Precision</th>\n",
       "      <th>Purpose  Recall</th>\n",
       "      <th>Purpose  F1-score</th>\n",
       "      <th>Purpose  Support</th>\n",
       "      <th>Summary  Precision</th>\n",
       "      <th>Summary  Recall</th>\n",
       "      <th>Summary  F1-score</th>\n",
       "      <th>Summary  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.510500</td>\n",
       "      <td>1.409996</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.776908</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.052933</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.064742</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.403479</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.493492</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.495300</td>\n",
       "      <td>1.509378</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.776908</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.052933</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.064742</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.403479</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.493492</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.476500</td>\n",
       "      <td>1.345837</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.776908</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.052933</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.064742</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.403479</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.493492</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.656200</td>\n",
       "      <td>1.762547</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.776908</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.052933</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.064742</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.403479</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.493492</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.503000</td>\n",
       "      <td>1.525919</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.776908</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.052933</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.064742</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.403479</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.493492</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.657400</td>\n",
       "      <td>1.703186</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.776908</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.052933</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.064742</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.403479</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.493492</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.564400</td>\n",
       "      <td>1.422323</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.776908</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.052933</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.064742</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.403479</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.493492</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.452100</td>\n",
       "      <td>1.508841</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.776908</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.052933</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.064742</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.403479</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.493492</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.615100</td>\n",
       "      <td>1.516856</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.776908</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.052933</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.064742</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.403479</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.493492</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.553400</td>\n",
       "      <td>1.535430</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.776908</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.052933</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.064742</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.403479</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.493492</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'config', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='18760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    2/18760 : < :, Epoch 0.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.76 GiB total capacity; 9.23 GiB already allocated; 4.25 MiB free; 9.64 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9b8d034b94ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# train & evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1270\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1732\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1734\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1766\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1767\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m         outputs = self.deberta(\n\u001b[0m\u001b[1;32m   1166\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    938\u001b[0m         )\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    941\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             hidden_states = layer_module(\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mnext_kv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, return_att, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     ):\n\u001b[0;32m--> 332\u001b[0;31m         attention_output = self.attention(\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, return_att, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     ):\n\u001b[0;32m--> 265\u001b[0;31m         self_output = self.self(\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, return_att, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelative_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mrel_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             \u001b[0mrel_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisentangled_att_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrel_att\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py\u001b[0m in \u001b[0;36mdisentangled_att_bias\u001b[0;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0mc2p_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelative_pos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0matt_span\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_span\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0mc2p_att\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc2p_att\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc2p_dynamic_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc2p_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc2p_att\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# position->content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.76 GiB total capacity; 9.23 GiB already allocated; 4.25 MiB free; 9.64 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "num_labels = len(labels)\n",
    "models = []\n",
    "\n",
    "for i in range(num_folds):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "    # import Fold data\n",
    "    train_origin, train_target, train_labels, test_origin, test_target, test_labels = import_fold(import_path, i)\n",
    "    # tokenize\n",
    "    train_encodings = tokenizer(train_origin, train_target, truncation=True, padding=True, return_token_type_ids=True)\n",
    "    test_encodings = tokenizer(test_origin, test_target, truncation=True, padding=True, return_token_type_ids=True)\n",
    "    # dataset creation\n",
    "    train_dataset = SemanticDataset(train_encodings, train_labels)\n",
    "    test_dataset = SemanticDataset(test_encodings, test_labels)\n",
    "    # create Trainer\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    # train & evaluate\n",
    "    trainer.train()\n",
    "    ev = trainer.evaluate(test_dataset)\n",
    "    acc = ev[\"eval_accuracy\"]\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    result.append(ev)\n",
    "    models.append(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e39bf6",
   "metadata": {},
   "source": [
    "## Interpret evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be62939",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(data):\n",
    "    \"\"\"Return the sample arithmetic mean of data.\"\"\"\n",
    "    n = len(data)\n",
    "    if n < 1:\n",
    "        raise ValueError('mean requires at least one data point')\n",
    "    return sum(data)/n # in Python 2 use sum(data)/float(n)\n",
    "\n",
    "def _ss(data):\n",
    "    \"\"\"Return sum of square deviations of sequence data.\"\"\"\n",
    "    c = mean(data)\n",
    "    ss = sum((x-c)**2 for x in data)\n",
    "    return ss\n",
    "\n",
    "def stddev(data, ddof=0):\n",
    "    \"\"\"Calculates the population standard deviation\n",
    "    by default; specify ddof=1 to compute the sample\n",
    "    standard deviation.\"\"\"\n",
    "    n = len(data)\n",
    "    if n < 2:\n",
    "        raise ValueError('variance requires at least two data points')\n",
    "    ss = _ss(data)\n",
    "    pvar = ss/(n-ddof)\n",
    "    return pvar**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d41df",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df0f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dad3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_to_regular_dict(result):\n",
    "    output_dict = {}\n",
    "    count = 0\n",
    "    for eval_item in result:\n",
    "        for key in eval_item:\n",
    "            if count == 0:\n",
    "              output_dict[key] = [float(eval_item[key])]\n",
    "            else:\n",
    "              output_dict[key].append(eval_item[key]) \n",
    "        count += 1\n",
    "    return output_dict\n",
    "            \n",
    "eval_dict = transform_to_regular_dict(result)\n",
    "eval_df = pd.DataFrame(eval_dict)\n",
    "\n",
    "def add_mean_std_row(df):\n",
    "    row_mean = []\n",
    "    row_std = []\n",
    "    for column in df:\n",
    "        row_mean.append(mean(df[column]))\n",
    "        row_std.append(stddev(df[column], ddof=1))\n",
    "    df = df.append(pd.DataFrame([row_mean], columns=df.columns), ignore_index=True)\n",
    "    df = df.append(pd.DataFrame([row_std], columns=df.columns), ignore_index=True)\n",
    "    # add better readable Index\n",
    "    df[\"fold\"] = [\"1\", \"2\", \"3\", \"4\", \"avg\", \"std\"]\n",
    "    df = df.set_index(\"fold\")\n",
    "    return df\n",
    "\n",
    "eval_df = add_mean_std_row(eval_df)\n",
    "display(HTML(eval_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad0e60",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ecf938",
   "metadata": {},
   "source": [
    "## Check Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56114a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2df57efb",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed76e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for model in models:\n",
    "    path = f\"{export_path_model}/{input_strategy}/{model_checkpoint.replace(r'/', '-')}/epoch_{num_epoch}/fold_{count}\"\n",
    "    # Check if Path exists\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    model.save_model(path)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd633be",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a300ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Path exists\n",
    "path = f\"{export_path_eval}/{input_strategy}/\"\n",
    "Path(path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f2271",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_csv(f\"{path}/{model_checkpoint.replace(r'/', '-')}_epoch_{num_epoch}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f09b986",
   "metadata": {},
   "source": [
    "## Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ef9e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Open the file in append & read mode ('a+')\n",
    "with open(\"logs.txt\", \"a+\") as file_object:\n",
    "    # Move read cursor to the start of file.\n",
    "    file_object.seek(0)\n",
    "    # If file is not empty then append '\\n'\n",
    "    data = file_object.read(100)\n",
    "    if len(data) > 0 :\n",
    "        file_object.write(\"\\n\")\n",
    "    # Append text at the end of file\n",
    "    file_object.write(f\"{now} - finished: {model_checkpoint}, Batch: {batch_size}, Epochs: {num_epoch}, Folds: {num_folds}, Subset: {subset_labels}, Input_strategy: {input_strategy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591b15e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
