{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aquatic-australia",
   "metadata": {},
   "source": [
    "# Parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfcfcb1",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d51af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import gc\n",
    "from IPython.display import display, HTML\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from transformers import BertTokenizerFast, DebertaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8188a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_fold(path, fold):\n",
    "    train = pd.read_csv(f\"{path}/train.{fold}.csv\")\n",
    "    test = pd.read_csv(f\"{path}/test.{fold}.csv\")\n",
    "    train_origin = train[\"origin\"].tolist()\n",
    "    train_target = train[\"target\"].tolist()\n",
    "    train_labels = train[\"label\"].tolist()\n",
    "    test_origin = test[\"origin\"].tolist()\n",
    "    test_target = test[\"target\"].tolist()\n",
    "    test_labels = test[\"label\"].tolist()\n",
    "    return train_origin, train_target, train_labels, test_origin, test_target, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-lightweight",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "impressive-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e62acc",
   "metadata": {},
   "source": [
    "## Text Segment Input Strategies\n",
    "Different Input Strategies used for Language Model Input. Input of Language Model is pair ´(A,B)´ with ´A´ and ´B´ of type:\n",
    "1. Sentence (S)\n",
    "2. Title + Sentence (TS)\n",
    "3. Title + Sentence + Date (TSD)\n",
    "4. Sentence + Title + Date (STD)\n",
    "\n",
    "*Sentence* is the extracted Sentence from News Article, *Title* is the News Article Title and *Date* is the publishing date of the article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-taste",
   "metadata": {},
   "source": [
    "## Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07a661cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_strategies = [4,3,2,1]\n",
    "subset_labels_types = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8c9780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_checkpoints = ['microsoft/deberta-base', 'bert-base-uncased', 'bert-base-cased', 'bert-large-uncased', 'bert-large-cased']\n",
    "batch_size = 8\n",
    "metric_name = \"accuracy\"\n",
    "num_epoch = 10\n",
    "\n",
    "# Fold\n",
    "num_folds = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "referenced-wichita",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"semantic-test\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epoch,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be62939",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b38c34d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import collections\n",
    "\n",
    "#classification_threshold = 0.\n",
    "\n",
    "def flatten(d, parent_key='', sep='__'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, collections.MutableMapping):\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a264f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(data):\n",
    "    \"\"\"Return the sample arithmetic mean of data.\"\"\"\n",
    "    n = len(data)\n",
    "    if n < 1:\n",
    "        raise ValueError('mean requires at least one data point')\n",
    "    return sum(data)/n # in Python 2 use sum(data)/float(n)\n",
    "\n",
    "def _ss(data):\n",
    "    \"\"\"Return sum of square deviations of sequence data.\"\"\"\n",
    "    c = mean(data)\n",
    "    ss = sum((x-c)**2 for x in data)\n",
    "    return ss\n",
    "\n",
    "def stddev(data, ddof=0):\n",
    "    \"\"\"Calculates the population standard deviation\n",
    "    by default; specify ddof=1 to compute the sample\n",
    "    standard deviation.\"\"\"\n",
    "    n = len(data)\n",
    "    if n < 2:\n",
    "        raise ValueError('variance requires at least two data points')\n",
    "    ss = _ss(data)\n",
    "    pvar = ss/(n-ddof)\n",
    "    return pvar**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a0245c",
   "metadata": {},
   "source": [
    "# Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b80045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Others  Precision</th>\n",
       "      <th>Others  Recall</th>\n",
       "      <th>Others  F1-score</th>\n",
       "      <th>Others  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.999048</td>\n",
       "      <td>0.802817</td>\n",
       "      <td>0.861461</td>\n",
       "      <td>0.831106</td>\n",
       "      <td>794</td>\n",
       "      <td>0.185567</td>\n",
       "      <td>0.165138</td>\n",
       "      <td>0.174757</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.235060</td>\n",
       "      <td>0.694118</td>\n",
       "      <td>0.351190</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.136054</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55</td>\n",
       "      <td>0.624301</td>\n",
       "      <td>0.230800</td>\n",
       "      <td>0.265253</td>\n",
       "      <td>0.222725</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.587762</td>\n",
       "      <td>0.624301</td>\n",
       "      <td>0.590323</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.876993</td>\n",
       "      <td>0.786441</td>\n",
       "      <td>0.876574</td>\n",
       "      <td>0.829065</td>\n",
       "      <td>794</td>\n",
       "      <td>0.359375</td>\n",
       "      <td>0.211009</td>\n",
       "      <td>0.265896</td>\n",
       "      <td>109</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>41</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.422111</td>\n",
       "      <td>85</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>20</td>\n",
       "      <td>0.394366</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.387543</td>\n",
       "      <td>147</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>55</td>\n",
       "      <td>0.664269</td>\n",
       "      <td>0.488344</td>\n",
       "      <td>0.345789</td>\n",
       "      <td>0.363450</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.642099</td>\n",
       "      <td>0.664269</td>\n",
       "      <td>0.639111</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.003400</td>\n",
       "      <td>0.859534</td>\n",
       "      <td>0.822967</td>\n",
       "      <td>0.866499</td>\n",
       "      <td>0.844172</td>\n",
       "      <td>794</td>\n",
       "      <td>0.305263</td>\n",
       "      <td>0.532110</td>\n",
       "      <td>0.387960</td>\n",
       "      <td>109</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>41</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.347222</td>\n",
       "      <td>85</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>20</td>\n",
       "      <td>0.430769</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.404332</td>\n",
       "      <td>147</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>55</td>\n",
       "      <td>0.672262</td>\n",
       "      <td>0.434329</td>\n",
       "      <td>0.378945</td>\n",
       "      <td>0.383698</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.655164</td>\n",
       "      <td>0.672262</td>\n",
       "      <td>0.655150</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.003400</td>\n",
       "      <td>0.861746</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.886650</td>\n",
       "      <td>0.857491</td>\n",
       "      <td>794</td>\n",
       "      <td>0.396825</td>\n",
       "      <td>0.458716</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>109</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>41</td>\n",
       "      <td>0.535211</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>85</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>20</td>\n",
       "      <td>0.471698</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>147</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>55</td>\n",
       "      <td>0.710631</td>\n",
       "      <td>0.516309</td>\n",
       "      <td>0.445070</td>\n",
       "      <td>0.458140</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.691208</td>\n",
       "      <td>0.710631</td>\n",
       "      <td>0.694269</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.646500</td>\n",
       "      <td>0.853811</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.861461</td>\n",
       "      <td>0.858218</td>\n",
       "      <td>794</td>\n",
       "      <td>0.409396</td>\n",
       "      <td>0.559633</td>\n",
       "      <td>0.472868</td>\n",
       "      <td>109</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>41</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>0.472050</td>\n",
       "      <td>85</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>20</td>\n",
       "      <td>0.442529</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.479751</td>\n",
       "      <td>147</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>55</td>\n",
       "      <td>0.705835</td>\n",
       "      <td>0.493846</td>\n",
       "      <td>0.469520</td>\n",
       "      <td>0.460087</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.698278</td>\n",
       "      <td>0.705835</td>\n",
       "      <td>0.695664</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-15fbd50be80d>:10: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  if isinstance(v, collections.MutableMapping):\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7106314948041567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Others  Precision</th>\n",
       "      <th>Others  Recall</th>\n",
       "      <th>Others  F1-score</th>\n",
       "      <th>Others  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.993899</td>\n",
       "      <td>0.733400</td>\n",
       "      <td>0.918136</td>\n",
       "      <td>0.815436</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.344444</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.354286</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.413174</td>\n",
       "      <td>0.466216</td>\n",
       "      <td>0.438095</td>\n",
       "      <td>148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55</td>\n",
       "      <td>0.662670</td>\n",
       "      <td>0.213003</td>\n",
       "      <td>0.249865</td>\n",
       "      <td>0.229688</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.537768</td>\n",
       "      <td>0.662670</td>\n",
       "      <td>0.593452</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.896098</td>\n",
       "      <td>0.823602</td>\n",
       "      <td>0.835013</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>794</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.120370</td>\n",
       "      <td>0.165605</td>\n",
       "      <td>108</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>41</td>\n",
       "      <td>0.278571</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>0.346667</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.346774</td>\n",
       "      <td>0.581081</td>\n",
       "      <td>0.434343</td>\n",
       "      <td>148</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>55</td>\n",
       "      <td>0.642686</td>\n",
       "      <td>0.357138</td>\n",
       "      <td>0.293720</td>\n",
       "      <td>0.269559</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.634539</td>\n",
       "      <td>0.642686</td>\n",
       "      <td>0.619927</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.039200</td>\n",
       "      <td>0.938469</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.755668</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>794</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>0.425926</td>\n",
       "      <td>0.343284</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.311475</td>\n",
       "      <td>0.670588</td>\n",
       "      <td>0.425373</td>\n",
       "      <td>85</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>20</td>\n",
       "      <td>0.403941</td>\n",
       "      <td>0.554054</td>\n",
       "      <td>0.467236</td>\n",
       "      <td>148</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.127273</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>55</td>\n",
       "      <td>0.635492</td>\n",
       "      <td>0.446321</td>\n",
       "      <td>0.383358</td>\n",
       "      <td>0.354175</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.688068</td>\n",
       "      <td>0.635492</td>\n",
       "      <td>0.645382</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.039200</td>\n",
       "      <td>0.868086</td>\n",
       "      <td>0.860892</td>\n",
       "      <td>0.826196</td>\n",
       "      <td>0.843188</td>\n",
       "      <td>794</td>\n",
       "      <td>0.394231</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.386792</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.445415</td>\n",
       "      <td>85</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.402174</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.445783</td>\n",
       "      <td>148</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>55</td>\n",
       "      <td>0.669864</td>\n",
       "      <td>0.435923</td>\n",
       "      <td>0.389144</td>\n",
       "      <td>0.381393</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.675421</td>\n",
       "      <td>0.669864</td>\n",
       "      <td>0.666724</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.684400</td>\n",
       "      <td>0.870770</td>\n",
       "      <td>0.876190</td>\n",
       "      <td>0.811083</td>\n",
       "      <td>0.842381</td>\n",
       "      <td>794</td>\n",
       "      <td>0.386861</td>\n",
       "      <td>0.490741</td>\n",
       "      <td>0.432653</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>0.564706</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>85</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.401015</td>\n",
       "      <td>0.533784</td>\n",
       "      <td>0.457971</td>\n",
       "      <td>148</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>55</td>\n",
       "      <td>0.669864</td>\n",
       "      <td>0.436021</td>\n",
       "      <td>0.397447</td>\n",
       "      <td>0.386065</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.684805</td>\n",
       "      <td>0.669864</td>\n",
       "      <td>0.670816</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6698641087130296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Others  Precision</th>\n",
       "      <th>Others  Recall</th>\n",
       "      <th>Others  F1-score</th>\n",
       "      <th>Others  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.117597</td>\n",
       "      <td>0.851964</td>\n",
       "      <td>0.710327</td>\n",
       "      <td>0.774725</td>\n",
       "      <td>794</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.086331</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.241135</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.302222</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.229064</td>\n",
       "      <td>0.632653</td>\n",
       "      <td>0.336347</td>\n",
       "      <td>147</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>56</td>\n",
       "      <td>0.558400</td>\n",
       "      <td>0.230816</td>\n",
       "      <td>0.260165</td>\n",
       "      <td>0.218561</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.605512</td>\n",
       "      <td>0.558400</td>\n",
       "      <td>0.560786</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.043238</td>\n",
       "      <td>0.876652</td>\n",
       "      <td>0.751889</td>\n",
       "      <td>0.809492</td>\n",
       "      <td>794</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.169355</td>\n",
       "      <td>108</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>41</td>\n",
       "      <td>0.249012</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.373887</td>\n",
       "      <td>84</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>20</td>\n",
       "      <td>0.395683</td>\n",
       "      <td>0.374150</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>147</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>0.160714</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>56</td>\n",
       "      <td>0.597600</td>\n",
       "      <td>0.490865</td>\n",
       "      <td>0.329370</td>\n",
       "      <td>0.297014</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.677334</td>\n",
       "      <td>0.597600</td>\n",
       "      <td>0.611187</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.022200</td>\n",
       "      <td>0.888580</td>\n",
       "      <td>0.823601</td>\n",
       "      <td>0.852645</td>\n",
       "      <td>0.837871</td>\n",
       "      <td>794</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>108</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.146341</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>41</td>\n",
       "      <td>0.325758</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.373832</td>\n",
       "      <td>147</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>56</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>0.361165</td>\n",
       "      <td>0.326494</td>\n",
       "      <td>0.324264</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.640352</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>0.641816</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.022200</td>\n",
       "      <td>0.892866</td>\n",
       "      <td>0.827879</td>\n",
       "      <td>0.860202</td>\n",
       "      <td>0.843731</td>\n",
       "      <td>794</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.336134</td>\n",
       "      <td>108</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>41</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>84</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>20</td>\n",
       "      <td>0.376543</td>\n",
       "      <td>0.414966</td>\n",
       "      <td>0.394822</td>\n",
       "      <td>147</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>56</td>\n",
       "      <td>0.664800</td>\n",
       "      <td>0.413886</td>\n",
       "      <td>0.367116</td>\n",
       "      <td>0.378739</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.650839</td>\n",
       "      <td>0.664800</td>\n",
       "      <td>0.654182</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.674600</td>\n",
       "      <td>0.915936</td>\n",
       "      <td>0.848293</td>\n",
       "      <td>0.845088</td>\n",
       "      <td>0.846688</td>\n",
       "      <td>794</td>\n",
       "      <td>0.324786</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.337778</td>\n",
       "      <td>108</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>41</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.429907</td>\n",
       "      <td>84</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>20</td>\n",
       "      <td>0.408805</td>\n",
       "      <td>0.442177</td>\n",
       "      <td>0.424837</td>\n",
       "      <td>147</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>56</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.431078</td>\n",
       "      <td>0.377349</td>\n",
       "      <td>0.380605</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.668017</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.662369</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Others  Precision</th>\n",
       "      <th>Others  Recall</th>\n",
       "      <th>Others  F1-score</th>\n",
       "      <th>Others  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.975911</td>\n",
       "      <td>0.787283</td>\n",
       "      <td>0.858764</td>\n",
       "      <td>0.821472</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.229839</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.452555</td>\n",
       "      <td>0.421769</td>\n",
       "      <td>0.436620</td>\n",
       "      <td>147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.209954</td>\n",
       "      <td>0.279872</td>\n",
       "      <td>0.228781</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.568118</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.595563</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.866879</td>\n",
       "      <td>0.795735</td>\n",
       "      <td>0.894073</td>\n",
       "      <td>0.842043</td>\n",
       "      <td>793</td>\n",
       "      <td>0.215909</td>\n",
       "      <td>0.174312</td>\n",
       "      <td>0.192893</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.373626</td>\n",
       "      <td>84</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>20</td>\n",
       "      <td>0.423358</td>\n",
       "      <td>0.394558</td>\n",
       "      <td>0.408451</td>\n",
       "      <td>147</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.183908</td>\n",
       "      <td>55</td>\n",
       "      <td>0.664800</td>\n",
       "      <td>0.433134</td>\n",
       "      <td>0.309023</td>\n",
       "      <td>0.323113</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.623743</td>\n",
       "      <td>0.664800</td>\n",
       "      <td>0.636420</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.032800</td>\n",
       "      <td>0.840006</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.873897</td>\n",
       "      <td>0.856613</td>\n",
       "      <td>793</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.146789</td>\n",
       "      <td>0.178771</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.301676</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.410646</td>\n",
       "      <td>84</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>20</td>\n",
       "      <td>0.507042</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.498270</td>\n",
       "      <td>147</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>55</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.405134</td>\n",
       "      <td>0.387490</td>\n",
       "      <td>0.376553</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.656909</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.660822</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.032800</td>\n",
       "      <td>0.798869</td>\n",
       "      <td>0.887268</td>\n",
       "      <td>0.843632</td>\n",
       "      <td>0.864900</td>\n",
       "      <td>793</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.577982</td>\n",
       "      <td>0.454874</td>\n",
       "      <td>109</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>42</td>\n",
       "      <td>0.457831</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.455090</td>\n",
       "      <td>84</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>20</td>\n",
       "      <td>0.483516</td>\n",
       "      <td>0.598639</td>\n",
       "      <td>0.534954</td>\n",
       "      <td>147</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>55</td>\n",
       "      <td>0.704800</td>\n",
       "      <td>0.494306</td>\n",
       "      <td>0.460036</td>\n",
       "      <td>0.452090</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.717867</td>\n",
       "      <td>0.704800</td>\n",
       "      <td>0.703442</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.660800</td>\n",
       "      <td>0.805209</td>\n",
       "      <td>0.903533</td>\n",
       "      <td>0.838588</td>\n",
       "      <td>0.869850</td>\n",
       "      <td>793</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.605505</td>\n",
       "      <td>0.476534</td>\n",
       "      <td>109</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>42</td>\n",
       "      <td>0.453704</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.510417</td>\n",
       "      <td>84</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.478495</td>\n",
       "      <td>0.605442</td>\n",
       "      <td>0.534535</td>\n",
       "      <td>147</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>55</td>\n",
       "      <td>0.715200</td>\n",
       "      <td>0.510816</td>\n",
       "      <td>0.496408</td>\n",
       "      <td>0.474499</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.733225</td>\n",
       "      <td>0.715200</td>\n",
       "      <td>0.714212</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7152\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_none__precision</th>\n",
       "      <th>eval_none__recall</th>\n",
       "      <th>eval_none__f1-score</th>\n",
       "      <th>eval_none__support</th>\n",
       "      <th>eval_causal__precision</th>\n",
       "      <th>eval_causal__recall</th>\n",
       "      <th>eval_causal__f1-score</th>\n",
       "      <th>eval_causal__support</th>\n",
       "      <th>eval_contrast__precision</th>\n",
       "      <th>eval_contrast__recall</th>\n",
       "      <th>eval_contrast__f1-score</th>\n",
       "      <th>eval_contrast__support</th>\n",
       "      <th>eval_equivalence__precision</th>\n",
       "      <th>eval_equivalence__recall</th>\n",
       "      <th>eval_equivalence__f1-score</th>\n",
       "      <th>eval_equivalence__support</th>\n",
       "      <th>eval_identity__precision</th>\n",
       "      <th>eval_identity__recall</th>\n",
       "      <th>eval_identity__f1-score</th>\n",
       "      <th>eval_identity__support</th>\n",
       "      <th>eval_temporal__precision</th>\n",
       "      <th>eval_temporal__recall</th>\n",
       "      <th>eval_temporal__f1-score</th>\n",
       "      <th>eval_temporal__support</th>\n",
       "      <th>eval_others__precision</th>\n",
       "      <th>eval_others__recall</th>\n",
       "      <th>eval_others__f1-score</th>\n",
       "      <th>eval_others__support</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_macro avg__precision</th>\n",
       "      <th>eval_macro avg__recall</th>\n",
       "      <th>eval_macro avg__f1-score</th>\n",
       "      <th>eval_macro avg__support</th>\n",
       "      <th>eval_weighted avg__precision</th>\n",
       "      <th>eval_weighted avg__recall</th>\n",
       "      <th>eval_weighted avg__f1-score</th>\n",
       "      <th>eval_weighted avg__support</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.861746</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.886650</td>\n",
       "      <td>0.857491</td>\n",
       "      <td>794.00</td>\n",
       "      <td>0.396825</td>\n",
       "      <td>0.458716</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>109.00000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.535211</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>0.487179</td>\n",
       "      <td>85.00000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.471698</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>147.00</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>55.00</td>\n",
       "      <td>0.710631</td>\n",
       "      <td>0.516309</td>\n",
       "      <td>0.445070</td>\n",
       "      <td>0.458140</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>0.691208</td>\n",
       "      <td>0.710631</td>\n",
       "      <td>0.694269</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>13.097200</td>\n",
       "      <td>95.517000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.868086</td>\n",
       "      <td>0.860892</td>\n",
       "      <td>0.826196</td>\n",
       "      <td>0.843188</td>\n",
       "      <td>794.00</td>\n",
       "      <td>0.394231</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.386792</td>\n",
       "      <td>108.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.445415</td>\n",
       "      <td>85.00000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.402174</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.445783</td>\n",
       "      <td>148.00</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>55.00</td>\n",
       "      <td>0.669864</td>\n",
       "      <td>0.435923</td>\n",
       "      <td>0.389144</td>\n",
       "      <td>0.381393</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>0.675421</td>\n",
       "      <td>0.669864</td>\n",
       "      <td>0.666724</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>13.257100</td>\n",
       "      <td>94.365000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.915936</td>\n",
       "      <td>0.848293</td>\n",
       "      <td>0.845088</td>\n",
       "      <td>0.846688</td>\n",
       "      <td>794.00</td>\n",
       "      <td>0.324786</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.337778</td>\n",
       "      <td>108.00000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.429907</td>\n",
       "      <td>84.00000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.408805</td>\n",
       "      <td>0.442177</td>\n",
       "      <td>0.424837</td>\n",
       "      <td>147.00</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.134831</td>\n",
       "      <td>56.00</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.431078</td>\n",
       "      <td>0.377349</td>\n",
       "      <td>0.380605</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>0.668017</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.662369</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>12.424000</td>\n",
       "      <td>100.612000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.805209</td>\n",
       "      <td>0.903533</td>\n",
       "      <td>0.838588</td>\n",
       "      <td>0.869850</td>\n",
       "      <td>793.00</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.605505</td>\n",
       "      <td>0.476534</td>\n",
       "      <td>109.00000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>42.00</td>\n",
       "      <td>0.453704</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.510417</td>\n",
       "      <td>84.00000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.478495</td>\n",
       "      <td>0.605442</td>\n",
       "      <td>0.534535</td>\n",
       "      <td>147.00</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>55.00</td>\n",
       "      <td>0.715200</td>\n",
       "      <td>0.510816</td>\n",
       "      <td>0.496408</td>\n",
       "      <td>0.474499</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>0.733225</td>\n",
       "      <td>0.715200</td>\n",
       "      <td>0.714212</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>10.802600</td>\n",
       "      <td>115.713000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>0.862744</td>\n",
       "      <td>0.860727</td>\n",
       "      <td>0.849131</td>\n",
       "      <td>0.854304</td>\n",
       "      <td>793.75</td>\n",
       "      <td>0.377175</td>\n",
       "      <td>0.448925</td>\n",
       "      <td>0.406659</td>\n",
       "      <td>108.50000</td>\n",
       "      <td>0.296970</td>\n",
       "      <td>0.060830</td>\n",
       "      <td>0.098404</td>\n",
       "      <td>41.25</td>\n",
       "      <td>0.424232</td>\n",
       "      <td>0.544503</td>\n",
       "      <td>0.468229</td>\n",
       "      <td>84.50000</td>\n",
       "      <td>0.632895</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.467179</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.440293</td>\n",
       "      <td>0.514456</td>\n",
       "      <td>0.473838</td>\n",
       "      <td>147.25</td>\n",
       "      <td>0.282432</td>\n",
       "      <td>0.158604</td>\n",
       "      <td>0.197002</td>\n",
       "      <td>55.25</td>\n",
       "      <td>0.690924</td>\n",
       "      <td>0.473532</td>\n",
       "      <td>0.426993</td>\n",
       "      <td>0.423659</td>\n",
       "      <td>1250.50000</td>\n",
       "      <td>0.691968</td>\n",
       "      <td>0.690924</td>\n",
       "      <td>0.684394</td>\n",
       "      <td>1250.50000</td>\n",
       "      <td>12.395225</td>\n",
       "      <td>101.551750</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.045348</td>\n",
       "      <td>0.031196</td>\n",
       "      <td>0.026212</td>\n",
       "      <td>0.012019</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.034964</td>\n",
       "      <td>0.113781</td>\n",
       "      <td>0.058816</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.204090</td>\n",
       "      <td>0.058183</td>\n",
       "      <td>0.090977</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.087651</td>\n",
       "      <td>0.068539</td>\n",
       "      <td>0.037095</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.124725</td>\n",
       "      <td>0.217466</td>\n",
       "      <td>0.162463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040374</td>\n",
       "      <td>0.067651</td>\n",
       "      <td>0.048785</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.098853</td>\n",
       "      <td>0.069113</td>\n",
       "      <td>0.073278</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.025474</td>\n",
       "      <td>0.046320</td>\n",
       "      <td>0.054901</td>\n",
       "      <td>0.049712</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.029156</td>\n",
       "      <td>0.025474</td>\n",
       "      <td>0.024385</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>1.121440</td>\n",
       "      <td>9.823301</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Attribution  Precision</th>\n",
       "      <th>Attribution  Recall</th>\n",
       "      <th>Attribution  F1-score</th>\n",
       "      <th>Attribution  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Conditional  Precision</th>\n",
       "      <th>Conditional  Recall</th>\n",
       "      <th>Conditional  F1-score</th>\n",
       "      <th>Conditional  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Description  Precision</th>\n",
       "      <th>Description  Recall</th>\n",
       "      <th>Description  F1-score</th>\n",
       "      <th>Description  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Fulfillment  Precision</th>\n",
       "      <th>Fulfillment  Recall</th>\n",
       "      <th>Fulfillment  F1-score</th>\n",
       "      <th>Fulfillment  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Purpose  Precision</th>\n",
       "      <th>Purpose  Recall</th>\n",
       "      <th>Purpose  F1-score</th>\n",
       "      <th>Purpose  Support</th>\n",
       "      <th>Summary  Precision</th>\n",
       "      <th>Summary  Recall</th>\n",
       "      <th>Summary  F1-score</th>\n",
       "      <th>Summary  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.056969</td>\n",
       "      <td>0.679603</td>\n",
       "      <td>0.948363</td>\n",
       "      <td>0.791798</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.018349</td>\n",
       "      <td>0.035088</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.188406</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.169935</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.405797</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>147</td>\n",
       "      <td>0.636291</td>\n",
       "      <td>0.139484</td>\n",
       "      <td>0.109329</td>\n",
       "      <td>0.104673</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.526525</td>\n",
       "      <td>0.636291</td>\n",
       "      <td>0.547480</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.958466</td>\n",
       "      <td>0.828431</td>\n",
       "      <td>0.851385</td>\n",
       "      <td>0.839752</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.146789</td>\n",
       "      <td>0.198758</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.304878</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.403226</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.383562</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.459016</td>\n",
       "      <td>147</td>\n",
       "      <td>0.660272</td>\n",
       "      <td>0.152047</td>\n",
       "      <td>0.180403</td>\n",
       "      <td>0.158396</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.618151</td>\n",
       "      <td>0.660272</td>\n",
       "      <td>0.631314</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.103300</td>\n",
       "      <td>0.903125</td>\n",
       "      <td>0.863049</td>\n",
       "      <td>0.841310</td>\n",
       "      <td>0.852041</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.275229</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.350318</td>\n",
       "      <td>0.654762</td>\n",
       "      <td>0.456432</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.408333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.506460</td>\n",
       "      <td>147</td>\n",
       "      <td>0.685851</td>\n",
       "      <td>0.214687</td>\n",
       "      <td>0.232331</td>\n",
       "      <td>0.214130</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.666873</td>\n",
       "      <td>0.685851</td>\n",
       "      <td>0.667400</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.103300</td>\n",
       "      <td>0.911714</td>\n",
       "      <td>0.850123</td>\n",
       "      <td>0.871537</td>\n",
       "      <td>0.860697</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.577982</td>\n",
       "      <td>0.443662</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.465909</td>\n",
       "      <td>0.488095</td>\n",
       "      <td>0.476744</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.449367</td>\n",
       "      <td>0.482993</td>\n",
       "      <td>0.465574</td>\n",
       "      <td>147</td>\n",
       "      <td>0.701839</td>\n",
       "      <td>0.234408</td>\n",
       "      <td>0.247551</td>\n",
       "      <td>0.238149</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.666012</td>\n",
       "      <td>0.701839</td>\n",
       "      <td>0.681423</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.741200</td>\n",
       "      <td>0.907691</td>\n",
       "      <td>0.870064</td>\n",
       "      <td>0.860202</td>\n",
       "      <td>0.865104</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.471074</td>\n",
       "      <td>0.522936</td>\n",
       "      <td>0.495652</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.406504</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.483092</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.448454</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.510264</td>\n",
       "      <td>147</td>\n",
       "      <td>0.709832</td>\n",
       "      <td>0.215746</td>\n",
       "      <td>0.260018</td>\n",
       "      <td>0.234370</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.679539</td>\n",
       "      <td>0.709832</td>\n",
       "      <td>0.691986</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.709832134292566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:21, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Attribution  Precision</th>\n",
       "      <th>Attribution  Recall</th>\n",
       "      <th>Attribution  F1-score</th>\n",
       "      <th>Attribution  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Conditional  Precision</th>\n",
       "      <th>Conditional  Recall</th>\n",
       "      <th>Conditional  F1-score</th>\n",
       "      <th>Conditional  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Description  Precision</th>\n",
       "      <th>Description  Recall</th>\n",
       "      <th>Description  F1-score</th>\n",
       "      <th>Description  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Fulfillment  Precision</th>\n",
       "      <th>Fulfillment  Recall</th>\n",
       "      <th>Fulfillment  F1-score</th>\n",
       "      <th>Fulfillment  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Purpose  Precision</th>\n",
       "      <th>Purpose  Recall</th>\n",
       "      <th>Purpose  F1-score</th>\n",
       "      <th>Purpose  Support</th>\n",
       "      <th>Summary  Precision</th>\n",
       "      <th>Summary  Recall</th>\n",
       "      <th>Summary  F1-score</th>\n",
       "      <th>Summary  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.051865</td>\n",
       "      <td>0.699454</td>\n",
       "      <td>0.967254</td>\n",
       "      <td>0.811839</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.238411</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.114865</td>\n",
       "      <td>0.179894</td>\n",
       "      <td>148</td>\n",
       "      <td>0.646683</td>\n",
       "      <td>0.126340</td>\n",
       "      <td>0.112663</td>\n",
       "      <td>0.109048</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.522541</td>\n",
       "      <td>0.646683</td>\n",
       "      <td>0.559330</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.031885</td>\n",
       "      <td>0.749748</td>\n",
       "      <td>0.935768</td>\n",
       "      <td>0.832493</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.337580</td>\n",
       "      <td>0.630952</td>\n",
       "      <td>0.439834</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.303922</td>\n",
       "      <td>0.209459</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>148</td>\n",
       "      <td>0.661071</td>\n",
       "      <td>0.115937</td>\n",
       "      <td>0.148015</td>\n",
       "      <td>0.126694</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.534482</td>\n",
       "      <td>0.661071</td>\n",
       "      <td>0.587250</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.142300</td>\n",
       "      <td>0.924397</td>\n",
       "      <td>0.901934</td>\n",
       "      <td>0.822418</td>\n",
       "      <td>0.860343</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.348624</td>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.350230</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.438356</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.451282</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>0.513120</td>\n",
       "      <td>148</td>\n",
       "      <td>0.678657</td>\n",
       "      <td>0.200794</td>\n",
       "      <td>0.235897</td>\n",
       "      <td>0.208742</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.682992</td>\n",
       "      <td>0.678657</td>\n",
       "      <td>0.671909</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.142300</td>\n",
       "      <td>0.861192</td>\n",
       "      <td>0.884363</td>\n",
       "      <td>0.847607</td>\n",
       "      <td>0.865595</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.355422</td>\n",
       "      <td>0.546296</td>\n",
       "      <td>0.430657</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.364341</td>\n",
       "      <td>0.559524</td>\n",
       "      <td>0.441315</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>148</td>\n",
       "      <td>0.689049</td>\n",
       "      <td>0.208189</td>\n",
       "      <td>0.241952</td>\n",
       "      <td>0.221325</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.673761</td>\n",
       "      <td>0.689049</td>\n",
       "      <td>0.677617</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.863960</td>\n",
       "      <td>0.899729</td>\n",
       "      <td>0.836272</td>\n",
       "      <td>0.866841</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.404412</td>\n",
       "      <td>0.654762</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.415842</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>148</td>\n",
       "      <td>0.699440</td>\n",
       "      <td>0.214153</td>\n",
       "      <td>0.267846</td>\n",
       "      <td>0.235519</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.689507</td>\n",
       "      <td>0.699440</td>\n",
       "      <td>0.689168</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6994404476418865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:16, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Attribution  Precision</th>\n",
       "      <th>Attribution  Recall</th>\n",
       "      <th>Attribution  F1-score</th>\n",
       "      <th>Attribution  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Conditional  Precision</th>\n",
       "      <th>Conditional  Recall</th>\n",
       "      <th>Conditional  F1-score</th>\n",
       "      <th>Conditional  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Description  Precision</th>\n",
       "      <th>Description  Recall</th>\n",
       "      <th>Description  F1-score</th>\n",
       "      <th>Description  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Fulfillment  Precision</th>\n",
       "      <th>Fulfillment  Recall</th>\n",
       "      <th>Fulfillment  F1-score</th>\n",
       "      <th>Fulfillment  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Purpose  Precision</th>\n",
       "      <th>Purpose  Recall</th>\n",
       "      <th>Purpose  F1-score</th>\n",
       "      <th>Purpose  Support</th>\n",
       "      <th>Summary  Precision</th>\n",
       "      <th>Summary  Recall</th>\n",
       "      <th>Summary  F1-score</th>\n",
       "      <th>Summary  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.050705</td>\n",
       "      <td>0.761597</td>\n",
       "      <td>0.889169</td>\n",
       "      <td>0.820453</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.279661</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.325123</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.328431</td>\n",
       "      <td>0.455782</td>\n",
       "      <td>0.381766</td>\n",
       "      <td>147</td>\n",
       "      <td>0.644800</td>\n",
       "      <td>0.114141</td>\n",
       "      <td>0.144432</td>\n",
       "      <td>0.127279</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.541407</td>\n",
       "      <td>0.644800</td>\n",
       "      <td>0.588156</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.021631</td>\n",
       "      <td>0.861518</td>\n",
       "      <td>0.814861</td>\n",
       "      <td>0.837540</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.179775</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.276923</td>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.417391</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.366864</td>\n",
       "      <td>0.421769</td>\n",
       "      <td>0.392405</td>\n",
       "      <td>147</td>\n",
       "      <td>0.637600</td>\n",
       "      <td>0.144490</td>\n",
       "      <td>0.185986</td>\n",
       "      <td>0.152259</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.628959</td>\n",
       "      <td>0.637600</td>\n",
       "      <td>0.622068</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.077600</td>\n",
       "      <td>1.071600</td>\n",
       "      <td>0.920188</td>\n",
       "      <td>0.740554</td>\n",
       "      <td>0.820656</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.204420</td>\n",
       "      <td>0.342593</td>\n",
       "      <td>0.256055</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.378378</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>147</td>\n",
       "      <td>0.628800</td>\n",
       "      <td>0.191745</td>\n",
       "      <td>0.235675</td>\n",
       "      <td>0.200730</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.683636</td>\n",
       "      <td>0.628800</td>\n",
       "      <td>0.639636</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.077600</td>\n",
       "      <td>0.993803</td>\n",
       "      <td>0.879739</td>\n",
       "      <td>0.847607</td>\n",
       "      <td>0.863374</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.308271</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.340249</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.408907</td>\n",
       "      <td>0.687075</td>\n",
       "      <td>0.512690</td>\n",
       "      <td>147</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.212978</td>\n",
       "      <td>0.235261</td>\n",
       "      <td>0.220735</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.672732</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.676884</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.717600</td>\n",
       "      <td>0.995817</td>\n",
       "      <td>0.885790</td>\n",
       "      <td>0.840050</td>\n",
       "      <td>0.862314</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.426087</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>147</td>\n",
       "      <td>0.688800</td>\n",
       "      <td>0.200154</td>\n",
       "      <td>0.245453</td>\n",
       "      <td>0.218732</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.675223</td>\n",
       "      <td>0.688800</td>\n",
       "      <td>0.678223</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:09, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Attribution  Precision</th>\n",
       "      <th>Attribution  Recall</th>\n",
       "      <th>Attribution  F1-score</th>\n",
       "      <th>Attribution  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Conditional  Precision</th>\n",
       "      <th>Conditional  Recall</th>\n",
       "      <th>Conditional  F1-score</th>\n",
       "      <th>Conditional  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Description  Precision</th>\n",
       "      <th>Description  Recall</th>\n",
       "      <th>Description  F1-score</th>\n",
       "      <th>Description  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Fulfillment  Precision</th>\n",
       "      <th>Fulfillment  Recall</th>\n",
       "      <th>Fulfillment  F1-score</th>\n",
       "      <th>Fulfillment  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Purpose  Precision</th>\n",
       "      <th>Purpose  Recall</th>\n",
       "      <th>Purpose  F1-score</th>\n",
       "      <th>Purpose  Support</th>\n",
       "      <th>Summary  Precision</th>\n",
       "      <th>Summary  Recall</th>\n",
       "      <th>Summary  F1-score</th>\n",
       "      <th>Summary  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.194395</td>\n",
       "      <td>0.851216</td>\n",
       "      <td>0.750315</td>\n",
       "      <td>0.797587</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.173034</td>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.290566</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.471698</td>\n",
       "      <td>0.340136</td>\n",
       "      <td>0.395257</td>\n",
       "      <td>147</td>\n",
       "      <td>0.577600</td>\n",
       "      <td>0.124662</td>\n",
       "      <td>0.166361</td>\n",
       "      <td>0.123618</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.607249</td>\n",
       "      <td>0.577600</td>\n",
       "      <td>0.572230</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.992059</td>\n",
       "      <td>0.818727</td>\n",
       "      <td>0.860025</td>\n",
       "      <td>0.838868</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.027523</td>\n",
       "      <td>0.052174</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.379822</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.408805</td>\n",
       "      <td>0.442177</td>\n",
       "      <td>0.424837</td>\n",
       "      <td>147</td>\n",
       "      <td>0.651200</td>\n",
       "      <td>0.165125</td>\n",
       "      <td>0.173556</td>\n",
       "      <td>0.141308</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.628346</td>\n",
       "      <td>0.651200</td>\n",
       "      <td>0.612516</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.109200</td>\n",
       "      <td>0.971936</td>\n",
       "      <td>0.875171</td>\n",
       "      <td>0.804540</td>\n",
       "      <td>0.838371</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.577982</td>\n",
       "      <td>0.406452</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.585034</td>\n",
       "      <td>0.507375</td>\n",
       "      <td>147</td>\n",
       "      <td>0.674400</td>\n",
       "      <td>0.315543</td>\n",
       "      <td>0.248594</td>\n",
       "      <td>0.235193</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.707215</td>\n",
       "      <td>0.674400</td>\n",
       "      <td>0.668814</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.109200</td>\n",
       "      <td>0.920490</td>\n",
       "      <td>0.866035</td>\n",
       "      <td>0.807062</td>\n",
       "      <td>0.835509</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.349112</td>\n",
       "      <td>0.541284</td>\n",
       "      <td>0.424460</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.389262</td>\n",
       "      <td>0.682353</td>\n",
       "      <td>0.495726</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.440476</td>\n",
       "      <td>0.503401</td>\n",
       "      <td>0.469841</td>\n",
       "      <td>147</td>\n",
       "      <td>0.674400</td>\n",
       "      <td>0.241009</td>\n",
       "      <td>0.256907</td>\n",
       "      <td>0.238424</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.675414</td>\n",
       "      <td>0.674400</td>\n",
       "      <td>0.667536</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.749100</td>\n",
       "      <td>0.916176</td>\n",
       "      <td>0.854756</td>\n",
       "      <td>0.838588</td>\n",
       "      <td>0.846595</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>0.495413</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.418033</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.452174</td>\n",
       "      <td>147</td>\n",
       "      <td>0.689600</td>\n",
       "      <td>0.229300</td>\n",
       "      <td>0.261584</td>\n",
       "      <td>0.235400</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.670873</td>\n",
       "      <td>0.689600</td>\n",
       "      <td>0.675201</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6896\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_none__precision</th>\n",
       "      <th>eval_none__recall</th>\n",
       "      <th>eval_none__f1-score</th>\n",
       "      <th>eval_none__support</th>\n",
       "      <th>eval_attribution__precision</th>\n",
       "      <th>eval_attribution__recall</th>\n",
       "      <th>eval_attribution__f1-score</th>\n",
       "      <th>eval_attribution__support</th>\n",
       "      <th>eval_causal__precision</th>\n",
       "      <th>eval_causal__recall</th>\n",
       "      <th>eval_causal__f1-score</th>\n",
       "      <th>eval_causal__support</th>\n",
       "      <th>eval_conditional__precision</th>\n",
       "      <th>eval_conditional__recall</th>\n",
       "      <th>eval_conditional__f1-score</th>\n",
       "      <th>eval_conditional__support</th>\n",
       "      <th>eval_contrast__precision</th>\n",
       "      <th>eval_contrast__recall</th>\n",
       "      <th>eval_contrast__f1-score</th>\n",
       "      <th>eval_contrast__support</th>\n",
       "      <th>eval_description__precision</th>\n",
       "      <th>eval_description__recall</th>\n",
       "      <th>eval_description__f1-score</th>\n",
       "      <th>eval_description__support</th>\n",
       "      <th>eval_equivalence__precision</th>\n",
       "      <th>eval_equivalence__recall</th>\n",
       "      <th>eval_equivalence__f1-score</th>\n",
       "      <th>eval_equivalence__support</th>\n",
       "      <th>eval_fulfillment__precision</th>\n",
       "      <th>eval_fulfillment__recall</th>\n",
       "      <th>eval_fulfillment__f1-score</th>\n",
       "      <th>eval_fulfillment__support</th>\n",
       "      <th>eval_identity__precision</th>\n",
       "      <th>eval_identity__recall</th>\n",
       "      <th>eval_identity__f1-score</th>\n",
       "      <th>eval_identity__support</th>\n",
       "      <th>eval_purpose__precision</th>\n",
       "      <th>eval_purpose__recall</th>\n",
       "      <th>eval_purpose__f1-score</th>\n",
       "      <th>eval_purpose__support</th>\n",
       "      <th>eval_summary__precision</th>\n",
       "      <th>eval_summary__recall</th>\n",
       "      <th>eval_summary__f1-score</th>\n",
       "      <th>eval_summary__support</th>\n",
       "      <th>eval_temporal__precision</th>\n",
       "      <th>eval_temporal__recall</th>\n",
       "      <th>eval_temporal__f1-score</th>\n",
       "      <th>eval_temporal__support</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_macro avg__precision</th>\n",
       "      <th>eval_macro avg__recall</th>\n",
       "      <th>eval_macro avg__f1-score</th>\n",
       "      <th>eval_macro avg__support</th>\n",
       "      <th>eval_weighted avg__precision</th>\n",
       "      <th>eval_weighted avg__recall</th>\n",
       "      <th>eval_weighted avg__f1-score</th>\n",
       "      <th>eval_weighted avg__support</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.907691</td>\n",
       "      <td>0.870064</td>\n",
       "      <td>0.860202</td>\n",
       "      <td>0.865104</td>\n",
       "      <td>794.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.471074</td>\n",
       "      <td>0.522936</td>\n",
       "      <td>0.495652</td>\n",
       "      <td>109.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.406504</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.483092</td>\n",
       "      <td>84.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.448454</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.510264</td>\n",
       "      <td>147.00</td>\n",
       "      <td>0.709832</td>\n",
       "      <td>0.215746</td>\n",
       "      <td>0.260018</td>\n",
       "      <td>0.234370</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>0.679539</td>\n",
       "      <td>0.709832</td>\n",
       "      <td>0.691986</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>13.211600</td>\n",
       "      <td>94.69000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.863960</td>\n",
       "      <td>0.899729</td>\n",
       "      <td>0.836272</td>\n",
       "      <td>0.866841</td>\n",
       "      <td>794.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.405405</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>108.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>0.404412</td>\n",
       "      <td>0.654762</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>84.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.415842</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>148.00</td>\n",
       "      <td>0.699440</td>\n",
       "      <td>0.214153</td>\n",
       "      <td>0.267846</td>\n",
       "      <td>0.235519</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>0.689507</td>\n",
       "      <td>0.699440</td>\n",
       "      <td>0.689168</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>13.282100</td>\n",
       "      <td>94.18700</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.993803</td>\n",
       "      <td>0.879739</td>\n",
       "      <td>0.847607</td>\n",
       "      <td>0.863374</td>\n",
       "      <td>794.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.308271</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.340249</td>\n",
       "      <td>108.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>85.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.408907</td>\n",
       "      <td>0.687075</td>\n",
       "      <td>0.512690</td>\n",
       "      <td>147.00</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.212978</td>\n",
       "      <td>0.235261</td>\n",
       "      <td>0.220735</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>0.672732</td>\n",
       "      <td>0.690400</td>\n",
       "      <td>0.676884</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>12.401600</td>\n",
       "      <td>100.79400</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.916176</td>\n",
       "      <td>0.854756</td>\n",
       "      <td>0.838588</td>\n",
       "      <td>0.846595</td>\n",
       "      <td>793.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>0.495413</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>109.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.418033</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>85.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.452174</td>\n",
       "      <td>147.00</td>\n",
       "      <td>0.689600</td>\n",
       "      <td>0.229300</td>\n",
       "      <td>0.261584</td>\n",
       "      <td>0.235400</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>0.670873</td>\n",
       "      <td>0.689600</td>\n",
       "      <td>0.675201</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>10.853300</td>\n",
       "      <td>115.17200</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>0.920407</td>\n",
       "      <td>0.876072</td>\n",
       "      <td>0.845667</td>\n",
       "      <td>0.860478</td>\n",
       "      <td>793.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.412567</td>\n",
       "      <td>0.488383</td>\n",
       "      <td>0.446163</td>\n",
       "      <td>108.50000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.006098</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>41.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.25</td>\n",
       "      <td>0.421943</td>\n",
       "      <td>0.577206</td>\n",
       "      <td>0.483667</td>\n",
       "      <td>84.50000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.75</td>\n",
       "      <td>0.439164</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.488115</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.50000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.416785</td>\n",
       "      <td>0.594273</td>\n",
       "      <td>0.488782</td>\n",
       "      <td>147.25</td>\n",
       "      <td>0.697318</td>\n",
       "      <td>0.218044</td>\n",
       "      <td>0.256177</td>\n",
       "      <td>0.231506</td>\n",
       "      <td>1250.50000</td>\n",
       "      <td>0.678163</td>\n",
       "      <td>0.697318</td>\n",
       "      <td>0.683310</td>\n",
       "      <td>1250.50000</td>\n",
       "      <td>12.437150</td>\n",
       "      <td>101.21075</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.054015</td>\n",
       "      <td>0.018829</td>\n",
       "      <td>0.010854</td>\n",
       "      <td>0.009364</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.075622</td>\n",
       "      <td>0.076557</td>\n",
       "      <td>0.071466</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.025306</td>\n",
       "      <td>0.083415</td>\n",
       "      <td>0.017952</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.045701</td>\n",
       "      <td>0.085391</td>\n",
       "      <td>0.026286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023006</td>\n",
       "      <td>0.066793</td>\n",
       "      <td>0.028579</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.009461</td>\n",
       "      <td>0.007589</td>\n",
       "      <td>0.014348</td>\n",
       "      <td>0.007199</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.008430</td>\n",
       "      <td>0.009461</td>\n",
       "      <td>0.008498</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>1.128946</td>\n",
       "      <td>9.77997</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:16, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Others  Precision</th>\n",
       "      <th>Others  Recall</th>\n",
       "      <th>Others  F1-score</th>\n",
       "      <th>Others  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.992509</td>\n",
       "      <td>0.812349</td>\n",
       "      <td>0.845088</td>\n",
       "      <td>0.828395</td>\n",
       "      <td>794</td>\n",
       "      <td>0.247934</td>\n",
       "      <td>0.275229</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.284404</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.409241</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>0.258503</td>\n",
       "      <td>0.327586</td>\n",
       "      <td>147</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>55</td>\n",
       "      <td>0.641087</td>\n",
       "      <td>0.398821</td>\n",
       "      <td>0.303774</td>\n",
       "      <td>0.265972</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.653015</td>\n",
       "      <td>0.641087</td>\n",
       "      <td>0.616375</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.885449</td>\n",
       "      <td>0.809965</td>\n",
       "      <td>0.880353</td>\n",
       "      <td>0.843693</td>\n",
       "      <td>794</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.091743</td>\n",
       "      <td>0.144928</td>\n",
       "      <td>109</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>41</td>\n",
       "      <td>0.295337</td>\n",
       "      <td>0.670588</td>\n",
       "      <td>0.410072</td>\n",
       "      <td>85</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>20</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.272109</td>\n",
       "      <td>0.337553</td>\n",
       "      <td>147</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.218182</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>55</td>\n",
       "      <td>0.669065</td>\n",
       "      <td>0.433882</td>\n",
       "      <td>0.433108</td>\n",
       "      <td>0.379365</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.652391</td>\n",
       "      <td>0.669065</td>\n",
       "      <td>0.640026</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.008200</td>\n",
       "      <td>0.840919</td>\n",
       "      <td>0.819523</td>\n",
       "      <td>0.909320</td>\n",
       "      <td>0.862090</td>\n",
       "      <td>794</td>\n",
       "      <td>0.365672</td>\n",
       "      <td>0.449541</td>\n",
       "      <td>0.403292</td>\n",
       "      <td>109</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>41</td>\n",
       "      <td>0.480519</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>85</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.464912</td>\n",
       "      <td>0.360544</td>\n",
       "      <td>0.406130</td>\n",
       "      <td>147</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>55</td>\n",
       "      <td>0.705835</td>\n",
       "      <td>0.497140</td>\n",
       "      <td>0.432141</td>\n",
       "      <td>0.442138</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.675828</td>\n",
       "      <td>0.705835</td>\n",
       "      <td>0.682347</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.008200</td>\n",
       "      <td>0.785800</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.886650</td>\n",
       "      <td>0.875622</td>\n",
       "      <td>794</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.495413</td>\n",
       "      <td>0.497696</td>\n",
       "      <td>109</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>41</td>\n",
       "      <td>0.514019</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.572917</td>\n",
       "      <td>85</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>20</td>\n",
       "      <td>0.464706</td>\n",
       "      <td>0.537415</td>\n",
       "      <td>0.498423</td>\n",
       "      <td>147</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>55</td>\n",
       "      <td>0.732214</td>\n",
       "      <td>0.514662</td>\n",
       "      <td>0.503487</td>\n",
       "      <td>0.485538</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.715737</td>\n",
       "      <td>0.732214</td>\n",
       "      <td>0.717709</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.654200</td>\n",
       "      <td>0.787168</td>\n",
       "      <td>0.901218</td>\n",
       "      <td>0.838791</td>\n",
       "      <td>0.868885</td>\n",
       "      <td>794</td>\n",
       "      <td>0.462585</td>\n",
       "      <td>0.623853</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>109</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>41</td>\n",
       "      <td>0.553398</td>\n",
       "      <td>0.670588</td>\n",
       "      <td>0.606383</td>\n",
       "      <td>85</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>20</td>\n",
       "      <td>0.450495</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.521490</td>\n",
       "      <td>147</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.197531</td>\n",
       "      <td>55</td>\n",
       "      <td>0.727418</td>\n",
       "      <td>0.541539</td>\n",
       "      <td>0.542185</td>\n",
       "      <td>0.512065</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.742591</td>\n",
       "      <td>0.727418</td>\n",
       "      <td>0.725398</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7322142286171063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Others  Precision</th>\n",
       "      <th>Others  Recall</th>\n",
       "      <th>Others  F1-score</th>\n",
       "      <th>Others  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.000948</td>\n",
       "      <td>0.756356</td>\n",
       "      <td>0.899244</td>\n",
       "      <td>0.821634</td>\n",
       "      <td>794</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>0.018349</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.355649</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>0.439276</td>\n",
       "      <td>148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55</td>\n",
       "      <td>0.656275</td>\n",
       "      <td>0.346491</td>\n",
       "      <td>0.247127</td>\n",
       "      <td>0.222225</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.629756</td>\n",
       "      <td>0.656275</td>\n",
       "      <td>0.593812</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.837410</td>\n",
       "      <td>0.849127</td>\n",
       "      <td>0.857683</td>\n",
       "      <td>0.853383</td>\n",
       "      <td>794</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>0.231579</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>85</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.394191</td>\n",
       "      <td>0.641892</td>\n",
       "      <td>0.488432</td>\n",
       "      <td>148</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>55</td>\n",
       "      <td>0.677858</td>\n",
       "      <td>0.466137</td>\n",
       "      <td>0.378352</td>\n",
       "      <td>0.377021</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.668975</td>\n",
       "      <td>0.677858</td>\n",
       "      <td>0.659025</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.002300</td>\n",
       "      <td>0.796516</td>\n",
       "      <td>0.902643</td>\n",
       "      <td>0.817380</td>\n",
       "      <td>0.857898</td>\n",
       "      <td>794</td>\n",
       "      <td>0.304636</td>\n",
       "      <td>0.425926</td>\n",
       "      <td>0.355212</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.366906</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.455357</td>\n",
       "      <td>85</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>20</td>\n",
       "      <td>0.502703</td>\n",
       "      <td>0.628378</td>\n",
       "      <td>0.558559</td>\n",
       "      <td>148</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>55</td>\n",
       "      <td>0.689049</td>\n",
       "      <td>0.424404</td>\n",
       "      <td>0.471929</td>\n",
       "      <td>0.439050</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.706372</td>\n",
       "      <td>0.689049</td>\n",
       "      <td>0.692076</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.002300</td>\n",
       "      <td>0.763758</td>\n",
       "      <td>0.888166</td>\n",
       "      <td>0.860202</td>\n",
       "      <td>0.873960</td>\n",
       "      <td>794</td>\n",
       "      <td>0.398230</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.407240</td>\n",
       "      <td>108</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.463158</td>\n",
       "      <td>0.517647</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>85</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.438776</td>\n",
       "      <td>0.581081</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>148</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.288462</td>\n",
       "      <td>55</td>\n",
       "      <td>0.711431</td>\n",
       "      <td>0.512700</td>\n",
       "      <td>0.485126</td>\n",
       "      <td>0.481222</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.719887</td>\n",
       "      <td>0.711431</td>\n",
       "      <td>0.710541</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.634400</td>\n",
       "      <td>0.774982</td>\n",
       "      <td>0.910204</td>\n",
       "      <td>0.842569</td>\n",
       "      <td>0.875082</td>\n",
       "      <td>794</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>108</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>41</td>\n",
       "      <td>0.439252</td>\n",
       "      <td>0.552941</td>\n",
       "      <td>0.489583</td>\n",
       "      <td>85</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>20</td>\n",
       "      <td>0.456731</td>\n",
       "      <td>0.641892</td>\n",
       "      <td>0.533708</td>\n",
       "      <td>148</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>55</td>\n",
       "      <td>0.709033</td>\n",
       "      <td>0.494366</td>\n",
       "      <td>0.490802</td>\n",
       "      <td>0.482271</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.727009</td>\n",
       "      <td>0.709033</td>\n",
       "      <td>0.713197</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7114308553157475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Others  Precision</th>\n",
       "      <th>Others  Recall</th>\n",
       "      <th>Others  F1-score</th>\n",
       "      <th>Others  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.168546</td>\n",
       "      <td>0.813986</td>\n",
       "      <td>0.732997</td>\n",
       "      <td>0.771372</td>\n",
       "      <td>794</td>\n",
       "      <td>0.134146</td>\n",
       "      <td>0.101852</td>\n",
       "      <td>0.115789</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.245946</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.352031</td>\n",
       "      <td>147</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.125984</td>\n",
       "      <td>56</td>\n",
       "      <td>0.554400</td>\n",
       "      <td>0.198584</td>\n",
       "      <td>0.229808</td>\n",
       "      <td>0.198001</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.568205</td>\n",
       "      <td>0.554400</td>\n",
       "      <td>0.548422</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.966809</td>\n",
       "      <td>0.836576</td>\n",
       "      <td>0.812343</td>\n",
       "      <td>0.824281</td>\n",
       "      <td>794</td>\n",
       "      <td>0.197183</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.224000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.260417</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.362319</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.374150</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>147</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>56</td>\n",
       "      <td>0.623200</td>\n",
       "      <td>0.311256</td>\n",
       "      <td>0.294121</td>\n",
       "      <td>0.260628</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.633560</td>\n",
       "      <td>0.623200</td>\n",
       "      <td>0.613437</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.063100</td>\n",
       "      <td>0.917192</td>\n",
       "      <td>0.849934</td>\n",
       "      <td>0.806045</td>\n",
       "      <td>0.827408</td>\n",
       "      <td>794</td>\n",
       "      <td>0.197917</td>\n",
       "      <td>0.175926</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>108</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>41</td>\n",
       "      <td>0.296703</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.406015</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.319249</td>\n",
       "      <td>0.462585</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>147</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>56</td>\n",
       "      <td>0.628800</td>\n",
       "      <td>0.487686</td>\n",
       "      <td>0.313757</td>\n",
       "      <td>0.285682</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.683860</td>\n",
       "      <td>0.628800</td>\n",
       "      <td>0.620837</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.063100</td>\n",
       "      <td>0.871450</td>\n",
       "      <td>0.868280</td>\n",
       "      <td>0.813602</td>\n",
       "      <td>0.840052</td>\n",
       "      <td>794</td>\n",
       "      <td>0.259669</td>\n",
       "      <td>0.435185</td>\n",
       "      <td>0.325260</td>\n",
       "      <td>108</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>41</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>84</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>20</td>\n",
       "      <td>0.372449</td>\n",
       "      <td>0.496599</td>\n",
       "      <td>0.425656</td>\n",
       "      <td>147</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>56</td>\n",
       "      <td>0.653600</td>\n",
       "      <td>0.415416</td>\n",
       "      <td>0.384707</td>\n",
       "      <td>0.383971</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.673680</td>\n",
       "      <td>0.653600</td>\n",
       "      <td>0.657219</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.697300</td>\n",
       "      <td>0.874430</td>\n",
       "      <td>0.869919</td>\n",
       "      <td>0.808564</td>\n",
       "      <td>0.838120</td>\n",
       "      <td>794</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>108</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>41</td>\n",
       "      <td>0.403670</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.455959</td>\n",
       "      <td>84</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>20</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>0.517007</td>\n",
       "      <td>0.440580</td>\n",
       "      <td>147</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>56</td>\n",
       "      <td>0.664800</td>\n",
       "      <td>0.456730</td>\n",
       "      <td>0.426347</td>\n",
       "      <td>0.420950</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.688883</td>\n",
       "      <td>0.664800</td>\n",
       "      <td>0.670052</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Others  Precision</th>\n",
       "      <th>Others  Recall</th>\n",
       "      <th>Others  F1-score</th>\n",
       "      <th>Others  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.980614</td>\n",
       "      <td>0.768973</td>\n",
       "      <td>0.868852</td>\n",
       "      <td>0.815867</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.242291</td>\n",
       "      <td>0.654762</td>\n",
       "      <td>0.353698</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.442623</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.401487</td>\n",
       "      <td>147</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>55</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.264841</td>\n",
       "      <td>0.275332</td>\n",
       "      <td>0.233960</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.573771</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.591503</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.909505</td>\n",
       "      <td>0.779888</td>\n",
       "      <td>0.880202</td>\n",
       "      <td>0.827014</td>\n",
       "      <td>793</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.119266</td>\n",
       "      <td>0.171053</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.292683</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.381944</td>\n",
       "      <td>0.374150</td>\n",
       "      <td>0.378007</td>\n",
       "      <td>147</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>55</td>\n",
       "      <td>0.652000</td>\n",
       "      <td>0.286692</td>\n",
       "      <td>0.280461</td>\n",
       "      <td>0.256724</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.596709</td>\n",
       "      <td>0.652000</td>\n",
       "      <td>0.611532</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.024500</td>\n",
       "      <td>0.916583</td>\n",
       "      <td>0.839037</td>\n",
       "      <td>0.834805</td>\n",
       "      <td>0.836915</td>\n",
       "      <td>793</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.091743</td>\n",
       "      <td>0.140845</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.324503</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.417021</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.371795</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.456693</td>\n",
       "      <td>147</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>55</td>\n",
       "      <td>0.654400</td>\n",
       "      <td>0.295846</td>\n",
       "      <td>0.326219</td>\n",
       "      <td>0.293651</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.634471</td>\n",
       "      <td>0.654400</td>\n",
       "      <td>0.633931</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.024500</td>\n",
       "      <td>0.887477</td>\n",
       "      <td>0.864973</td>\n",
       "      <td>0.815889</td>\n",
       "      <td>0.839714</td>\n",
       "      <td>793</td>\n",
       "      <td>0.316456</td>\n",
       "      <td>0.458716</td>\n",
       "      <td>0.374532</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.408602</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.429379</td>\n",
       "      <td>84</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>20</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>147</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.247788</td>\n",
       "      <td>55</td>\n",
       "      <td>0.660800</td>\n",
       "      <td>0.435977</td>\n",
       "      <td>0.388761</td>\n",
       "      <td>0.386923</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.673269</td>\n",
       "      <td>0.660800</td>\n",
       "      <td>0.662139</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.724900</td>\n",
       "      <td>0.899333</td>\n",
       "      <td>0.857702</td>\n",
       "      <td>0.828499</td>\n",
       "      <td>0.842848</td>\n",
       "      <td>793</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>0.440367</td>\n",
       "      <td>0.395062</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.480874</td>\n",
       "      <td>84</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>20</td>\n",
       "      <td>0.381443</td>\n",
       "      <td>0.503401</td>\n",
       "      <td>0.434018</td>\n",
       "      <td>147</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.236364</td>\n",
       "      <td>0.242991</td>\n",
       "      <td>55</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.470257</td>\n",
       "      <td>0.390349</td>\n",
       "      <td>0.389875</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.677087</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.668532</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.672\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_none__precision</th>\n",
       "      <th>eval_none__recall</th>\n",
       "      <th>eval_none__f1-score</th>\n",
       "      <th>eval_none__support</th>\n",
       "      <th>eval_causal__precision</th>\n",
       "      <th>eval_causal__recall</th>\n",
       "      <th>eval_causal__f1-score</th>\n",
       "      <th>eval_causal__support</th>\n",
       "      <th>eval_contrast__precision</th>\n",
       "      <th>eval_contrast__recall</th>\n",
       "      <th>eval_contrast__f1-score</th>\n",
       "      <th>eval_contrast__support</th>\n",
       "      <th>eval_equivalence__precision</th>\n",
       "      <th>eval_equivalence__recall</th>\n",
       "      <th>eval_equivalence__f1-score</th>\n",
       "      <th>eval_equivalence__support</th>\n",
       "      <th>eval_identity__precision</th>\n",
       "      <th>eval_identity__recall</th>\n",
       "      <th>eval_identity__f1-score</th>\n",
       "      <th>eval_identity__support</th>\n",
       "      <th>eval_temporal__precision</th>\n",
       "      <th>eval_temporal__recall</th>\n",
       "      <th>eval_temporal__f1-score</th>\n",
       "      <th>eval_temporal__support</th>\n",
       "      <th>eval_others__precision</th>\n",
       "      <th>eval_others__recall</th>\n",
       "      <th>eval_others__f1-score</th>\n",
       "      <th>eval_others__support</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_macro avg__precision</th>\n",
       "      <th>eval_macro avg__recall</th>\n",
       "      <th>eval_macro avg__f1-score</th>\n",
       "      <th>eval_macro avg__support</th>\n",
       "      <th>eval_weighted avg__precision</th>\n",
       "      <th>eval_weighted avg__recall</th>\n",
       "      <th>eval_weighted avg__f1-score</th>\n",
       "      <th>eval_weighted avg__support</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.785800</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.886650</td>\n",
       "      <td>0.875622</td>\n",
       "      <td>794.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.495413</td>\n",
       "      <td>0.497696</td>\n",
       "      <td>109.00000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.514019</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.572917</td>\n",
       "      <td>85.00000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.464706</td>\n",
       "      <td>0.537415</td>\n",
       "      <td>0.498423</td>\n",
       "      <td>147.00</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>55.00</td>\n",
       "      <td>0.732214</td>\n",
       "      <td>0.514662</td>\n",
       "      <td>0.503487</td>\n",
       "      <td>0.485538</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>0.715737</td>\n",
       "      <td>0.732214</td>\n",
       "      <td>0.717709</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>13.182100</td>\n",
       "      <td>94.902000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.763758</td>\n",
       "      <td>0.888166</td>\n",
       "      <td>0.860202</td>\n",
       "      <td>0.873960</td>\n",
       "      <td>794.00</td>\n",
       "      <td>0.398230</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.407240</td>\n",
       "      <td>108.00000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.463158</td>\n",
       "      <td>0.517647</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>85.00000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.438776</td>\n",
       "      <td>0.581081</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>148.00</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.288462</td>\n",
       "      <td>55.00</td>\n",
       "      <td>0.711431</td>\n",
       "      <td>0.512700</td>\n",
       "      <td>0.485126</td>\n",
       "      <td>0.481222</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>0.719887</td>\n",
       "      <td>0.711431</td>\n",
       "      <td>0.710541</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>13.178100</td>\n",
       "      <td>94.930000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.874430</td>\n",
       "      <td>0.869919</td>\n",
       "      <td>0.808564</td>\n",
       "      <td>0.838120</td>\n",
       "      <td>794.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>108.00000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.403670</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.455959</td>\n",
       "      <td>84.00000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>0.517007</td>\n",
       "      <td>0.440580</td>\n",
       "      <td>147.00</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>56.00</td>\n",
       "      <td>0.664800</td>\n",
       "      <td>0.456730</td>\n",
       "      <td>0.426347</td>\n",
       "      <td>0.420950</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>0.688883</td>\n",
       "      <td>0.664800</td>\n",
       "      <td>0.670052</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>12.277500</td>\n",
       "      <td>101.813000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.899333</td>\n",
       "      <td>0.857702</td>\n",
       "      <td>0.828499</td>\n",
       "      <td>0.842848</td>\n",
       "      <td>793.00</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>0.440367</td>\n",
       "      <td>0.395062</td>\n",
       "      <td>109.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.00</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.480874</td>\n",
       "      <td>84.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.381443</td>\n",
       "      <td>0.503401</td>\n",
       "      <td>0.434018</td>\n",
       "      <td>147.00</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.236364</td>\n",
       "      <td>0.242991</td>\n",
       "      <td>55.00</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.470257</td>\n",
       "      <td>0.390349</td>\n",
       "      <td>0.389875</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>0.677087</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.668532</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>10.728400</td>\n",
       "      <td>116.514000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>0.830830</td>\n",
       "      <td>0.870163</td>\n",
       "      <td>0.845979</td>\n",
       "      <td>0.857638</td>\n",
       "      <td>793.75</td>\n",
       "      <td>0.397443</td>\n",
       "      <td>0.442278</td>\n",
       "      <td>0.417592</td>\n",
       "      <td>108.50000</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.067073</td>\n",
       "      <td>0.110296</td>\n",
       "      <td>41.25</td>\n",
       "      <td>0.456323</td>\n",
       "      <td>0.553081</td>\n",
       "      <td>0.499660</td>\n",
       "      <td>84.50000</td>\n",
       "      <td>0.683611</td>\n",
       "      <td>0.512500</td>\n",
       "      <td>0.528874</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.417191</td>\n",
       "      <td>0.534726</td>\n",
       "      <td>0.468255</td>\n",
       "      <td>147.25</td>\n",
       "      <td>0.275935</td>\n",
       "      <td>0.203653</td>\n",
       "      <td>0.228459</td>\n",
       "      <td>55.25</td>\n",
       "      <td>0.695111</td>\n",
       "      <td>0.488587</td>\n",
       "      <td>0.451327</td>\n",
       "      <td>0.444396</td>\n",
       "      <td>1250.50000</td>\n",
       "      <td>0.700398</td>\n",
       "      <td>0.695111</td>\n",
       "      <td>0.691709</td>\n",
       "      <td>1250.50000</td>\n",
       "      <td>12.341525</td>\n",
       "      <td>102.039750</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.066131</td>\n",
       "      <td>0.013007</td>\n",
       "      <td>0.034457</td>\n",
       "      <td>0.019913</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.073412</td>\n",
       "      <td>0.037143</td>\n",
       "      <td>0.055562</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.223952</td>\n",
       "      <td>0.054082</td>\n",
       "      <td>0.086850</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.045786</td>\n",
       "      <td>0.062719</td>\n",
       "      <td>0.050811</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.231181</td>\n",
       "      <td>0.265754</td>\n",
       "      <td>0.180503</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041287</td>\n",
       "      <td>0.033918</td>\n",
       "      <td>0.035851</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.025009</td>\n",
       "      <td>0.070322</td>\n",
       "      <td>0.054180</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.032124</td>\n",
       "      <td>0.029508</td>\n",
       "      <td>0.052299</td>\n",
       "      <td>0.046801</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.020746</td>\n",
       "      <td>0.032124</td>\n",
       "      <td>0.026057</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>1.156532</td>\n",
       "      <td>10.182523</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Attribution  Precision</th>\n",
       "      <th>Attribution  Recall</th>\n",
       "      <th>Attribution  F1-score</th>\n",
       "      <th>Attribution  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Conditional  Precision</th>\n",
       "      <th>Conditional  Recall</th>\n",
       "      <th>Conditional  F1-score</th>\n",
       "      <th>Conditional  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Description  Precision</th>\n",
       "      <th>Description  Recall</th>\n",
       "      <th>Description  F1-score</th>\n",
       "      <th>Description  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Fulfillment  Precision</th>\n",
       "      <th>Fulfillment  Recall</th>\n",
       "      <th>Fulfillment  F1-score</th>\n",
       "      <th>Fulfillment  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Purpose  Precision</th>\n",
       "      <th>Purpose  Recall</th>\n",
       "      <th>Purpose  F1-score</th>\n",
       "      <th>Purpose  Support</th>\n",
       "      <th>Summary  Precision</th>\n",
       "      <th>Summary  Recall</th>\n",
       "      <th>Summary  F1-score</th>\n",
       "      <th>Summary  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.079407</td>\n",
       "      <td>0.677716</td>\n",
       "      <td>0.926952</td>\n",
       "      <td>0.782979</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.110092</td>\n",
       "      <td>0.154839</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.278912</td>\n",
       "      <td>0.321569</td>\n",
       "      <td>147</td>\n",
       "      <td>0.632294</td>\n",
       "      <td>0.125003</td>\n",
       "      <td>0.111647</td>\n",
       "      <td>0.108458</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.509688</td>\n",
       "      <td>0.632294</td>\n",
       "      <td>0.551055</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.992112</td>\n",
       "      <td>0.858453</td>\n",
       "      <td>0.824937</td>\n",
       "      <td>0.841362</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.055046</td>\n",
       "      <td>0.096000</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.277108</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.414414</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.412556</td>\n",
       "      <td>0.625850</td>\n",
       "      <td>0.497297</td>\n",
       "      <td>147</td>\n",
       "      <td>0.657074</td>\n",
       "      <td>0.160260</td>\n",
       "      <td>0.193938</td>\n",
       "      <td>0.154089</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.644612</td>\n",
       "      <td>0.657074</td>\n",
       "      <td>0.628632</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.125500</td>\n",
       "      <td>0.932905</td>\n",
       "      <td>0.878830</td>\n",
       "      <td>0.794710</td>\n",
       "      <td>0.834656</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.385321</td>\n",
       "      <td>0.348548</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.365517</td>\n",
       "      <td>0.630952</td>\n",
       "      <td>0.462882</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.395918</td>\n",
       "      <td>0.659864</td>\n",
       "      <td>0.494898</td>\n",
       "      <td>147</td>\n",
       "      <td>0.663469</td>\n",
       "      <td>0.216234</td>\n",
       "      <td>0.235071</td>\n",
       "      <td>0.216050</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.666750</td>\n",
       "      <td>0.663469</td>\n",
       "      <td>0.656573</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.125500</td>\n",
       "      <td>0.913321</td>\n",
       "      <td>0.876316</td>\n",
       "      <td>0.838791</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.513761</td>\n",
       "      <td>0.414815</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.448864</td>\n",
       "      <td>0.537415</td>\n",
       "      <td>0.489164</td>\n",
       "      <td>147</td>\n",
       "      <td>0.691447</td>\n",
       "      <td>0.230737</td>\n",
       "      <td>0.243212</td>\n",
       "      <td>0.227316</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.677011</td>\n",
       "      <td>0.691447</td>\n",
       "      <td>0.678678</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.764300</td>\n",
       "      <td>0.930380</td>\n",
       "      <td>0.878113</td>\n",
       "      <td>0.843829</td>\n",
       "      <td>0.860629</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.401639</td>\n",
       "      <td>0.449541</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.446970</td>\n",
       "      <td>0.702381</td>\n",
       "      <td>0.546296</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.466346</td>\n",
       "      <td>0.659864</td>\n",
       "      <td>0.546479</td>\n",
       "      <td>147</td>\n",
       "      <td>0.711431</td>\n",
       "      <td>0.230833</td>\n",
       "      <td>0.283801</td>\n",
       "      <td>0.252485</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.686360</td>\n",
       "      <td>0.711431</td>\n",
       "      <td>0.694522</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7114308553157475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Attribution  Precision</th>\n",
       "      <th>Attribution  Recall</th>\n",
       "      <th>Attribution  F1-score</th>\n",
       "      <th>Attribution  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Conditional  Precision</th>\n",
       "      <th>Conditional  Recall</th>\n",
       "      <th>Conditional  F1-score</th>\n",
       "      <th>Conditional  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Description  Precision</th>\n",
       "      <th>Description  Recall</th>\n",
       "      <th>Description  F1-score</th>\n",
       "      <th>Description  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Fulfillment  Precision</th>\n",
       "      <th>Fulfillment  Recall</th>\n",
       "      <th>Fulfillment  F1-score</th>\n",
       "      <th>Fulfillment  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Purpose  Precision</th>\n",
       "      <th>Purpose  Recall</th>\n",
       "      <th>Purpose  F1-score</th>\n",
       "      <th>Purpose  Support</th>\n",
       "      <th>Summary  Precision</th>\n",
       "      <th>Summary  Recall</th>\n",
       "      <th>Summary  F1-score</th>\n",
       "      <th>Summary  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.063474</td>\n",
       "      <td>0.686047</td>\n",
       "      <td>0.965995</td>\n",
       "      <td>0.802301</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.092593</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>148</td>\n",
       "      <td>0.640288</td>\n",
       "      <td>0.115210</td>\n",
       "      <td>0.101729</td>\n",
       "      <td>0.096440</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.514273</td>\n",
       "      <td>0.640288</td>\n",
       "      <td>0.547980</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.010493</td>\n",
       "      <td>0.752753</td>\n",
       "      <td>0.947103</td>\n",
       "      <td>0.838818</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>0.202703</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>148</td>\n",
       "      <td>0.665867</td>\n",
       "      <td>0.118515</td>\n",
       "      <td>0.146412</td>\n",
       "      <td>0.125371</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.541432</td>\n",
       "      <td>0.665867</td>\n",
       "      <td>0.590432</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.112200</td>\n",
       "      <td>0.942800</td>\n",
       "      <td>0.869961</td>\n",
       "      <td>0.842569</td>\n",
       "      <td>0.856046</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.279621</td>\n",
       "      <td>0.702381</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.458101</td>\n",
       "      <td>0.554054</td>\n",
       "      <td>0.501529</td>\n",
       "      <td>148</td>\n",
       "      <td>0.674660</td>\n",
       "      <td>0.164771</td>\n",
       "      <td>0.201152</td>\n",
       "      <td>0.174798</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.657034</td>\n",
       "      <td>0.674660</td>\n",
       "      <td>0.658870</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.112200</td>\n",
       "      <td>0.915744</td>\n",
       "      <td>0.876153</td>\n",
       "      <td>0.837531</td>\n",
       "      <td>0.856407</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.374101</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.323077</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.392523</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.439791</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.495575</td>\n",
       "      <td>148</td>\n",
       "      <td>0.674660</td>\n",
       "      <td>0.245409</td>\n",
       "      <td>0.203048</td>\n",
       "      <td>0.184487</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.672205</td>\n",
       "      <td>0.674660</td>\n",
       "      <td>0.662360</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.772300</td>\n",
       "      <td>0.925294</td>\n",
       "      <td>0.878468</td>\n",
       "      <td>0.837531</td>\n",
       "      <td>0.857511</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.417582</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.434286</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>148</td>\n",
       "      <td>0.682654</td>\n",
       "      <td>0.209365</td>\n",
       "      <td>0.227997</td>\n",
       "      <td>0.210532</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.670533</td>\n",
       "      <td>0.682654</td>\n",
       "      <td>0.671347</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6826538768984812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Attribution  Precision</th>\n",
       "      <th>Attribution  Recall</th>\n",
       "      <th>Attribution  F1-score</th>\n",
       "      <th>Attribution  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Conditional  Precision</th>\n",
       "      <th>Conditional  Recall</th>\n",
       "      <th>Conditional  F1-score</th>\n",
       "      <th>Conditional  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Description  Precision</th>\n",
       "      <th>Description  Recall</th>\n",
       "      <th>Description  F1-score</th>\n",
       "      <th>Description  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Fulfillment  Precision</th>\n",
       "      <th>Fulfillment  Recall</th>\n",
       "      <th>Fulfillment  F1-score</th>\n",
       "      <th>Fulfillment  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Purpose  Precision</th>\n",
       "      <th>Purpose  Recall</th>\n",
       "      <th>Purpose  F1-score</th>\n",
       "      <th>Purpose  Support</th>\n",
       "      <th>Summary  Precision</th>\n",
       "      <th>Summary  Recall</th>\n",
       "      <th>Summary  F1-score</th>\n",
       "      <th>Summary  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.096000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.960957</td>\n",
       "      <td>0.809979</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.225490</td>\n",
       "      <td>0.156463</td>\n",
       "      <td>0.184739</td>\n",
       "      <td>147</td>\n",
       "      <td>0.646400</td>\n",
       "      <td>0.108733</td>\n",
       "      <td>0.114687</td>\n",
       "      <td>0.108534</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.496951</td>\n",
       "      <td>0.646400</td>\n",
       "      <td>0.557147</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.001097</td>\n",
       "      <td>0.830846</td>\n",
       "      <td>0.841310</td>\n",
       "      <td>0.836045</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.253731</td>\n",
       "      <td>0.157407</td>\n",
       "      <td>0.194286</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.741176</td>\n",
       "      <td>0.417219</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.265432</td>\n",
       "      <td>0.292517</td>\n",
       "      <td>0.278317</td>\n",
       "      <td>147</td>\n",
       "      <td>0.632800</td>\n",
       "      <td>0.136694</td>\n",
       "      <td>0.169368</td>\n",
       "      <td>0.143822</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.600632</td>\n",
       "      <td>0.632800</td>\n",
       "      <td>0.608943</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.090900</td>\n",
       "      <td>1.042858</td>\n",
       "      <td>0.903125</td>\n",
       "      <td>0.727960</td>\n",
       "      <td>0.806137</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.251748</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.286853</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.306011</td>\n",
       "      <td>0.658824</td>\n",
       "      <td>0.417910</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.271605</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.394619</td>\n",
       "      <td>0.598639</td>\n",
       "      <td>0.475676</td>\n",
       "      <td>147</td>\n",
       "      <td>0.615200</td>\n",
       "      <td>0.169653</td>\n",
       "      <td>0.239063</td>\n",
       "      <td>0.188182</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.665517</td>\n",
       "      <td>0.615200</td>\n",
       "      <td>0.625545</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.090900</td>\n",
       "      <td>1.004309</td>\n",
       "      <td>0.858073</td>\n",
       "      <td>0.829975</td>\n",
       "      <td>0.843790</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.323171</td>\n",
       "      <td>0.490741</td>\n",
       "      <td>0.389706</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.429907</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.383420</td>\n",
       "      <td>0.503401</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>147</td>\n",
       "      <td>0.670400</td>\n",
       "      <td>0.197464</td>\n",
       "      <td>0.222108</td>\n",
       "      <td>0.206774</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.653294</td>\n",
       "      <td>0.670400</td>\n",
       "      <td>0.658753</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.722200</td>\n",
       "      <td>1.001140</td>\n",
       "      <td>0.861219</td>\n",
       "      <td>0.836272</td>\n",
       "      <td>0.848562</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.326797</td>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.383142</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.420561</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.376344</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.420420</td>\n",
       "      <td>147</td>\n",
       "      <td>0.671200</td>\n",
       "      <td>0.192292</td>\n",
       "      <td>0.233736</td>\n",
       "      <td>0.209419</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.653299</td>\n",
       "      <td>0.671200</td>\n",
       "      <td>0.659701</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 09:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Attribution  Precision</th>\n",
       "      <th>Attribution  Recall</th>\n",
       "      <th>Attribution  F1-score</th>\n",
       "      <th>Attribution  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Conditional  Precision</th>\n",
       "      <th>Conditional  Recall</th>\n",
       "      <th>Conditional  F1-score</th>\n",
       "      <th>Conditional  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Description  Precision</th>\n",
       "      <th>Description  Recall</th>\n",
       "      <th>Description  F1-score</th>\n",
       "      <th>Description  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Fulfillment  Precision</th>\n",
       "      <th>Fulfillment  Recall</th>\n",
       "      <th>Fulfillment  F1-score</th>\n",
       "      <th>Fulfillment  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Purpose  Precision</th>\n",
       "      <th>Purpose  Recall</th>\n",
       "      <th>Purpose  F1-score</th>\n",
       "      <th>Purpose  Support</th>\n",
       "      <th>Summary  Precision</th>\n",
       "      <th>Summary  Recall</th>\n",
       "      <th>Summary  F1-score</th>\n",
       "      <th>Summary  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.084606</td>\n",
       "      <td>0.814010</td>\n",
       "      <td>0.849937</td>\n",
       "      <td>0.831585</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.212308</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.336585</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.432990</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.344262</td>\n",
       "      <td>147</td>\n",
       "      <td>0.628000</td>\n",
       "      <td>0.121609</td>\n",
       "      <td>0.162285</td>\n",
       "      <td>0.126036</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.581764</td>\n",
       "      <td>0.628000</td>\n",
       "      <td>0.590931</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.985756</td>\n",
       "      <td>0.824755</td>\n",
       "      <td>0.848676</td>\n",
       "      <td>0.836544</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.027523</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.255556</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.388732</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.359477</td>\n",
       "      <td>0.374150</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>147</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.142710</td>\n",
       "      <td>0.171843</td>\n",
       "      <td>0.136829</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.606659</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.604618</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.113400</td>\n",
       "      <td>0.953088</td>\n",
       "      <td>0.879383</td>\n",
       "      <td>0.790668</td>\n",
       "      <td>0.832669</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>0.412844</td>\n",
       "      <td>0.373444</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.338542</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.469314</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.517007</td>\n",
       "      <td>0.451039</td>\n",
       "      <td>147</td>\n",
       "      <td>0.661600</td>\n",
       "      <td>0.255597</td>\n",
       "      <td>0.259033</td>\n",
       "      <td>0.239791</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.681702</td>\n",
       "      <td>0.661600</td>\n",
       "      <td>0.659839</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.113400</td>\n",
       "      <td>0.905619</td>\n",
       "      <td>0.872456</td>\n",
       "      <td>0.810845</td>\n",
       "      <td>0.840523</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.633028</td>\n",
       "      <td>0.453947</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>0.507042</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.362500</td>\n",
       "      <td>0.394558</td>\n",
       "      <td>0.377850</td>\n",
       "      <td>147</td>\n",
       "      <td>0.672800</td>\n",
       "      <td>0.268251</td>\n",
       "      <td>0.268440</td>\n",
       "      <td>0.245040</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.686192</td>\n",
       "      <td>0.672800</td>\n",
       "      <td>0.666763</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.757800</td>\n",
       "      <td>0.901690</td>\n",
       "      <td>0.870396</td>\n",
       "      <td>0.804540</td>\n",
       "      <td>0.836173</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.439490</td>\n",
       "      <td>0.633028</td>\n",
       "      <td>0.518797</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.467213</td>\n",
       "      <td>0.670588</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.336898</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.377246</td>\n",
       "      <td>147</td>\n",
       "      <td>0.681600</td>\n",
       "      <td>0.258237</td>\n",
       "      <td>0.296353</td>\n",
       "      <td>0.265983</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.686050</td>\n",
       "      <td>0.681600</td>\n",
       "      <td>0.677188</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6816\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_none__precision</th>\n",
       "      <th>eval_none__recall</th>\n",
       "      <th>eval_none__f1-score</th>\n",
       "      <th>eval_none__support</th>\n",
       "      <th>eval_attribution__precision</th>\n",
       "      <th>eval_attribution__recall</th>\n",
       "      <th>eval_attribution__f1-score</th>\n",
       "      <th>eval_attribution__support</th>\n",
       "      <th>eval_causal__precision</th>\n",
       "      <th>eval_causal__recall</th>\n",
       "      <th>eval_causal__f1-score</th>\n",
       "      <th>eval_causal__support</th>\n",
       "      <th>eval_conditional__precision</th>\n",
       "      <th>eval_conditional__recall</th>\n",
       "      <th>eval_conditional__f1-score</th>\n",
       "      <th>eval_conditional__support</th>\n",
       "      <th>eval_contrast__precision</th>\n",
       "      <th>eval_contrast__recall</th>\n",
       "      <th>eval_contrast__f1-score</th>\n",
       "      <th>eval_contrast__support</th>\n",
       "      <th>eval_description__precision</th>\n",
       "      <th>eval_description__recall</th>\n",
       "      <th>eval_description__f1-score</th>\n",
       "      <th>eval_description__support</th>\n",
       "      <th>eval_equivalence__precision</th>\n",
       "      <th>eval_equivalence__recall</th>\n",
       "      <th>eval_equivalence__f1-score</th>\n",
       "      <th>eval_equivalence__support</th>\n",
       "      <th>eval_fulfillment__precision</th>\n",
       "      <th>eval_fulfillment__recall</th>\n",
       "      <th>eval_fulfillment__f1-score</th>\n",
       "      <th>eval_fulfillment__support</th>\n",
       "      <th>eval_identity__precision</th>\n",
       "      <th>eval_identity__recall</th>\n",
       "      <th>eval_identity__f1-score</th>\n",
       "      <th>eval_identity__support</th>\n",
       "      <th>eval_purpose__precision</th>\n",
       "      <th>eval_purpose__recall</th>\n",
       "      <th>eval_purpose__f1-score</th>\n",
       "      <th>eval_purpose__support</th>\n",
       "      <th>eval_summary__precision</th>\n",
       "      <th>eval_summary__recall</th>\n",
       "      <th>eval_summary__f1-score</th>\n",
       "      <th>eval_summary__support</th>\n",
       "      <th>eval_temporal__precision</th>\n",
       "      <th>eval_temporal__recall</th>\n",
       "      <th>eval_temporal__f1-score</th>\n",
       "      <th>eval_temporal__support</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_macro avg__precision</th>\n",
       "      <th>eval_macro avg__recall</th>\n",
       "      <th>eval_macro avg__f1-score</th>\n",
       "      <th>eval_macro avg__support</th>\n",
       "      <th>eval_weighted avg__precision</th>\n",
       "      <th>eval_weighted avg__recall</th>\n",
       "      <th>eval_weighted avg__f1-score</th>\n",
       "      <th>eval_weighted avg__support</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.930380</td>\n",
       "      <td>0.878113</td>\n",
       "      <td>0.843829</td>\n",
       "      <td>0.860629</td>\n",
       "      <td>794.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.401639</td>\n",
       "      <td>0.449541</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>109.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.446970</td>\n",
       "      <td>0.702381</td>\n",
       "      <td>0.546296</td>\n",
       "      <td>84.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.466346</td>\n",
       "      <td>0.659864</td>\n",
       "      <td>0.546479</td>\n",
       "      <td>147.00</td>\n",
       "      <td>0.711431</td>\n",
       "      <td>0.230833</td>\n",
       "      <td>0.283801</td>\n",
       "      <td>0.252485</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>0.686360</td>\n",
       "      <td>0.711431</td>\n",
       "      <td>0.694522</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>13.137200</td>\n",
       "      <td>95.226000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.925294</td>\n",
       "      <td>0.878468</td>\n",
       "      <td>0.837531</td>\n",
       "      <td>0.857511</td>\n",
       "      <td>794.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.417582</td>\n",
       "      <td>108.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>84.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.434286</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>148.00</td>\n",
       "      <td>0.682654</td>\n",
       "      <td>0.209365</td>\n",
       "      <td>0.227997</td>\n",
       "      <td>0.210532</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>0.670533</td>\n",
       "      <td>0.682654</td>\n",
       "      <td>0.671347</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>13.043100</td>\n",
       "      <td>95.913000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.001140</td>\n",
       "      <td>0.861219</td>\n",
       "      <td>0.836272</td>\n",
       "      <td>0.848562</td>\n",
       "      <td>794.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.326797</td>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.383142</td>\n",
       "      <td>108.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.420561</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>85.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.376344</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.420420</td>\n",
       "      <td>147.00</td>\n",
       "      <td>0.671200</td>\n",
       "      <td>0.192292</td>\n",
       "      <td>0.233736</td>\n",
       "      <td>0.209419</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>0.653299</td>\n",
       "      <td>0.671200</td>\n",
       "      <td>0.659701</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>12.258100</td>\n",
       "      <td>101.973000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.901690</td>\n",
       "      <td>0.870396</td>\n",
       "      <td>0.804540</td>\n",
       "      <td>0.836173</td>\n",
       "      <td>793.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.439490</td>\n",
       "      <td>0.633028</td>\n",
       "      <td>0.518797</td>\n",
       "      <td>109.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.467213</td>\n",
       "      <td>0.670588</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>85.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.336898</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.377246</td>\n",
       "      <td>147.00</td>\n",
       "      <td>0.681600</td>\n",
       "      <td>0.258237</td>\n",
       "      <td>0.296353</td>\n",
       "      <td>0.265983</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>0.686050</td>\n",
       "      <td>0.681600</td>\n",
       "      <td>0.677188</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>10.731700</td>\n",
       "      <td>116.477000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>0.939626</td>\n",
       "      <td>0.872049</td>\n",
       "      <td>0.830543</td>\n",
       "      <td>0.850719</td>\n",
       "      <td>793.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.378345</td>\n",
       "      <td>0.518327</td>\n",
       "      <td>0.435941</td>\n",
       "      <td>108.50000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.054878</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>41.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.25</td>\n",
       "      <td>0.422228</td>\n",
       "      <td>0.627381</td>\n",
       "      <td>0.503285</td>\n",
       "      <td>84.50000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.75</td>\n",
       "      <td>0.471088</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.495359</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.50000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.403469</td>\n",
       "      <td>0.519535</td>\n",
       "      <td>0.453683</td>\n",
       "      <td>147.25</td>\n",
       "      <td>0.686721</td>\n",
       "      <td>0.222682</td>\n",
       "      <td>0.260472</td>\n",
       "      <td>0.234605</td>\n",
       "      <td>1250.50000</td>\n",
       "      <td>0.674061</td>\n",
       "      <td>0.686721</td>\n",
       "      <td>0.675690</td>\n",
       "      <td>1250.50000</td>\n",
       "      <td>12.292525</td>\n",
       "      <td>102.397250</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.042872</td>\n",
       "      <td>0.008124</td>\n",
       "      <td>0.017648</td>\n",
       "      <td>0.010963</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.051705</td>\n",
       "      <td>0.083750</td>\n",
       "      <td>0.058100</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.109756</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.049231</td>\n",
       "      <td>0.076375</td>\n",
       "      <td>0.052977</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.106897</td>\n",
       "      <td>0.253311</td>\n",
       "      <td>0.156256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057939</td>\n",
       "      <td>0.099802</td>\n",
       "      <td>0.072677</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.017265</td>\n",
       "      <td>0.028469</td>\n",
       "      <td>0.034646</td>\n",
       "      <td>0.028972</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.015690</td>\n",
       "      <td>0.017265</td>\n",
       "      <td>0.014507</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>1.112684</td>\n",
       "      <td>9.863936</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Others  Precision</th>\n",
       "      <th>Others  Recall</th>\n",
       "      <th>Others  F1-score</th>\n",
       "      <th>Others  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.994385</td>\n",
       "      <td>0.771186</td>\n",
       "      <td>0.916877</td>\n",
       "      <td>0.837745</td>\n",
       "      <td>794</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.119266</td>\n",
       "      <td>0.164557</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.261468</td>\n",
       "      <td>0.670588</td>\n",
       "      <td>0.376238</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>0.196721</td>\n",
       "      <td>147</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>55</td>\n",
       "      <td>0.653877</td>\n",
       "      <td>0.328280</td>\n",
       "      <td>0.266506</td>\n",
       "      <td>0.234722</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.611083</td>\n",
       "      <td>0.653877</td>\n",
       "      <td>0.597708</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.906118</td>\n",
       "      <td>0.802632</td>\n",
       "      <td>0.845088</td>\n",
       "      <td>0.823313</td>\n",
       "      <td>794</td>\n",
       "      <td>0.278689</td>\n",
       "      <td>0.155963</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>109</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>41</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.457565</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.360902</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>147</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>55</td>\n",
       "      <td>0.643485</td>\n",
       "      <td>0.325900</td>\n",
       "      <td>0.313812</td>\n",
       "      <td>0.289698</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.617268</td>\n",
       "      <td>0.643485</td>\n",
       "      <td>0.619377</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.004900</td>\n",
       "      <td>0.888227</td>\n",
       "      <td>0.824540</td>\n",
       "      <td>0.846348</td>\n",
       "      <td>0.835301</td>\n",
       "      <td>794</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.449541</td>\n",
       "      <td>0.372624</td>\n",
       "      <td>109</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.170732</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.432099</td>\n",
       "      <td>85</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>20</td>\n",
       "      <td>0.418440</td>\n",
       "      <td>0.401361</td>\n",
       "      <td>0.409722</td>\n",
       "      <td>147</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>55</td>\n",
       "      <td>0.667466</td>\n",
       "      <td>0.492797</td>\n",
       "      <td>0.382171</td>\n",
       "      <td>0.403654</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.668823</td>\n",
       "      <td>0.667466</td>\n",
       "      <td>0.661520</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.004900</td>\n",
       "      <td>0.851704</td>\n",
       "      <td>0.824519</td>\n",
       "      <td>0.863980</td>\n",
       "      <td>0.843788</td>\n",
       "      <td>794</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.403670</td>\n",
       "      <td>0.405530</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.403509</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.462312</td>\n",
       "      <td>85</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.401316</td>\n",
       "      <td>0.414966</td>\n",
       "      <td>0.408027</td>\n",
       "      <td>147</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>55</td>\n",
       "      <td>0.681855</td>\n",
       "      <td>0.474115</td>\n",
       "      <td>0.381970</td>\n",
       "      <td>0.393386</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.661775</td>\n",
       "      <td>0.681855</td>\n",
       "      <td>0.666922</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.666900</td>\n",
       "      <td>0.868790</td>\n",
       "      <td>0.877778</td>\n",
       "      <td>0.795970</td>\n",
       "      <td>0.834875</td>\n",
       "      <td>794</td>\n",
       "      <td>0.407895</td>\n",
       "      <td>0.568807</td>\n",
       "      <td>0.475096</td>\n",
       "      <td>109</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>41</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>0.507042</td>\n",
       "      <td>85</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>20</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>147</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>55</td>\n",
       "      <td>0.674660</td>\n",
       "      <td>0.550002</td>\n",
       "      <td>0.447156</td>\n",
       "      <td>0.441430</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.709838</td>\n",
       "      <td>0.674660</td>\n",
       "      <td>0.677552</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6818545163868905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Others  Precision</th>\n",
       "      <th>Others  Recall</th>\n",
       "      <th>Others  F1-score</th>\n",
       "      <th>Others  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.021926</td>\n",
       "      <td>0.717413</td>\n",
       "      <td>0.908060</td>\n",
       "      <td>0.801556</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.105882</td>\n",
       "      <td>0.141732</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55</td>\n",
       "      <td>0.644285</td>\n",
       "      <td>0.186321</td>\n",
       "      <td>0.218208</td>\n",
       "      <td>0.196444</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.513971</td>\n",
       "      <td>0.644285</td>\n",
       "      <td>0.569458</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.924565</td>\n",
       "      <td>0.813174</td>\n",
       "      <td>0.855164</td>\n",
       "      <td>0.833640</td>\n",
       "      <td>794</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.123288</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.359788</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.313869</td>\n",
       "      <td>0.581081</td>\n",
       "      <td>0.407583</td>\n",
       "      <td>148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55</td>\n",
       "      <td>0.645883</td>\n",
       "      <td>0.241544</td>\n",
       "      <td>0.274225</td>\n",
       "      <td>0.246328</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.595907</td>\n",
       "      <td>0.645883</td>\n",
       "      <td>0.612414</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.007200</td>\n",
       "      <td>0.865311</td>\n",
       "      <td>0.880985</td>\n",
       "      <td>0.811083</td>\n",
       "      <td>0.844590</td>\n",
       "      <td>794</td>\n",
       "      <td>0.276190</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.364780</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.381356</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.443350</td>\n",
       "      <td>85</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>20</td>\n",
       "      <td>0.401099</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>0.442424</td>\n",
       "      <td>148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55</td>\n",
       "      <td>0.661871</td>\n",
       "      <td>0.404074</td>\n",
       "      <td>0.395825</td>\n",
       "      <td>0.378124</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.670573</td>\n",
       "      <td>0.661871</td>\n",
       "      <td>0.658832</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.007200</td>\n",
       "      <td>0.889199</td>\n",
       "      <td>0.887943</td>\n",
       "      <td>0.788413</td>\n",
       "      <td>0.835223</td>\n",
       "      <td>794</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.331169</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.426778</td>\n",
       "      <td>85</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.362162</td>\n",
       "      <td>0.452703</td>\n",
       "      <td>0.402402</td>\n",
       "      <td>148</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>55</td>\n",
       "      <td>0.649081</td>\n",
       "      <td>0.388831</td>\n",
       "      <td>0.433002</td>\n",
       "      <td>0.401902</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.676015</td>\n",
       "      <td>0.649081</td>\n",
       "      <td>0.656326</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.671800</td>\n",
       "      <td>0.877790</td>\n",
       "      <td>0.883784</td>\n",
       "      <td>0.823678</td>\n",
       "      <td>0.852673</td>\n",
       "      <td>794</td>\n",
       "      <td>0.358621</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.411067</td>\n",
       "      <td>108</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>41</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.476636</td>\n",
       "      <td>85</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>20</td>\n",
       "      <td>0.394872</td>\n",
       "      <td>0.520270</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>148</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>55</td>\n",
       "      <td>0.682654</td>\n",
       "      <td>0.489535</td>\n",
       "      <td>0.455644</td>\n",
       "      <td>0.447372</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.701021</td>\n",
       "      <td>0.682654</td>\n",
       "      <td>0.683099</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6826538768984812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:03, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Others  Precision</th>\n",
       "      <th>Others  Recall</th>\n",
       "      <th>Others  F1-score</th>\n",
       "      <th>Others  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.981110</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.826196</td>\n",
       "      <td>0.812887</td>\n",
       "      <td>794</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.120370</td>\n",
       "      <td>0.155689</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.243902</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.340314</td>\n",
       "      <td>0.442177</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>147</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>56</td>\n",
       "      <td>0.621600</td>\n",
       "      <td>0.256008</td>\n",
       "      <td>0.274072</td>\n",
       "      <td>0.251301</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.592008</td>\n",
       "      <td>0.621600</td>\n",
       "      <td>0.600439</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.967158</td>\n",
       "      <td>0.839262</td>\n",
       "      <td>0.802267</td>\n",
       "      <td>0.820348</td>\n",
       "      <td>794</td>\n",
       "      <td>0.241546</td>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.317460</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.286624</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.373444</td>\n",
       "      <td>84</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>20</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.345679</td>\n",
       "      <td>147</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.063492</td>\n",
       "      <td>56</td>\n",
       "      <td>0.628800</td>\n",
       "      <td>0.358188</td>\n",
       "      <td>0.374625</td>\n",
       "      <td>0.339281</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.644147</td>\n",
       "      <td>0.628800</td>\n",
       "      <td>0.624378</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.003100</td>\n",
       "      <td>0.888350</td>\n",
       "      <td>0.831039</td>\n",
       "      <td>0.836272</td>\n",
       "      <td>0.833647</td>\n",
       "      <td>794</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.257426</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.440476</td>\n",
       "      <td>0.430233</td>\n",
       "      <td>84</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>20</td>\n",
       "      <td>0.327660</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.403141</td>\n",
       "      <td>147</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.188235</td>\n",
       "      <td>56</td>\n",
       "      <td>0.652800</td>\n",
       "      <td>0.447373</td>\n",
       "      <td>0.340594</td>\n",
       "      <td>0.349431</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.646920</td>\n",
       "      <td>0.652800</td>\n",
       "      <td>0.641862</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.003100</td>\n",
       "      <td>0.862323</td>\n",
       "      <td>0.851562</td>\n",
       "      <td>0.823678</td>\n",
       "      <td>0.837388</td>\n",
       "      <td>794</td>\n",
       "      <td>0.339869</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.398467</td>\n",
       "      <td>108</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>41</td>\n",
       "      <td>0.517647</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.520710</td>\n",
       "      <td>84</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>20</td>\n",
       "      <td>0.351064</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>0.394030</td>\n",
       "      <td>147</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>56</td>\n",
       "      <td>0.670400</td>\n",
       "      <td>0.452230</td>\n",
       "      <td>0.444971</td>\n",
       "      <td>0.430237</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.677205</td>\n",
       "      <td>0.670400</td>\n",
       "      <td>0.667824</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>0.868268</td>\n",
       "      <td>0.861559</td>\n",
       "      <td>0.807305</td>\n",
       "      <td>0.833550</td>\n",
       "      <td>794</td>\n",
       "      <td>0.368794</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.417671</td>\n",
       "      <td>108</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>41</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>84</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>20</td>\n",
       "      <td>0.343284</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.396552</td>\n",
       "      <td>147</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.247191</td>\n",
       "      <td>56</td>\n",
       "      <td>0.670400</td>\n",
       "      <td>0.472854</td>\n",
       "      <td>0.458647</td>\n",
       "      <td>0.447623</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.686546</td>\n",
       "      <td>0.670400</td>\n",
       "      <td>0.672113</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 09:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Others  Precision</th>\n",
       "      <th>Others  Recall</th>\n",
       "      <th>Others  F1-score</th>\n",
       "      <th>Others  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.999537</td>\n",
       "      <td>0.800499</td>\n",
       "      <td>0.809584</td>\n",
       "      <td>0.805016</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.207031</td>\n",
       "      <td>0.630952</td>\n",
       "      <td>0.311765</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.517007</td>\n",
       "      <td>0.448378</td>\n",
       "      <td>147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55</td>\n",
       "      <td>0.616800</td>\n",
       "      <td>0.200480</td>\n",
       "      <td>0.279649</td>\n",
       "      <td>0.223594</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.568299</td>\n",
       "      <td>0.616800</td>\n",
       "      <td>0.584382</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.903779</td>\n",
       "      <td>0.855263</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.837090</td>\n",
       "      <td>793</td>\n",
       "      <td>0.279570</td>\n",
       "      <td>0.238532</td>\n",
       "      <td>0.257426</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.341270</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.385321</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.460274</td>\n",
       "      <td>147</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0.163636</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>55</td>\n",
       "      <td>0.649600</td>\n",
       "      <td>0.290176</td>\n",
       "      <td>0.329311</td>\n",
       "      <td>0.304426</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.642676</td>\n",
       "      <td>0.649600</td>\n",
       "      <td>0.642479</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.031500</td>\n",
       "      <td>0.891654</td>\n",
       "      <td>0.849934</td>\n",
       "      <td>0.807062</td>\n",
       "      <td>0.827943</td>\n",
       "      <td>793</td>\n",
       "      <td>0.328571</td>\n",
       "      <td>0.211009</td>\n",
       "      <td>0.256983</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>84</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>20</td>\n",
       "      <td>0.320513</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.393701</td>\n",
       "      <td>147</td>\n",
       "      <td>0.215385</td>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>55</td>\n",
       "      <td>0.641600</td>\n",
       "      <td>0.371643</td>\n",
       "      <td>0.350607</td>\n",
       "      <td>0.334309</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.649031</td>\n",
       "      <td>0.641600</td>\n",
       "      <td>0.637904</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.031500</td>\n",
       "      <td>0.868948</td>\n",
       "      <td>0.895894</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.828475</td>\n",
       "      <td>793</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.403670</td>\n",
       "      <td>0.339768</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.415254</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.485149</td>\n",
       "      <td>84</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>20</td>\n",
       "      <td>0.376068</td>\n",
       "      <td>0.598639</td>\n",
       "      <td>0.461942</td>\n",
       "      <td>147</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.236364</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>55</td>\n",
       "      <td>0.651200</td>\n",
       "      <td>0.406551</td>\n",
       "      <td>0.434643</td>\n",
       "      <td>0.411374</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.687338</td>\n",
       "      <td>0.651200</td>\n",
       "      <td>0.661367</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.695600</td>\n",
       "      <td>0.862931</td>\n",
       "      <td>0.892199</td>\n",
       "      <td>0.793190</td>\n",
       "      <td>0.839786</td>\n",
       "      <td>793</td>\n",
       "      <td>0.330935</td>\n",
       "      <td>0.422018</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.439655</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>84</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>20</td>\n",
       "      <td>0.377193</td>\n",
       "      <td>0.585034</td>\n",
       "      <td>0.458667</td>\n",
       "      <td>147</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>55</td>\n",
       "      <td>0.665600</td>\n",
       "      <td>0.408385</td>\n",
       "      <td>0.441315</td>\n",
       "      <td>0.417256</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.689239</td>\n",
       "      <td>0.665600</td>\n",
       "      <td>0.671203</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6656\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_none__precision</th>\n",
       "      <th>eval_none__recall</th>\n",
       "      <th>eval_none__f1-score</th>\n",
       "      <th>eval_none__support</th>\n",
       "      <th>eval_causal__precision</th>\n",
       "      <th>eval_causal__recall</th>\n",
       "      <th>eval_causal__f1-score</th>\n",
       "      <th>eval_causal__support</th>\n",
       "      <th>eval_contrast__precision</th>\n",
       "      <th>eval_contrast__recall</th>\n",
       "      <th>eval_contrast__f1-score</th>\n",
       "      <th>eval_contrast__support</th>\n",
       "      <th>eval_equivalence__precision</th>\n",
       "      <th>eval_equivalence__recall</th>\n",
       "      <th>eval_equivalence__f1-score</th>\n",
       "      <th>eval_equivalence__support</th>\n",
       "      <th>eval_identity__precision</th>\n",
       "      <th>eval_identity__recall</th>\n",
       "      <th>eval_identity__f1-score</th>\n",
       "      <th>eval_identity__support</th>\n",
       "      <th>eval_temporal__precision</th>\n",
       "      <th>eval_temporal__recall</th>\n",
       "      <th>eval_temporal__f1-score</th>\n",
       "      <th>eval_temporal__support</th>\n",
       "      <th>eval_others__precision</th>\n",
       "      <th>eval_others__recall</th>\n",
       "      <th>eval_others__f1-score</th>\n",
       "      <th>eval_others__support</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_macro avg__precision</th>\n",
       "      <th>eval_macro avg__recall</th>\n",
       "      <th>eval_macro avg__f1-score</th>\n",
       "      <th>eval_macro avg__support</th>\n",
       "      <th>eval_weighted avg__precision</th>\n",
       "      <th>eval_weighted avg__recall</th>\n",
       "      <th>eval_weighted avg__f1-score</th>\n",
       "      <th>eval_weighted avg__support</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.851704</td>\n",
       "      <td>0.824519</td>\n",
       "      <td>0.863980</td>\n",
       "      <td>0.843788</td>\n",
       "      <td>794.00</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.403670</td>\n",
       "      <td>0.405530</td>\n",
       "      <td>109.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.403509</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.462312</td>\n",
       "      <td>85.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.401316</td>\n",
       "      <td>0.414966</td>\n",
       "      <td>0.408027</td>\n",
       "      <td>147.00</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>55.00</td>\n",
       "      <td>0.681855</td>\n",
       "      <td>0.474115</td>\n",
       "      <td>0.381970</td>\n",
       "      <td>0.393386</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>0.661775</td>\n",
       "      <td>0.681855</td>\n",
       "      <td>0.666922</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>13.078300</td>\n",
       "      <td>95.655000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.877790</td>\n",
       "      <td>0.883784</td>\n",
       "      <td>0.823678</td>\n",
       "      <td>0.852673</td>\n",
       "      <td>794.00</td>\n",
       "      <td>0.358621</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.411067</td>\n",
       "      <td>108.00000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.476636</td>\n",
       "      <td>85.00000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.394872</td>\n",
       "      <td>0.520270</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>148.00</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>55.00</td>\n",
       "      <td>0.682654</td>\n",
       "      <td>0.489535</td>\n",
       "      <td>0.455644</td>\n",
       "      <td>0.447372</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>0.701021</td>\n",
       "      <td>0.682654</td>\n",
       "      <td>0.683099</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>13.024800</td>\n",
       "      <td>96.048000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.862323</td>\n",
       "      <td>0.851562</td>\n",
       "      <td>0.823678</td>\n",
       "      <td>0.837388</td>\n",
       "      <td>794.00</td>\n",
       "      <td>0.339869</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.398467</td>\n",
       "      <td>108.00000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.517647</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.520710</td>\n",
       "      <td>84.00000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.351064</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>0.394030</td>\n",
       "      <td>147.00</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>56.00</td>\n",
       "      <td>0.670400</td>\n",
       "      <td>0.452230</td>\n",
       "      <td>0.444971</td>\n",
       "      <td>0.430237</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>0.677205</td>\n",
       "      <td>0.670400</td>\n",
       "      <td>0.667824</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>12.180900</td>\n",
       "      <td>102.620000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.862931</td>\n",
       "      <td>0.892199</td>\n",
       "      <td>0.793190</td>\n",
       "      <td>0.839786</td>\n",
       "      <td>793.00</td>\n",
       "      <td>0.330935</td>\n",
       "      <td>0.422018</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>109.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.00</td>\n",
       "      <td>0.439655</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>84.00000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.377193</td>\n",
       "      <td>0.585034</td>\n",
       "      <td>0.458667</td>\n",
       "      <td>147.00</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>55.00</td>\n",
       "      <td>0.665600</td>\n",
       "      <td>0.408385</td>\n",
       "      <td>0.441315</td>\n",
       "      <td>0.417256</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>0.689239</td>\n",
       "      <td>0.665600</td>\n",
       "      <td>0.671203</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>10.351900</td>\n",
       "      <td>120.751000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>0.863687</td>\n",
       "      <td>0.863016</td>\n",
       "      <td>0.826131</td>\n",
       "      <td>0.843409</td>\n",
       "      <td>793.75</td>\n",
       "      <td>0.359208</td>\n",
       "      <td>0.447163</td>\n",
       "      <td>0.396508</td>\n",
       "      <td>108.50000</td>\n",
       "      <td>0.151923</td>\n",
       "      <td>0.042683</td>\n",
       "      <td>0.066449</td>\n",
       "      <td>41.25</td>\n",
       "      <td>0.439040</td>\n",
       "      <td>0.568032</td>\n",
       "      <td>0.492414</td>\n",
       "      <td>84.50000</td>\n",
       "      <td>0.718889</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.547452</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.381111</td>\n",
       "      <td>0.492312</td>\n",
       "      <td>0.427426</td>\n",
       "      <td>147.25</td>\n",
       "      <td>0.279276</td>\n",
       "      <td>0.140503</td>\n",
       "      <td>0.180780</td>\n",
       "      <td>55.25</td>\n",
       "      <td>0.675127</td>\n",
       "      <td>0.456066</td>\n",
       "      <td>0.430975</td>\n",
       "      <td>0.422063</td>\n",
       "      <td>1250.50000</td>\n",
       "      <td>0.682310</td>\n",
       "      <td>0.675127</td>\n",
       "      <td>0.672262</td>\n",
       "      <td>1250.50000</td>\n",
       "      <td>12.158975</td>\n",
       "      <td>103.768500</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.010722</td>\n",
       "      <td>0.031070</td>\n",
       "      <td>0.029038</td>\n",
       "      <td>0.006717</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.034141</td>\n",
       "      <td>0.040330</td>\n",
       "      <td>0.017791</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.175454</td>\n",
       "      <td>0.050282</td>\n",
       "      <td>0.077732</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.055830</td>\n",
       "      <td>0.041747</td>\n",
       "      <td>0.027478</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.224972</td>\n",
       "      <td>0.177951</td>\n",
       "      <td>0.118638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022478</td>\n",
       "      <td>0.075805</td>\n",
       "      <td>0.031263</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.012780</td>\n",
       "      <td>0.058679</td>\n",
       "      <td>0.051153</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.008466</td>\n",
       "      <td>0.035280</td>\n",
       "      <td>0.033231</td>\n",
       "      <td>0.022751</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.016792</td>\n",
       "      <td>0.008466</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>1.272899</td>\n",
       "      <td>11.763777</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Attribution  Precision</th>\n",
       "      <th>Attribution  Recall</th>\n",
       "      <th>Attribution  F1-score</th>\n",
       "      <th>Attribution  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Conditional  Precision</th>\n",
       "      <th>Conditional  Recall</th>\n",
       "      <th>Conditional  F1-score</th>\n",
       "      <th>Conditional  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Description  Precision</th>\n",
       "      <th>Description  Recall</th>\n",
       "      <th>Description  F1-score</th>\n",
       "      <th>Description  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Fulfillment  Precision</th>\n",
       "      <th>Fulfillment  Recall</th>\n",
       "      <th>Fulfillment  F1-score</th>\n",
       "      <th>Fulfillment  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Purpose  Precision</th>\n",
       "      <th>Purpose  Recall</th>\n",
       "      <th>Purpose  F1-score</th>\n",
       "      <th>Purpose  Support</th>\n",
       "      <th>Summary  Precision</th>\n",
       "      <th>Summary  Recall</th>\n",
       "      <th>Summary  F1-score</th>\n",
       "      <th>Summary  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.065094</td>\n",
       "      <td>0.743169</td>\n",
       "      <td>0.856423</td>\n",
       "      <td>0.795787</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>0.183486</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.322751</td>\n",
       "      <td>0.414966</td>\n",
       "      <td>0.363095</td>\n",
       "      <td>147</td>\n",
       "      <td>0.614708</td>\n",
       "      <td>0.120810</td>\n",
       "      <td>0.129176</td>\n",
       "      <td>0.122744</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.539150</td>\n",
       "      <td>0.614708</td>\n",
       "      <td>0.572551</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.968260</td>\n",
       "      <td>0.846554</td>\n",
       "      <td>0.819899</td>\n",
       "      <td>0.833013</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.192661</td>\n",
       "      <td>0.253012</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.385827</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.464455</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.342657</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.452656</td>\n",
       "      <td>147</td>\n",
       "      <td>0.661871</td>\n",
       "      <td>0.224455</td>\n",
       "      <td>0.226047</td>\n",
       "      <td>0.213803</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.647563</td>\n",
       "      <td>0.661871</td>\n",
       "      <td>0.644121</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.110100</td>\n",
       "      <td>0.928195</td>\n",
       "      <td>0.905185</td>\n",
       "      <td>0.769521</td>\n",
       "      <td>0.831858</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.385321</td>\n",
       "      <td>0.370044</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.427481</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.520930</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.349153</td>\n",
       "      <td>0.700680</td>\n",
       "      <td>0.466063</td>\n",
       "      <td>147</td>\n",
       "      <td>0.661071</td>\n",
       "      <td>0.208875</td>\n",
       "      <td>0.272682</td>\n",
       "      <td>0.230485</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.682752</td>\n",
       "      <td>0.661071</td>\n",
       "      <td>0.659183</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.110100</td>\n",
       "      <td>0.926172</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.787154</td>\n",
       "      <td>0.831117</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.307359</td>\n",
       "      <td>0.651376</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.433071</td>\n",
       "      <td>0.654762</td>\n",
       "      <td>0.521327</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.413580</td>\n",
       "      <td>0.455782</td>\n",
       "      <td>0.433657</td>\n",
       "      <td>147</td>\n",
       "      <td>0.664269</td>\n",
       "      <td>0.221112</td>\n",
       "      <td>0.266590</td>\n",
       "      <td>0.236491</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.673062</td>\n",
       "      <td>0.664269</td>\n",
       "      <td>0.659994</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.746700</td>\n",
       "      <td>0.917879</td>\n",
       "      <td>0.874145</td>\n",
       "      <td>0.804786</td>\n",
       "      <td>0.838033</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.568807</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.654762</td>\n",
       "      <td>0.539216</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.361809</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.416185</td>\n",
       "      <td>147</td>\n",
       "      <td>0.675460</td>\n",
       "      <td>0.217282</td>\n",
       "      <td>0.280679</td>\n",
       "      <td>0.242045</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.668647</td>\n",
       "      <td>0.675460</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6754596322941646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Attribution  Precision</th>\n",
       "      <th>Attribution  Recall</th>\n",
       "      <th>Attribution  F1-score</th>\n",
       "      <th>Attribution  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Conditional  Precision</th>\n",
       "      <th>Conditional  Recall</th>\n",
       "      <th>Conditional  F1-score</th>\n",
       "      <th>Conditional  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Description  Precision</th>\n",
       "      <th>Description  Recall</th>\n",
       "      <th>Description  F1-score</th>\n",
       "      <th>Description  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Fulfillment  Precision</th>\n",
       "      <th>Fulfillment  Recall</th>\n",
       "      <th>Fulfillment  F1-score</th>\n",
       "      <th>Fulfillment  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Purpose  Precision</th>\n",
       "      <th>Purpose  Recall</th>\n",
       "      <th>Purpose  F1-score</th>\n",
       "      <th>Purpose  Support</th>\n",
       "      <th>Summary  Precision</th>\n",
       "      <th>Summary  Recall</th>\n",
       "      <th>Summary  F1-score</th>\n",
       "      <th>Summary  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.046376</td>\n",
       "      <td>0.711575</td>\n",
       "      <td>0.944584</td>\n",
       "      <td>0.811688</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.064815</td>\n",
       "      <td>0.080925</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.310811</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>148</td>\n",
       "      <td>0.648281</td>\n",
       "      <td>0.127439</td>\n",
       "      <td>0.117954</td>\n",
       "      <td>0.116793</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.532135</td>\n",
       "      <td>0.648281</td>\n",
       "      <td>0.575308</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.993604</td>\n",
       "      <td>0.763788</td>\n",
       "      <td>0.924433</td>\n",
       "      <td>0.836467</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.064815</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.321678</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.405286</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.369231</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.345324</td>\n",
       "      <td>148</td>\n",
       "      <td>0.667466</td>\n",
       "      <td>0.155538</td>\n",
       "      <td>0.155099</td>\n",
       "      <td>0.141590</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.585600</td>\n",
       "      <td>0.667466</td>\n",
       "      <td>0.608635</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.110700</td>\n",
       "      <td>0.936369</td>\n",
       "      <td>0.880697</td>\n",
       "      <td>0.827456</td>\n",
       "      <td>0.853247</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.296552</td>\n",
       "      <td>0.398148</td>\n",
       "      <td>0.339921</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.430380</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.432990</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>148</td>\n",
       "      <td>0.673861</td>\n",
       "      <td>0.213246</td>\n",
       "      <td>0.233360</td>\n",
       "      <td>0.216635</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.668019</td>\n",
       "      <td>0.673861</td>\n",
       "      <td>0.665659</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.110700</td>\n",
       "      <td>0.937424</td>\n",
       "      <td>0.874834</td>\n",
       "      <td>0.827456</td>\n",
       "      <td>0.850485</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.335079</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.428094</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.361702</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.382022</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.414508</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.469208</td>\n",
       "      <td>148</td>\n",
       "      <td>0.677858</td>\n",
       "      <td>0.222528</td>\n",
       "      <td>0.251279</td>\n",
       "      <td>0.233040</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.668442</td>\n",
       "      <td>0.677858</td>\n",
       "      <td>0.668574</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.744100</td>\n",
       "      <td>0.956131</td>\n",
       "      <td>0.889503</td>\n",
       "      <td>0.811083</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.427184</td>\n",
       "      <td>84</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.439791</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.495575</td>\n",
       "      <td>148</td>\n",
       "      <td>0.685052</td>\n",
       "      <td>0.255076</td>\n",
       "      <td>0.274081</td>\n",
       "      <td>0.250574</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.694682</td>\n",
       "      <td>0.685052</td>\n",
       "      <td>0.680078</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6850519584332534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 10:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Attribution  Precision</th>\n",
       "      <th>Attribution  Recall</th>\n",
       "      <th>Attribution  F1-score</th>\n",
       "      <th>Attribution  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Conditional  Precision</th>\n",
       "      <th>Conditional  Recall</th>\n",
       "      <th>Conditional  F1-score</th>\n",
       "      <th>Conditional  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Description  Precision</th>\n",
       "      <th>Description  Recall</th>\n",
       "      <th>Description  F1-score</th>\n",
       "      <th>Description  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Fulfillment  Precision</th>\n",
       "      <th>Fulfillment  Recall</th>\n",
       "      <th>Fulfillment  F1-score</th>\n",
       "      <th>Fulfillment  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Purpose  Precision</th>\n",
       "      <th>Purpose  Recall</th>\n",
       "      <th>Purpose  F1-score</th>\n",
       "      <th>Purpose  Support</th>\n",
       "      <th>Summary  Precision</th>\n",
       "      <th>Summary  Recall</th>\n",
       "      <th>Summary  F1-score</th>\n",
       "      <th>Summary  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.102807</td>\n",
       "      <td>0.798111</td>\n",
       "      <td>0.851385</td>\n",
       "      <td>0.823888</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.217228</td>\n",
       "      <td>0.682353</td>\n",
       "      <td>0.329545</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.419118</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.402827</td>\n",
       "      <td>147</td>\n",
       "      <td>0.632800</td>\n",
       "      <td>0.119538</td>\n",
       "      <td>0.160124</td>\n",
       "      <td>0.129688</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.571020</td>\n",
       "      <td>0.632800</td>\n",
       "      <td>0.593115</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.037345</td>\n",
       "      <td>0.784444</td>\n",
       "      <td>0.889169</td>\n",
       "      <td>0.833530</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.247312</td>\n",
       "      <td>0.212963</td>\n",
       "      <td>0.228856</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.267380</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.367647</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.136054</td>\n",
       "      <td>0.184332</td>\n",
       "      <td>147</td>\n",
       "      <td>0.639200</td>\n",
       "      <td>0.132071</td>\n",
       "      <td>0.152202</td>\n",
       "      <td>0.134530</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.639200</td>\n",
       "      <td>0.595909</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.124600</td>\n",
       "      <td>1.185421</td>\n",
       "      <td>0.919440</td>\n",
       "      <td>0.661209</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.167513</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.216393</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.382271</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.475921</td>\n",
       "      <td>147</td>\n",
       "      <td>0.568800</td>\n",
       "      <td>0.145393</td>\n",
       "      <td>0.195830</td>\n",
       "      <td>0.153651</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.663455</td>\n",
       "      <td>0.568800</td>\n",
       "      <td>0.589275</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.124600</td>\n",
       "      <td>1.019781</td>\n",
       "      <td>0.865489</td>\n",
       "      <td>0.802267</td>\n",
       "      <td>0.832680</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.300546</td>\n",
       "      <td>0.509259</td>\n",
       "      <td>0.378007</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.611765</td>\n",
       "      <td>0.483721</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.370558</td>\n",
       "      <td>0.496599</td>\n",
       "      <td>0.424419</td>\n",
       "      <td>147</td>\n",
       "      <td>0.654400</td>\n",
       "      <td>0.203049</td>\n",
       "      <td>0.203690</td>\n",
       "      <td>0.180445</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.662904</td>\n",
       "      <td>0.654400</td>\n",
       "      <td>0.645908</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.805200</td>\n",
       "      <td>1.042702</td>\n",
       "      <td>0.860403</td>\n",
       "      <td>0.807305</td>\n",
       "      <td>0.833008</td>\n",
       "      <td>794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.329193</td>\n",
       "      <td>0.490741</td>\n",
       "      <td>0.394052</td>\n",
       "      <td>108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.426230</td>\n",
       "      <td>0.611765</td>\n",
       "      <td>0.502415</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.347619</td>\n",
       "      <td>0.496599</td>\n",
       "      <td>0.408964</td>\n",
       "      <td>147</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.284832</td>\n",
       "      <td>0.223400</td>\n",
       "      <td>0.209053</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.684906</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.652155</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1175' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1175/1175 09:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Attribution  Precision</th>\n",
       "      <th>Attribution  Recall</th>\n",
       "      <th>Attribution  F1-score</th>\n",
       "      <th>Attribution  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Conditional  Precision</th>\n",
       "      <th>Conditional  Recall</th>\n",
       "      <th>Conditional  F1-score</th>\n",
       "      <th>Conditional  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Description  Precision</th>\n",
       "      <th>Description  Recall</th>\n",
       "      <th>Description  F1-score</th>\n",
       "      <th>Description  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Fulfillment  Precision</th>\n",
       "      <th>Fulfillment  Recall</th>\n",
       "      <th>Fulfillment  F1-score</th>\n",
       "      <th>Fulfillment  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Purpose  Precision</th>\n",
       "      <th>Purpose  Recall</th>\n",
       "      <th>Purpose  F1-score</th>\n",
       "      <th>Purpose  Support</th>\n",
       "      <th>Summary  Precision</th>\n",
       "      <th>Summary  Recall</th>\n",
       "      <th>Summary  F1-score</th>\n",
       "      <th>Summary  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.060329</td>\n",
       "      <td>0.798387</td>\n",
       "      <td>0.873897</td>\n",
       "      <td>0.834437</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.018018</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.219672</td>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.343590</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.413333</td>\n",
       "      <td>0.210884</td>\n",
       "      <td>0.279279</td>\n",
       "      <td>147</td>\n",
       "      <td>0.633600</td>\n",
       "      <td>0.160949</td>\n",
       "      <td>0.156849</td>\n",
       "      <td>0.122944</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.613642</td>\n",
       "      <td>0.633600</td>\n",
       "      <td>0.587145</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.005140</td>\n",
       "      <td>0.797497</td>\n",
       "      <td>0.883985</td>\n",
       "      <td>0.838517</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.239216</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.358824</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>147</td>\n",
       "      <td>0.644000</td>\n",
       "      <td>0.138476</td>\n",
       "      <td>0.158043</td>\n",
       "      <td>0.128280</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.588099</td>\n",
       "      <td>0.644000</td>\n",
       "      <td>0.596039</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.108700</td>\n",
       "      <td>0.973943</td>\n",
       "      <td>0.869507</td>\n",
       "      <td>0.823455</td>\n",
       "      <td>0.845855</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.312925</td>\n",
       "      <td>0.422018</td>\n",
       "      <td>0.359375</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.552941</td>\n",
       "      <td>0.417778</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.385714</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.453782</td>\n",
       "      <td>147</td>\n",
       "      <td>0.663200</td>\n",
       "      <td>0.241988</td>\n",
       "      <td>0.204120</td>\n",
       "      <td>0.188217</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.663091</td>\n",
       "      <td>0.663200</td>\n",
       "      <td>0.652631</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.108700</td>\n",
       "      <td>0.972318</td>\n",
       "      <td>0.871056</td>\n",
       "      <td>0.800757</td>\n",
       "      <td>0.834428</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.458716</td>\n",
       "      <td>0.346021</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.348684</td>\n",
       "      <td>0.623529</td>\n",
       "      <td>0.447257</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>147</td>\n",
       "      <td>0.649600</td>\n",
       "      <td>0.211422</td>\n",
       "      <td>0.227665</td>\n",
       "      <td>0.211259</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.655048</td>\n",
       "      <td>0.649600</td>\n",
       "      <td>0.645859</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.755300</td>\n",
       "      <td>0.987990</td>\n",
       "      <td>0.869448</td>\n",
       "      <td>0.814628</td>\n",
       "      <td>0.841146</td>\n",
       "      <td>793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.316456</td>\n",
       "      <td>0.458716</td>\n",
       "      <td>0.374532</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.564706</td>\n",
       "      <td>0.426667</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.347368</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>0.391691</td>\n",
       "      <td>147</td>\n",
       "      <td>0.656800</td>\n",
       "      <td>0.207270</td>\n",
       "      <td>0.236419</td>\n",
       "      <td>0.217749</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.653115</td>\n",
       "      <td>0.656800</td>\n",
       "      <td>0.650621</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [79/79 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6632\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_none__precision</th>\n",
       "      <th>eval_none__recall</th>\n",
       "      <th>eval_none__f1-score</th>\n",
       "      <th>eval_none__support</th>\n",
       "      <th>eval_attribution__precision</th>\n",
       "      <th>eval_attribution__recall</th>\n",
       "      <th>eval_attribution__f1-score</th>\n",
       "      <th>eval_attribution__support</th>\n",
       "      <th>eval_causal__precision</th>\n",
       "      <th>eval_causal__recall</th>\n",
       "      <th>eval_causal__f1-score</th>\n",
       "      <th>eval_causal__support</th>\n",
       "      <th>eval_conditional__precision</th>\n",
       "      <th>eval_conditional__recall</th>\n",
       "      <th>eval_conditional__f1-score</th>\n",
       "      <th>eval_conditional__support</th>\n",
       "      <th>eval_contrast__precision</th>\n",
       "      <th>eval_contrast__recall</th>\n",
       "      <th>eval_contrast__f1-score</th>\n",
       "      <th>eval_contrast__support</th>\n",
       "      <th>eval_description__precision</th>\n",
       "      <th>eval_description__recall</th>\n",
       "      <th>eval_description__f1-score</th>\n",
       "      <th>eval_description__support</th>\n",
       "      <th>eval_equivalence__precision</th>\n",
       "      <th>eval_equivalence__recall</th>\n",
       "      <th>eval_equivalence__f1-score</th>\n",
       "      <th>eval_equivalence__support</th>\n",
       "      <th>eval_fulfillment__precision</th>\n",
       "      <th>eval_fulfillment__recall</th>\n",
       "      <th>eval_fulfillment__f1-score</th>\n",
       "      <th>eval_fulfillment__support</th>\n",
       "      <th>eval_identity__precision</th>\n",
       "      <th>eval_identity__recall</th>\n",
       "      <th>eval_identity__f1-score</th>\n",
       "      <th>eval_identity__support</th>\n",
       "      <th>eval_purpose__precision</th>\n",
       "      <th>eval_purpose__recall</th>\n",
       "      <th>eval_purpose__f1-score</th>\n",
       "      <th>eval_purpose__support</th>\n",
       "      <th>eval_summary__precision</th>\n",
       "      <th>eval_summary__recall</th>\n",
       "      <th>eval_summary__f1-score</th>\n",
       "      <th>eval_summary__support</th>\n",
       "      <th>eval_temporal__precision</th>\n",
       "      <th>eval_temporal__recall</th>\n",
       "      <th>eval_temporal__f1-score</th>\n",
       "      <th>eval_temporal__support</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_macro avg__precision</th>\n",
       "      <th>eval_macro avg__recall</th>\n",
       "      <th>eval_macro avg__f1-score</th>\n",
       "      <th>eval_macro avg__support</th>\n",
       "      <th>eval_weighted avg__precision</th>\n",
       "      <th>eval_weighted avg__recall</th>\n",
       "      <th>eval_weighted avg__f1-score</th>\n",
       "      <th>eval_weighted avg__support</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.917879</td>\n",
       "      <td>0.874145</td>\n",
       "      <td>0.804786</td>\n",
       "      <td>0.838033</td>\n",
       "      <td>794.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.568807</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>109.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.654762</td>\n",
       "      <td>0.539216</td>\n",
       "      <td>84.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.361809</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.416185</td>\n",
       "      <td>147.00</td>\n",
       "      <td>0.675460</td>\n",
       "      <td>0.217282</td>\n",
       "      <td>0.280679</td>\n",
       "      <td>0.242045</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>0.668647</td>\n",
       "      <td>0.675460</td>\n",
       "      <td>0.666386</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>13.014900</td>\n",
       "      <td>96.121000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.956131</td>\n",
       "      <td>0.889503</td>\n",
       "      <td>0.811083</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>794.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>108.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>42.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.427184</td>\n",
       "      <td>84.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.439791</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>0.495575</td>\n",
       "      <td>148.00</td>\n",
       "      <td>0.685052</td>\n",
       "      <td>0.255076</td>\n",
       "      <td>0.274081</td>\n",
       "      <td>0.250574</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>0.694682</td>\n",
       "      <td>0.685052</td>\n",
       "      <td>0.680078</td>\n",
       "      <td>1251.00000</td>\n",
       "      <td>13.112200</td>\n",
       "      <td>95.408000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.042702</td>\n",
       "      <td>0.860403</td>\n",
       "      <td>0.807305</td>\n",
       "      <td>0.833008</td>\n",
       "      <td>794.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.329193</td>\n",
       "      <td>0.490741</td>\n",
       "      <td>0.394052</td>\n",
       "      <td>108.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.426230</td>\n",
       "      <td>0.611765</td>\n",
       "      <td>0.502415</td>\n",
       "      <td>85.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.347619</td>\n",
       "      <td>0.496599</td>\n",
       "      <td>0.408964</td>\n",
       "      <td>147.00</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.284832</td>\n",
       "      <td>0.223400</td>\n",
       "      <td>0.209053</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>0.684906</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.652155</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>12.211600</td>\n",
       "      <td>102.362000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.973943</td>\n",
       "      <td>0.869507</td>\n",
       "      <td>0.823455</td>\n",
       "      <td>0.845855</td>\n",
       "      <td>793.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.312925</td>\n",
       "      <td>0.422018</td>\n",
       "      <td>0.359375</td>\n",
       "      <td>109.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.552941</td>\n",
       "      <td>0.417778</td>\n",
       "      <td>85.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.385714</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.453782</td>\n",
       "      <td>147.00</td>\n",
       "      <td>0.663200</td>\n",
       "      <td>0.241988</td>\n",
       "      <td>0.204120</td>\n",
       "      <td>0.188217</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>0.663091</td>\n",
       "      <td>0.663200</td>\n",
       "      <td>0.652631</td>\n",
       "      <td>1250.00000</td>\n",
       "      <td>10.381000</td>\n",
       "      <td>120.412000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>0.972664</td>\n",
       "      <td>0.873389</td>\n",
       "      <td>0.811657</td>\n",
       "      <td>0.841345</td>\n",
       "      <td>793.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.344448</td>\n",
       "      <td>0.530114</td>\n",
       "      <td>0.416815</td>\n",
       "      <td>108.50000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.018002</td>\n",
       "      <td>0.032738</td>\n",
       "      <td>41.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.25</td>\n",
       "      <td>0.395233</td>\n",
       "      <td>0.585819</td>\n",
       "      <td>0.471648</td>\n",
       "      <td>84.50000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.75</td>\n",
       "      <td>0.667400</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.463498</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.50000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.383733</td>\n",
       "      <td>0.526246</td>\n",
       "      <td>0.443626</td>\n",
       "      <td>147.25</td>\n",
       "      <td>0.670928</td>\n",
       "      <td>0.249795</td>\n",
       "      <td>0.245570</td>\n",
       "      <td>0.222473</td>\n",
       "      <td>1250.50000</td>\n",
       "      <td>0.677832</td>\n",
       "      <td>0.670928</td>\n",
       "      <td>0.662812</td>\n",
       "      <td>1250.50000</td>\n",
       "      <td>12.179925</td>\n",
       "      <td>103.575750</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.052223</td>\n",
       "      <td>0.012165</td>\n",
       "      <td>0.008280</td>\n",
       "      <td>0.007113</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.027929</td>\n",
       "      <td>0.094099</td>\n",
       "      <td>0.049480</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.471405</td>\n",
       "      <td>0.022848</td>\n",
       "      <td>0.040517</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.056805</td>\n",
       "      <td>0.058744</td>\n",
       "      <td>0.058853</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.238114</td>\n",
       "      <td>0.357071</td>\n",
       "      <td>0.250751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040543</td>\n",
       "      <td>0.038854</td>\n",
       "      <td>0.039818</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.011535</td>\n",
       "      <td>0.028128</td>\n",
       "      <td>0.037662</td>\n",
       "      <td>0.029019</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.014556</td>\n",
       "      <td>0.011535</td>\n",
       "      <td>0.013268</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>1.265366</td>\n",
       "      <td>11.650722</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.weight', 'config', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='529' max='1175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 529/1175 04:21 < 05:20, 2.02 it/s, Epoch 2.25/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>None  Precision</th>\n",
       "      <th>None  Recall</th>\n",
       "      <th>None  F1-score</th>\n",
       "      <th>None  Support</th>\n",
       "      <th>Causal  Precision</th>\n",
       "      <th>Causal  Recall</th>\n",
       "      <th>Causal  F1-score</th>\n",
       "      <th>Causal  Support</th>\n",
       "      <th>Contrast  Precision</th>\n",
       "      <th>Contrast  Recall</th>\n",
       "      <th>Contrast  F1-score</th>\n",
       "      <th>Contrast  Support</th>\n",
       "      <th>Equivalence  Precision</th>\n",
       "      <th>Equivalence  Recall</th>\n",
       "      <th>Equivalence  F1-score</th>\n",
       "      <th>Equivalence  Support</th>\n",
       "      <th>Identity  Precision</th>\n",
       "      <th>Identity  Recall</th>\n",
       "      <th>Identity  F1-score</th>\n",
       "      <th>Identity  Support</th>\n",
       "      <th>Temporal  Precision</th>\n",
       "      <th>Temporal  Recall</th>\n",
       "      <th>Temporal  F1-score</th>\n",
       "      <th>Temporal  Support</th>\n",
       "      <th>Others  Precision</th>\n",
       "      <th>Others  Recall</th>\n",
       "      <th>Others  F1-score</th>\n",
       "      <th>Others  Support</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro avg  Precision</th>\n",
       "      <th>Macro avg  Recall</th>\n",
       "      <th>Macro avg  F1-score</th>\n",
       "      <th>Macro avg  Support</th>\n",
       "      <th>Weighted avg  Precision</th>\n",
       "      <th>Weighted avg  Recall</th>\n",
       "      <th>Weighted avg  F1-score</th>\n",
       "      <th>Weighted avg  Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.065366</td>\n",
       "      <td>0.715706</td>\n",
       "      <td>0.906801</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>794</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.064220</td>\n",
       "      <td>0.099291</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41</td>\n",
       "      <td>0.216346</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.307167</td>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>55</td>\n",
       "      <td>0.618705</td>\n",
       "      <td>0.259638</td>\n",
       "      <td>0.219542</td>\n",
       "      <td>0.182203</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.517322</td>\n",
       "      <td>0.618705</td>\n",
       "      <td>0.540308</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.963294</td>\n",
       "      <td>0.772414</td>\n",
       "      <td>0.846348</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>794</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.192661</td>\n",
       "      <td>0.248521</td>\n",
       "      <td>109</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>41</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>85</td>\n",
       "      <td>0.242857</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>20</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.258503</td>\n",
       "      <td>0.310204</td>\n",
       "      <td>147</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>55</td>\n",
       "      <td>0.633094</td>\n",
       "      <td>0.371247</td>\n",
       "      <td>0.397119</td>\n",
       "      <td>0.346525</td>\n",
       "      <td>1251</td>\n",
       "      <td>0.611498</td>\n",
       "      <td>0.633094</td>\n",
       "      <td>0.613077</td>\n",
       "      <td>1251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/data/experiments/raring/miniconda2/envs/semantic_relation/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "for model_checkpoint in model_checkpoints:\n",
    "    for input_strategy in input_strategies:\n",
    "        for subset_labels in subset_labels_types:            \n",
    "\n",
    "            labels = [\"none\", \"attribution\", \"causal\", \"conditional\", \"contrast\", \"description\", \"equivalence\", \"fulfillment\", \"identity\", \"purpose\", \"summary\", \"temporal\"]\n",
    "\n",
    "            # Use subset of labels (defined below)\n",
    "            if subset_labels:\n",
    "                labels = [\"none\", \"causal\", \"contrast\", \"equivalence\", \"identity\", \"temporal\", \"others\"]\n",
    "\n",
    "            # Import Paths\n",
    "            import_path = f\"data/export/{input_strategy}\"\n",
    "            if subset_labels:\n",
    "                import_path = f\"data/export_subset/{input_strategy}\"\n",
    "\n",
    "            # Export Paths\n",
    "            export_path_model = \"data/model\"\n",
    "            export_path_eval = \"data/eval\"\n",
    "\n",
    "            if subset_labels:\n",
    "                export_path_model += \"_subset\"\n",
    "                export_path_eval += \"_subset\"\n",
    "\n",
    "            # Metrics\n",
    "            def compute_metrics(eval_pred):\n",
    "                global labels\n",
    "                predictions, true_labels = eval_pred\n",
    "                # take most probable guess\n",
    "                predictions = np.argmax(predictions, axis=-1)\n",
    "                return flatten(classification_report(\n",
    "                    y_true=true_labels,\n",
    "                    y_pred=predictions,\n",
    "                    target_names=labels,\n",
    "                    zero_division=0,\n",
    "                    output_dict=True))\n",
    "\n",
    "            # Tokenizer\n",
    "            if \"microsoft\" in model_checkpoint:\n",
    "                tokenizer = DebertaTokenizerFast.from_pretrained(model_checkpoint)\n",
    "            else:\n",
    "                tokenizer = BertTokenizerFast.from_pretrained(model_checkpoint)\n",
    "\n",
    "            # Train\n",
    "            result = []\n",
    "            num_labels = len(labels)\n",
    "            count = 0\n",
    "            for i in range(num_folds):\n",
    "                model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "                # import Fold data\n",
    "                train_origin, train_target, train_labels, test_origin, test_target, test_labels = import_fold(import_path, i)\n",
    "                # tokenize\n",
    "                train_encodings = tokenizer(train_origin, train_target, truncation=True, padding=True, return_token_type_ids=True)\n",
    "                test_encodings = tokenizer(test_origin, test_target, truncation=True, padding=True, return_token_type_ids=True)\n",
    "                # dataset creation\n",
    "                train_dataset = SemanticDataset(train_encodings, train_labels)\n",
    "                test_dataset = SemanticDataset(test_encodings, test_labels)\n",
    "                # create Trainer\n",
    "                trainer = Trainer(\n",
    "                    model,\n",
    "                    args,\n",
    "                    train_dataset=train_dataset,\n",
    "                    eval_dataset=test_dataset,\n",
    "                    tokenizer=tokenizer,\n",
    "                    compute_metrics=compute_metrics\n",
    "                )\n",
    "                # train & evaluate\n",
    "                trainer.train()\n",
    "                ev = trainer.evaluate(test_dataset)\n",
    "                acc = ev[\"eval_accuracy\"]\n",
    "                print(f\"Accuracy: {acc}\")\n",
    "                result.append(ev)\n",
    "                # Save Model\n",
    "                path = f\"{export_path_model}/{input_strategy}/{model_checkpoint.replace(r'/', '-')}/epoch_{num_epoch}/fold_{count}\"\n",
    "                # Check if Path exists\n",
    "                Path(path).mkdir(parents=True, exist_ok=True)\n",
    "                trainer.save_model(path)\n",
    "                # Free memory\n",
    "                del model, train_origin, train_target, train_labels, test_origin, test_target, test_labels, train_encodings, test_encodings, train_dataset, test_dataset, trainer \n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                count += 1\n",
    "\n",
    "            # Calc Metrics\n",
    "            def transform_to_regular_dict(result):\n",
    "                output_dict = {}\n",
    "                count = 0\n",
    "                for eval_item in result:\n",
    "                    for key in eval_item:\n",
    "                        if count == 0:\n",
    "                          output_dict[key] = [float(eval_item[key])]\n",
    "                        else:\n",
    "                          output_dict[key].append(eval_item[key]) \n",
    "                    count += 1\n",
    "                return output_dict\n",
    "\n",
    "            eval_dict = transform_to_regular_dict(result)\n",
    "            eval_df = pd.DataFrame(eval_dict)\n",
    "\n",
    "            def add_mean_std_row(df):\n",
    "                row_mean = []\n",
    "                row_std = []\n",
    "                for column in df:\n",
    "                    row_mean.append(mean(df[column]))\n",
    "                    row_std.append(stddev(df[column], ddof=1))\n",
    "                df = df.append(pd.DataFrame([row_mean], columns=df.columns), ignore_index=True)\n",
    "                df = df.append(pd.DataFrame([row_std], columns=df.columns), ignore_index=True)\n",
    "                # add better readable Index\n",
    "                df[\"fold\"] = [\"1\", \"2\", \"3\", \"4\", \"avg\", \"std\"]\n",
    "                df = df.set_index(\"fold\")\n",
    "                return df\n",
    "\n",
    "            eval_df = add_mean_std_row(eval_df)\n",
    "            display(HTML(eval_df.to_html()))\n",
    "\n",
    "            # Eval\n",
    "            # Check if Path exists\n",
    "            path = f\"{export_path_eval}/{input_strategy}/\"\n",
    "            Path(path).mkdir(parents=True, exist_ok=True)\n",
    "            eval_df.to_csv(f\"{path}/{model_checkpoint.replace(r'/', '-')}_epoch_{num_epoch}.csv\")\n",
    "\n",
    "            # datetime object containing current date and time\n",
    "            now = datetime.now()\n",
    "\n",
    "            # LOGS\n",
    "            # Open the file in append & read mode ('a+')\n",
    "            with open(\"logs.txt\", \"a+\") as file_object:\n",
    "                # Move read cursor to the start of file.\n",
    "                file_object.seek(0)\n",
    "                # If file is not empty then append '\\n'\n",
    "                data = file_object.read(100)\n",
    "                if len(data) > 0 :\n",
    "                    file_object.write(\"\\n\")\n",
    "                # Append text at the end of file\n",
    "                file_object.write(f\"{now} - finished: {model_checkpoint}, Batch: {batch_size}, Epochs: {num_epoch}, Folds: {num_folds}, Subset: {subset_labels}, Input_strategy: {input_strategy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
