{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aquatic-australia",
   "metadata": {},
   "source": [
    "# Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c9780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_checkpoint = 'bert-large-cased'\n",
    "batch_size = 1\n",
    "metric_name = \"accuracy\"\n",
    "num_epoch = 10\n",
    "\n",
    "# Fold\n",
    "num_folds = 4\n",
    "\n",
    "# Experiment\n",
    "labels = [\"none\", \"attribution\", \"causal\", \"conditional\", \"contrast\", \"description\", \"equivalence\", \"fulfillment\", \"identity\", \"purpose\", \"summary\", \"temporal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfcfcb1",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d51af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8188a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_fold(path, fold):\n",
    "    train = pd.read_csv(f\"{path}/train.{fold}.csv\")\n",
    "    test = pd.read_csv(f\"{path}/test.{fold}.csv\")\n",
    "    train_origin = train[\"origin\"].tolist()\n",
    "    train_target = train[\"target\"].tolist()\n",
    "    train_labels = train[\"label\"].tolist()\n",
    "    test_origin = test[\"origin\"].tolist()\n",
    "    test_target = test[\"target\"].tolist()\n",
    "    test_labels = test[\"label\"].tolist()\n",
    "    return train_origin, train_target, train_labels, test_origin, test_target, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1845b3",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a464f6",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "verbal-lexington",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import collections\n",
    "\n",
    "#classification_threshold = 0.\n",
    "\n",
    "def flatten(d, parent_key='', sep='__'):\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, collections.MutableMapping):\n",
    "            items.extend(flatten(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    global labels\n",
    "    predictions, true_labels = eval_pred\n",
    "    # take most probable guess\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    return flatten(classification_report(\n",
    "        y_true=true_labels,\n",
    "        y_pred=predictions,\n",
    "        target_names=labels,\n",
    "        zero_division=0,\n",
    "        output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "opened-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST\n",
    "#flatten(classification_report(\n",
    "#    y_true=[0,1,2,3,4,5,6,7,8,9,10,11,12],\n",
    "#    y_pred=[0,0,0,1,3,0,0,0,0,0,0,0,0],\n",
    "#    target_names=labels,\n",
    "#    zero_division=0,\n",
    "#    output_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-taste",
   "metadata": {},
   "source": [
    "## Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "referenced-wichita",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"semantic-test\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epoch,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-wedding",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sensitive-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, DebertaTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_checkpoint)\n",
    "#tokenizer = DebertaTokenizerFast.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b7ee1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cf32517",
   "metadata": {},
   "source": [
    "## Print Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efbd918e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbe4fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(origin_list, target_list, label_list, encodings, num_examples=10):\n",
    "    global labels\n",
    "    assert num_examples <= len(origin_list), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(origin_list)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(origin_list)-1)\n",
    "        picks.append(pick)\n",
    "    data = []\n",
    "    for n in picks:\n",
    "        data.append([n, origin_list[n], labels[label_list[n]], target_list[n], encodings.input_ids[n], encodings.token_type_ids[n], encodings.attention_mask[n]])\n",
    "    df = pd.DataFrame(data, columns=['index', 'Origin', 'Label', 'Target', 'Input_ids', 'Token_type_ids', 'Attention_mask'])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cb93df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_random_elements(train_origin, train_target, train_labels, train_encodings)\n",
    "# Output adjustet to Folds\n",
    "#show_random_elements(k_fold_origin[0][0], k_fold_target[0][0], k_fold_labels[0][0], train_encodings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-lightweight",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "impressive-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-destiny",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-winning",
   "metadata": {},
   "source": [
    "## Train & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-medication",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1064' max='37510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1064/37510 02:36 < 1:29:24, 6.79 it/s, Epoch 0.28/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = []\n",
    "num_labels = len(labels)\n",
    "models = []\n",
    "\n",
    "for i in range(num_folds):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "    # import Fold data\n",
    "    train_origin, train_target, train_labels, test_origin, test_target, test_labels = import_fold(\"data/export-umgekehrt\", i)\n",
    "    # tokenize\n",
    "    train_encodings = tokenizer(train_origin, train_target, truncation=True, padding=True, return_token_type_ids=True)\n",
    "    test_encodings = tokenizer(test_origin, test_target, truncation=True, padding=True, return_token_type_ids=True)\n",
    "    # dataset creation\n",
    "    train_dataset = SemanticDataset(train_encodings, train_labels)\n",
    "    test_dataset = SemanticDataset(test_encodings, test_labels)\n",
    "    # create Trainer\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    # train & evaluate\n",
    "    trainer.train()\n",
    "    ev = trainer.evaluate(test_dataset)\n",
    "    acc = ev[\"eval_accuracy\"]\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    result.append(ev)\n",
    "    models.append(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e39bf6",
   "metadata": {},
   "source": [
    "## Interpret evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be62939",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(data):\n",
    "    \"\"\"Return the sample arithmetic mean of data.\"\"\"\n",
    "    n = len(data)\n",
    "    if n < 1:\n",
    "        raise ValueError('mean requires at least one data point')\n",
    "    return sum(data)/n # in Python 2 use sum(data)/float(n)\n",
    "\n",
    "def _ss(data):\n",
    "    \"\"\"Return sum of square deviations of sequence data.\"\"\"\n",
    "    c = mean(data)\n",
    "    ss = sum((x-c)**2 for x in data)\n",
    "    return ss\n",
    "\n",
    "def stddev(data, ddof=0):\n",
    "    \"\"\"Calculates the population standard deviation\n",
    "    by default; specify ddof=1 to compute the sample\n",
    "    standard deviation.\"\"\"\n",
    "    n = len(data)\n",
    "    if n < 2:\n",
    "        raise ValueError('variance requires at least two data points')\n",
    "    ss = _ss(data)\n",
    "    pvar = ss/(n-ddof)\n",
    "    return pvar**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d41df",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df0f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dad3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_to_regular_dict(result):\n",
    "    output_dict = {}\n",
    "    count = 0\n",
    "    for eval_item in result:\n",
    "        for key in eval_item:\n",
    "            if count == 0:\n",
    "              output_dict[key] = [float(eval_item[key])]\n",
    "            else:\n",
    "              output_dict[key].append(eval_item[key]) \n",
    "        count += 1\n",
    "    return output_dict\n",
    "            \n",
    "eval_dict = transform_to_regular_dict(result)\n",
    "eval_df = pd.DataFrame(eval_dict)\n",
    "\n",
    "def add_mean_std_row(df):\n",
    "    row_mean = []\n",
    "    row_std = []\n",
    "    for column in df:\n",
    "        row_mean.append(mean(df[column]))\n",
    "        row_std.append(stddev(df[column], ddof=1))\n",
    "    df = df.append(pd.DataFrame([row_mean], columns=df.columns), ignore_index=True)\n",
    "    df = df.append(pd.DataFrame([row_std], columns=df.columns), ignore_index=True)\n",
    "    # add better readable Index\n",
    "    df[\"fold\"] = [\"1\", \"2\", \"3\", \"4\", \"avg\", \"std\"]\n",
    "    df = df.set_index(\"fold\")\n",
    "    return df\n",
    "\n",
    "eval_df = add_mean_std_row(eval_df)\n",
    "display(HTML(eval_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad0e60",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df57efb",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed76e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for model in models:\n",
    "    model.save_model(f\"/data/experiments/raring/semantic_storytelling/data/model-umgekehrt/{model_checkpoint.replace(r'/', '-')}/epoch_{num_epoch}/fold_{count}\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd633be",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f2271",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_csv(f\"data/eval-umgekehrt/{model_checkpoint.replace(r'/', '-')}_epoch_{num_epoch}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ef9e14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
